wordCutOff = 0
featCutOff = 0
initRange = 0.01
maxIter = 100
batchSize = 1
adaEps = 1e-06
adaAlpha = 0.01
regParameter = 1e-08
dropProb = -1
hiddenSize = 50
wordEmbSize = 50
wordcontext = 2
wordEmbFineTune = 1
cnnLayerSize = 2
verboseIter = 10
saveItermediate = 1
train = 0
maxInstance = 100
outBest = .debug
seg = 0
wordFile = 

instance num: 100

instance num: 100

instance num: 100
Creating Alphabet...
10 20 30 40 50 60 70 80 90 100 100 
Label num: 2
Sparse Feature num: 0
Word num: 776
Adding word Alphabet...
10 20 30 40 50 60 70 80 90 100 100 
Adding word Alphabet...
10 20 30 40 50 60 70 80 90 100 100 
10 20 30 40 50 60 70 80 90 100 100 
10 20 30 40 50 60 70 80 90 100 100 
10 20 30 40 50 60 70 80 90 100 100 
##### Iteration 0
random: 0, 99
Iteration: 0, Checking gradient for words E[19][26]:	mock grad = -0.076062783106956022, computed grad = -0.076062783047958341
Iteration: 0, Checking gradient for hidden W[81][9]:	mock grad = 0.019677631588110867, computed grad = 0.019677631644363657
Iteration: 0, Checking gradient for hidden b[0][9]:	mock grad = -0.071410134140692527, computed grad = -0.071410135406560854
Iteration: 0, Checking gradient for output layer W[15][0]:	mock grad = -0.093505919386449587, computed grad = -0.093505919330228199
Iteration: 1, Checking gradient for words E[78][35]:	mock grad = 0.001450398480262738, computed grad = 0.001450398494841117
Iteration: 1, Checking gradient for hidden W[89][10]:	mock grad = 0.015318016050702710, computed grad = 0.015318016088130272
Iteration: 1, Checking gradient for hidden b[0][16]:	mock grad = 0.026432694401112844, computed grad = 0.026432703870669417
Iteration: 1, Checking gradient for output layer W[82][0]:	mock grad = -0.269543921897319461, computed grad = -0.269543920355993394
Iteration: 2, Checking gradient for words E[32][34]:	mock grad = -0.016157441180053578, computed grad = -0.016157441202789908
Iteration: 2, Checking gradient for hidden W[152][42]:	mock grad = 0.008171520969990631, computed grad = 0.008171521003008568
Iteration: 2, Checking gradient for hidden b[0][12]:	mock grad = -0.133543244691003427, computed grad = -0.133543289055785108
Iteration: 2, Checking gradient for output layer W[96][0]:	mock grad = 0.359600155056427617, computed grad = 0.359600156542038918
Iteration: 3, Checking gradient for words E[102][41]:	mock grad = -0.040315320655848197, computed grad = -0.040315320727463376
Iteration: 3, Checking gradient for hidden W[183][48]:	mock grad = 0.010871145710078522, computed grad = 0.010871145706226402
Iteration: 3, Checking gradient for hidden b[0][5]:	mock grad = 0.021164605885082288, computed grad = 0.021164595589533920
Iteration: 3, Checking gradient for output layer W[51][0]:	mock grad = 0.344160434445628383, computed grad = 0.344160436142378123
Iteration: 4, Checking gradient for words E[64][27]:	mock grad = -0.008181440745158586, computed grad = -0.008181440721434563
Iteration: 4, Checking gradient for hidden W[90][14]:	mock grad = 0.018429900364658192, computed grad = 0.018429900492959828
Iteration: 4, Checking gradient for hidden b[0][46]:	mock grad = -0.281857840406352089, computed grad = -0.281857913048014108
Iteration: 4, Checking gradient for output layer W[97][0]:	mock grad = 0.328484746611512612, computed grad = 0.328484748024862883
Iteration: 5, Checking gradient for words E[240][45]:	mock grad = 0.000111014935022524, computed grad = 0.000111014888019120
Iteration: 5, Checking gradient for hidden W[231][46]:	mock grad = 0.034702284754772883, computed grad = 0.034702284949584848
Iteration: 5, Checking gradient for hidden b[0][37]:	mock grad = -0.099483731046845492, computed grad = -0.099483765582847425
Iteration: 5, Checking gradient for output layer W[63][0]:	mock grad = 0.183606839503247521, computed grad = 0.183606839684021500
Iteration: 6, Checking gradient for words E[277][7]:	mock grad = -0.006060078746983333, computed grad = -0.006060078629912777
Iteration: 6, Checking gradient for hidden W[172][25]:	mock grad = 0.008769226951310660, computed grad = 0.008769227003164881
Iteration: 6, Checking gradient for hidden b[0][30]:	mock grad = -0.000492844928867697, computed grad = -0.000492850065550962
Iteration: 6, Checking gradient for output layer W[118][1]:	mock grad = 0.036640901853457475, computed grad = 0.036640901853523256
Iteration: 7, Checking gradient for words E[25][15]:	mock grad = -0.070015732953565557, computed grad = -0.070015733044876904
Iteration: 7, Checking gradient for hidden W[235][41]:	mock grad = -0.008954575650532703, computed grad = -0.008954575773704157
Iteration: 7, Checking gradient for hidden b[0][44]:	mock grad = -0.007121426595479985, computed grad = -0.007121446963692039
Iteration: 7, Checking gradient for output layer W[22][0]:	mock grad = 0.017194036408318958, computed grad = 0.017194036408252195
Iteration: 8, Checking gradient for words E[48][14]:	mock grad = -0.044514307644516471, computed grad = -0.044514307892276325
Iteration: 8, Checking gradient for hidden W[51][15]:	mock grad = -0.013716031209920487, computed grad = -0.013716031283473571
Iteration: 8, Checking gradient for hidden b[0][24]:	mock grad = 0.145421871164952421, computed grad = 0.145421891922564273
Iteration: 8, Checking gradient for output layer W[69][1]:	mock grad = -0.039022101399666109, computed grad = -0.039022101401412448
current: 10, Cost = 0.6112, Correct(%) = 0.4, time = 0.771529
Iteration: 9, Checking gradient for words E[25][25]:	mock grad = 0.007222697863573657, computed grad = 0.009685290537104735
Iteration: 9, Checking gradient for hidden W[177][17]:	mock grad = 0.003073995010094421, computed grad = 0.003073995016898337
Iteration: 9, Checking gradient for hidden b[0][48]:	mock grad = 0.113995158548596542, computed grad = 0.113995154569396004
Iteration: 9, Checking gradient for output layer W[135][0]:	mock grad = 0.070312022055041279, computed grad = 0.070312022042190142
Iteration: 10, Checking gradient for words E[124][24]:	mock grad = 0.017973419758121612, computed grad = 0.017973419841717336
Iteration: 10, Checking gradient for hidden W[112][29]:	mock grad = 0.023832436969961623, computed grad = 0.023832437211355795
Iteration: 10, Checking gradient for hidden b[0][22]:	mock grad = -0.071975832985016197, computed grad = -0.071975851481238570
Iteration: 10, Checking gradient for output layer W[65][1]:	mock grad = -0.289923903658007021, computed grad = -0.289923903591257803
Iteration: 11, Checking gradient for words E[102][0]:	mock grad = 0.024195227570023015, computed grad = 0.024195227542826506
Iteration: 11, Checking gradient for hidden W[210][43]:	mock grad = 0.000517683600376184, computed grad = 0.000517683598389999
Iteration: 11, Checking gradient for hidden b[0][26]:	mock grad = -0.047539282958253271, computed grad = -0.047539294667190958
Iteration: 11, Checking gradient for output layer W[33][0]:	mock grad = -0.028516798199929028, computed grad = -0.028516798199442181
Iteration: 12, Checking gradient for words E[776][14]:	mock grad = -0.018942841291391677, computed grad = -0.018942841346026459
Iteration: 12, Checking gradient for hidden W[7][12]:	mock grad = 0.001993030283198305, computed grad = 0.001993030301306288
Iteration: 12, Checking gradient for hidden b[0][12]:	mock grad = -0.124242380954875387, computed grad = -0.124242421961323429
Iteration: 12, Checking gradient for output layer W[132][1]:	mock grad = -0.020637574022985827, computed grad = -0.020637574023207848
Iteration: 13, Checking gradient for words E[368][3]:	mock grad = -0.035025487240436526, computed grad = -0.034668689232671045
Iteration: 13, Checking gradient for hidden W[23][2]:	mock grad = -0.050565081604114770, computed grad = -0.050565082298522017
Iteration: 13, Checking gradient for hidden b[0][33]:	mock grad = -0.159243695783906070, computed grad = -0.159243733304133772
Iteration: 13, Checking gradient for output layer W[4][0]:	mock grad = -0.009552524864953060, computed grad = -0.009552524864926287
Iteration: 14, Checking gradient for words E[69][24]:	mock grad = 0.002829792518588281, computed grad = 0.002829792450137237
Iteration: 14, Checking gradient for hidden W[86][38]:	mock grad = -0.020997463373473302, computed grad = -0.020997463639863941
Iteration: 14, Checking gradient for hidden b[0][37]:	mock grad = -0.097089100172143006, computed grad = -0.097089124977000327
Iteration: 14, Checking gradient for output layer W[94][0]:	mock grad = 0.278380146715773158, computed grad = 0.278380147493628105
Iteration: 15, Checking gradient for words E[102][14]:	mock grad = 0.054930079367188078, computed grad = 0.054930079412296294
Iteration: 15, Checking gradient for hidden W[74][13]:	mock grad = 0.009183008445601271, computed grad = 0.009183008670106921
Iteration: 15, Checking gradient for hidden b[0][27]:	mock grad = 0.045890716901031414, computed grad = 0.045890737717223136
Iteration: 15, Checking gradient for output layer W[19][0]:	mock grad = 0.119953018986707072, computed grad = 0.119953018921662574
Iteration: 16, Checking gradient for words E[197][10]:	mock grad = -0.025162401482647923, computed grad = -0.025162401629944275
Iteration: 16, Checking gradient for hidden W[56][0]:	mock grad = 0.037369414774945930, computed grad = 0.037369415391886214
Iteration: 16, Checking gradient for hidden b[0][43]:	mock grad = 0.019599699612404820, computed grad = 0.019599704752835616
Iteration: 16, Checking gradient for output layer W[0][0]:	mock grad = 0.156214787530306598, computed grad = 0.156214787640825054
Iteration: 17, Checking gradient for words E[53][10]:	mock grad = -0.004826406125035820, computed grad = -0.004826406139695356
Iteration: 17, Checking gradient for hidden W[161][36]:	mock grad = -0.009621922996638776, computed grad = -0.009621922951308646
Iteration: 17, Checking gradient for hidden b[0][36]:	mock grad = -0.004544134843020853, computed grad = -0.004544154513259905
Iteration: 17, Checking gradient for output layer W[21][0]:	mock grad = -0.004296944115023127, computed grad = -0.004296944114975954
Iteration: 18, Checking gradient for words E[424][22]:	mock grad = 0.004018856884224764, computed grad = 0.004018856986609176
Iteration: 18, Checking gradient for hidden W[214][25]:	mock grad = 0.000122813745395689, computed grad = 0.000122813726178452
Iteration: 18, Checking gradient for hidden b[0][45]:	mock grad = 0.011305582587439744, computed grad = 0.011305581416006003
Iteration: 18, Checking gradient for output layer W[71][1]:	mock grad = -0.102051121057489258, computed grad = -0.102051121059254374
current: 20, Cost = 0.587352, Correct(%) = 0.45, time = 1.43245
Iteration: 19, Checking gradient for words E[58][29]:	mock grad = -0.049981041622182421, computed grad = -0.049981041749400955
Iteration: 19, Checking gradient for hidden W[139][13]:	mock grad = -0.001663750223146465, computed grad = -0.001663750225962059
Iteration: 19, Checking gradient for hidden b[0][29]:	mock grad = -0.118657059473392668, computed grad = -0.118657097160633773
Iteration: 19, Checking gradient for output layer W[12][0]:	mock grad = -0.117697527478755060, computed grad = -0.117697527393381282
Iteration: 20, Checking gradient for words E[443][7]:	mock grad = 0.034020719571703495, computed grad = 0.034020719626234180
Iteration: 20, Checking gradient for hidden W[85][43]:	mock grad = -0.002211377718608354, computed grad = -0.002211377727713702
Iteration: 20, Checking gradient for hidden b[0][10]:	mock grad = -0.153405384268945344, computed grad = -0.153405436601612238
Iteration: 20, Checking gradient for output layer W[42][0]:	mock grad = -0.098237927388111768, computed grad = -0.098237927401616951
Iteration: 21, Checking gradient for words E[102][17]:	mock grad = -0.096488740644107285, computed grad = -0.096488740948674850
Iteration: 21, Checking gradient for hidden W[52][18]:	mock grad = 0.010563256290341272, computed grad = 0.010563256328670919
Iteration: 21, Checking gradient for hidden b[0][46]:	mock grad = -0.231121226152264381, computed grad = -0.231121285969073054
Iteration: 21, Checking gradient for output layer W[33][1]:	mock grad = 0.032398519400511638, computed grad = 0.032398519400358670
Iteration: 22, Checking gradient for words E[156][0]:	mock grad = -0.054008607704048028, computed grad = -0.054008607757688010
Iteration: 22, Checking gradient for hidden W[225][25]:	mock grad = -0.011812713488879023, computed grad = -0.011812713518761194
Iteration: 22, Checking gradient for hidden b[0][10]:	mock grad = -0.136213007268015662, computed grad = -0.136213048301416173
Iteration: 22, Checking gradient for output layer W[63][0]:	mock grad = 0.124588621065346850, computed grad = 0.124588621055336940
Iteration: 23, Checking gradient for words E[119][16]:	mock grad = -0.019868727710370404, computed grad = -0.019868727820637450
Iteration: 23, Checking gradient for hidden W[61][25]:	mock grad = 0.015231057188092034, computed grad = 0.015231057357704294
Iteration: 23, Checking gradient for hidden b[0][19]:	mock grad = 0.057466994392518789, computed grad = 0.057466989483062801
Iteration: 23, Checking gradient for output layer W[88][0]:	mock grad = 0.130783668415346277, computed grad = 0.130783668359699123
Iteration: 24, Checking gradient for words E[463][15]:	mock grad = -0.068874378516159407, computed grad = -0.068874378838664832
Iteration: 24, Checking gradient for hidden W[79][35]:	mock grad = 0.002984684809637095, computed grad = 0.002984684830542005
Iteration: 24, Checking gradient for hidden b[0][1]:	mock grad = -0.182794504925753998, computed grad = -0.182794558758959003
Iteration: 24, Checking gradient for output layer W[120][0]:	mock grad = 0.103236659256544794, computed grad = 0.103236659269288669
Iteration: 25, Checking gradient for words E[118][48]:	mock grad = 0.040934491240929294, computed grad = 0.040934491345303650
Iteration: 25, Checking gradient for hidden W[145][22]:	mock grad = -0.013254835028064793, computed grad = -0.013254835143292867
Iteration: 25, Checking gradient for hidden b[0][10]:	mock grad = 0.129163233812457090, computed grad = 0.129163268081879778
Iteration: 25, Checking gradient for output layer W[74][1]:	mock grad = 0.127760297982060234, computed grad = 0.127760297875063239
Iteration: 26, Checking gradient for words E[403][3]:	mock grad = -0.018484634441096048, computed grad = -0.018484634540133903
Iteration: 26, Checking gradient for hidden W[234][43]:	mock grad = 0.002758906079702861, computed grad = 0.002758906101566691
Iteration: 26, Checking gradient for hidden b[0][46]:	mock grad = -0.230632058799418171, computed grad = -0.230632113223752599
Iteration: 26, Checking gradient for output layer W[46][1]:	mock grad = -0.140158231883846796, computed grad = -0.140158231928039501
Iteration: 27, Checking gradient for words E[69][17]:	mock grad = 0.063181384626076476, computed grad = 0.063181384794488565
Iteration: 27, Checking gradient for hidden W[195][34]:	mock grad = 0.005432827902329684, computed grad = 0.005432827933055872
Iteration: 27, Checking gradient for hidden b[0][11]:	mock grad = -0.161972891455031132, computed grad = -0.161972943623526627
Iteration: 27, Checking gradient for output layer W[44][1]:	mock grad = 0.071506915622632228, computed grad = 0.071506915627744125
Iteration: 28, Checking gradient for words E[183][3]:	mock grad = 0.004842071475497089, computed grad = 0.004842071477797609
Iteration: 28, Checking gradient for hidden W[215][5]:	mock grad = 0.002274221354015005, computed grad = 0.002274221555831936
Iteration: 28, Checking gradient for hidden b[0][2]:	mock grad = 0.110130548672981998, computed grad = 0.110130610052939920
Iteration: 28, Checking gradient for output layer W[16][0]:	mock grad = 0.048347682214622267, computed grad = 0.048347682214469272
current: 30, Cost = 0.666242, Correct(%) = 0.5, time = 2.12686
Iteration: 29, Checking gradient for words E[143][1]:	mock grad = -0.008094633929034423, computed grad = -0.008094633968251372
Iteration: 29, Checking gradient for hidden W[39][9]:	mock grad = -0.014467205818269324, computed grad = -0.014467205848061594
Iteration: 29, Checking gradient for hidden b[0][0]:	mock grad = 0.254681205249074338, computed grad = 0.254681281995151276
Iteration: 29, Checking gradient for output layer W[36][1]:	mock grad = 0.083668288788030232, computed grad = 0.083668288782252520
Iteration: 30, Checking gradient for words E[172][45]:	mock grad = -0.022280250858708328, computed grad = -0.022280250954231386
Iteration: 30, Checking gradient for hidden W[130][7]:	mock grad = -0.004927570598889375, computed grad = -0.004927570616263079
Iteration: 30, Checking gradient for hidden b[0][9]:	mock grad = -0.089280150554760240, computed grad = -0.089280169196751788
Iteration: 30, Checking gradient for output layer W[108][0]:	mock grad = 0.148148799804903586, computed grad = 0.148148799619448768
Iteration: 31, Checking gradient for words E[216][39]:	mock grad = 0.027703007012047642, computed grad = 0.027703007058923233
Iteration: 31, Checking gradient for hidden W[157][38]:	mock grad = -0.006325202890122839, computed grad = -0.006325203020152744
Iteration: 31, Checking gradient for hidden b[0][37]:	mock grad = 0.087961493635546617, computed grad = 0.087961510075330457
Iteration: 31, Checking gradient for output layer W[88][0]:	mock grad = -0.101886197675427059, computed grad = -0.101886197698138878
Iteration: 32, Checking gradient for words E[25][40]:	mock grad = 0.028120316993007766, computed grad = 0.028120317066079582
Iteration: 32, Checking gradient for hidden W[32][3]:	mock grad = -0.022382085744621083, computed grad = -0.022382086113176464
Iteration: 32, Checking gradient for hidden b[0][16]:	mock grad = 0.019493906778023096, computed grad = 0.019493909894049682
Iteration: 32, Checking gradient for output layer W[126][0]:	mock grad = 0.151507042050441632, computed grad = 0.151507041549642424
Iteration: 33, Checking gradient for words E[143][13]:	mock grad = 0.024616534044619165, computed grad = 0.024616534093935646
Iteration: 33, Checking gradient for hidden W[43][7]:	mock grad = -0.009285438989647954, computed grad = -0.009285439168482717
Iteration: 33, Checking gradient for hidden b[0][45]:	mock grad = 0.014713345972694469, computed grad = 0.014713349184814633
Iteration: 33, Checking gradient for output layer W[147][0]:	mock grad = -0.005642198968702772, computed grad = -0.005642198968731012
Iteration: 34, Checking gradient for words E[21][39]:	mock grad = 0.022997620711340261, computed grad = 0.022997620756282495
Iteration: 34, Checking gradient for hidden W[88][3]:	mock grad = 0.016369529297455276, computed grad = 0.016369529587518295
Iteration: 34, Checking gradient for hidden b[0][36]:	mock grad = -0.002301520829417480, computed grad = -0.002301506667406630
Iteration: 34, Checking gradient for output layer W[45][0]:	mock grad = 0.000072480179247325, computed grad = 0.000072480179241310
Iteration: 35, Checking gradient for words E[201][29]:	mock grad = -0.025310235200193265, computed grad = -0.025310235324598432
Iteration: 35, Checking gradient for hidden W[22][5]:	mock grad = -0.006069944962272533, computed grad = -0.006069944986969643
Iteration: 35, Checking gradient for hidden b[0][43]:	mock grad = -0.017709643204188996, computed grad = -0.017709647881497256
Iteration: 35, Checking gradient for output layer W[2][0]:	mock grad = 0.126999199038080324, computed grad = 0.126999199006127078
Iteration: 36, Checking gradient for words E[10][24]:	mock grad = -0.068484999381446254, computed grad = -0.068484999837535282
Iteration: 36, Checking gradient for hidden W[245][25]:	mock grad = -0.000747937654754782, computed grad = -0.000747937668535765
Iteration: 36, Checking gradient for hidden b[0][30]:	mock grad = -0.003475558287169278, computed grad = -0.003475557745462697
Iteration: 36, Checking gradient for output layer W[107][0]:	mock grad = -0.024444515100419117, computed grad = -0.024444515099950576
Iteration: 37, Checking gradient for words E[69][25]:	mock grad = 0.032092479831824505, computed grad = 0.032092479882498290
Iteration: 37, Checking gradient for hidden W[140][22]:	mock grad = 0.013795363214996748, computed grad = 0.013795363274236257
Iteration: 37, Checking gradient for hidden b[0][35]:	mock grad = -0.020260321092646638, computed grad = -0.020260317831414593
Iteration: 37, Checking gradient for output layer W[3][0]:	mock grad = -0.020659141431456085, computed grad = -0.020659141431679292
Iteration: 38, Checking gradient for words E[102][15]:	mock grad = 0.020821891741962251, computed grad = 0.020821891921642709
Iteration: 38, Checking gradient for hidden W[79][12]:	mock grad = 0.009824404968605815, computed grad = 0.009824405073337584
Iteration: 38, Checking gradient for hidden b[0][36]:	mock grad = -0.015233452120455571, computed grad = -0.015233448792721939
Iteration: 38, Checking gradient for output layer W[137][0]:	mock grad = 0.160440981060661070, computed grad = 0.160440981059187360
current: 40, Cost = 0.681351, Correct(%) = 0.55, time = 2.56504
Iteration: 39, Checking gradient for words E[274][17]:	mock grad = -0.055461845698823176, computed grad = -0.055461845703868168
Iteration: 39, Checking gradient for hidden W[162][16]:	mock grad = -0.002060426196603959, computed grad = -0.002060426196824252
Iteration: 39, Checking gradient for hidden b[0][30]:	mock grad = -0.008145424305217031, computed grad = -0.008145437248586962
Iteration: 39, Checking gradient for output layer W[32][1]:	mock grad = -0.150838155613619573, computed grad = -0.150838155599591156
Iteration: 40, Checking gradient for words E[356][2]:	mock grad = 0.030867962226122092, computed grad = 0.030867962353695280
Iteration: 40, Checking gradient for hidden W[59][9]:	mock grad = 0.003590147311105518, computed grad = 0.003590147314235032
Iteration: 40, Checking gradient for hidden b[0][49]:	mock grad = -0.054371229392280451, computed grad = -0.054371227780589941
Iteration: 40, Checking gradient for output layer W[92][0]:	mock grad = 0.158600953566623293, computed grad = 0.158600953550980417
Iteration: 41, Checking gradient for words E[225][43]:	mock grad = -0.002513000356640926, computed grad = -0.002513000313589375
Iteration: 41, Checking gradient for hidden W[214][25]:	mock grad = 0.011698018114070230, computed grad = 0.011698018245640790
Iteration: 41, Checking gradient for hidden b[0][40]:	mock grad = 0.175877113224087722, computed grad = 0.175877165423377613
Iteration: 41, Checking gradient for output layer W[92][1]:	mock grad = 0.077202870485293396, computed grad = 0.077202870490542669
Iteration: 42, Checking gradient for words E[34][38]:	mock grad = 0.064835261432583113, computed grad = 0.064835261670198319
Iteration: 42, Checking gradient for hidden W[196][13]:	mock grad = 0.007824556401136995, computed grad = 0.007824556428820130
Iteration: 42, Checking gradient for hidden b[0][22]:	mock grad = 0.072275144817779413, computed grad = 0.072275163766710310
Iteration: 42, Checking gradient for output layer W[77][0]:	mock grad = 0.021431548423933577, computed grad = 0.021431548423747188
Iteration: 43, Checking gradient for words E[10][8]:	mock grad = 0.065288607517088870, computed grad = 0.065288607667706028
Iteration: 43, Checking gradient for hidden W[230][20]:	mock grad = -0.018030561693693770, computed grad = -0.018030561953953018
Iteration: 43, Checking gradient for hidden b[0][44]:	mock grad = -0.023655950964318606, computed grad = -0.023655987538125426
Iteration: 43, Checking gradient for output layer W[125][0]:	mock grad = 0.104865224625994014, computed grad = 0.104865224649617617
Iteration: 44, Checking gradient for words E[184][10]:	mock grad = 0.012773401954768726, computed grad = 0.012773401978761673
Iteration: 44, Checking gradient for hidden W[220][36]:	mock grad = 0.016159875360133302, computed grad = 0.016159875330432664
Iteration: 44, Checking gradient for hidden b[0][39]:	mock grad = -0.147229439454221733, computed grad = -0.147229499665622848
Iteration: 44, Checking gradient for output layer W[51][1]:	mock grad = -0.102994107608833652, computed grad = -0.102994107623693654
Iteration: 45, Checking gradient for words E[544][37]:	mock grad = 0.037426098680848341, computed grad = 0.037426098857633885
Iteration: 45, Checking gradient for hidden W[69][20]:	mock grad = 0.005187500288550417, computed grad = 0.005187500320243269
Iteration: 45, Checking gradient for hidden b[0][19]:	mock grad = -0.077821323252014185, computed grad = -0.077821339302076312
Iteration: 45, Checking gradient for output layer W[136][0]:	mock grad = 0.356172121055042812, computed grad = 0.356172121194389846
Iteration: 46, Checking gradient for words E[27][4]:	mock grad = -0.001486631747926737, computed grad = -0.001486631848822103
Iteration: 46, Checking gradient for hidden W[59][6]:	mock grad = -0.001488456219223000, computed grad = -0.001488456257423417
Iteration: 46, Checking gradient for hidden b[0][3]:	mock grad = -0.054622341555510268, computed grad = -0.054622354388982275
Iteration: 46, Checking gradient for output layer W[84][1]:	mock grad = 0.166678131803599250, computed grad = 0.166678131575432320
Iteration: 47, Checking gradient for words E[166][49]:	mock grad = -0.008922055685589569, computed grad = -0.008922055619159500
Iteration: 47, Checking gradient for hidden W[13][7]:	mock grad = 0.018001062063566220, computed grad = 0.018001062062339267
Iteration: 47, Checking gradient for hidden b[0][16]:	mock grad = -0.032980865439846685, computed grad = -0.032980878024983673
Iteration: 47, Checking gradient for output layer W[106][1]:	mock grad = 0.147086214215330635, computed grad = 0.147086214274775168
Iteration: 48, Checking gradient for words E[604][31]:	mock grad = -0.025474731673025275, computed grad = -0.025474731670219856
Iteration: 48, Checking gradient for hidden W[146][40]:	mock grad = -0.000752799226455547, computed grad = -0.000752799228864389
Iteration: 48, Checking gradient for hidden b[0][11]:	mock grad = -0.119286231646476004, computed grad = -0.119286263980529494
Iteration: 48, Checking gradient for output layer W[93][1]:	mock grad = 0.056156784839556106, computed grad = 0.056156784822991621
current: 50, Cost = 0.647621, Correct(%) = 0.54, time = 3.45223
Iteration: 49, Checking gradient for words E[506][45]:	mock grad = -0.042019131467063708, computed grad = -0.042019131636415873
Iteration: 49, Checking gradient for hidden W[133][44]:	mock grad = 0.018631091264931232, computed grad = 0.018631091298207586
Iteration: 49, Checking gradient for hidden b[0][41]:	mock grad = -0.053909829100307061, computed grad = -0.053909841026296999
Iteration: 49, Checking gradient for output layer W[25][0]:	mock grad = -0.143630820750140220, computed grad = -0.143630820697216305
Iteration: 50, Checking gradient for words E[25][40]:	mock grad = 0.042588091307749210, computed grad = 0.042588091448589417
Iteration: 50, Checking gradient for hidden W[158][24]:	mock grad = -0.016848409992165081, computed grad = -0.016848410070857786
Iteration: 50, Checking gradient for hidden b[0][30]:	mock grad = -0.002821774959194112, computed grad = -0.002821773080467277
Iteration: 50, Checking gradient for output layer W[103][0]:	mock grad = 0.139488201224224895, computed grad = 0.139488201248687105
Iteration: 51, Checking gradient for words E[621][49]:	mock grad = 0.015943179799915441, computed grad = 0.015943179818421881
Iteration: 51, Checking gradient for hidden W[66][39]:	mock grad = -0.009174108234311085, computed grad = -0.009174108286502697
Iteration: 51, Checking gradient for hidden b[0][24]:	mock grad = 0.150431092829139690, computed grad = 0.150431116871786896
Iteration: 51, Checking gradient for output layer W[37][1]:	mock grad = 0.088676824393840725, computed grad = 0.088676824413897237
Iteration: 52, Checking gradient for words E[211][38]:	mock grad = -0.025143756713030818, computed grad = -0.025044589996843926
Iteration: 52, Checking gradient for hidden W[6][47]:	mock grad = -0.002387680499815747, computed grad = -0.002387680472022084
Iteration: 52, Checking gradient for hidden b[0][24]:	mock grad = 0.179879196796961516, computed grad = 0.179879240483467290
Iteration: 52, Checking gradient for output layer W[18][0]:	mock grad = 0.136877089361220339, computed grad = 0.136877089454131601
Iteration: 53, Checking gradient for words E[97][30]:	mock grad = -0.008815428883257592, computed grad = -0.008815428819909628
Iteration: 53, Checking gradient for hidden W[178][33]:	mock grad = -0.007119962849577721, computed grad = -0.007119963081825295
Iteration: 53, Checking gradient for hidden b[0][5]:	mock grad = -0.013387987399804402, computed grad = -0.013387977006418453
Iteration: 53, Checking gradient for output layer W[40][1]:	mock grad = -0.013277337213624119, computed grad = -0.013277337213541992
Iteration: 54, Checking gradient for words E[630][38]:	mock grad = -0.013278202379451987, computed grad = -0.013278202355633064
Iteration: 54, Checking gradient for hidden W[216][40]:	mock grad = -0.011995867280401029, computed grad = -0.011995867408510970
Iteration: 54, Checking gradient for hidden b[0][20]:	mock grad = -0.173983884811934608, computed grad = -0.173983936011813306
Iteration: 54, Checking gradient for output layer W[111][0]:	mock grad = -0.131021002764408756, computed grad = -0.131021002799531161
Iteration: 55, Checking gradient for words E[25][35]:	mock grad = -0.004904570877317660, computed grad = -0.004904570911108132
Iteration: 55, Checking gradient for hidden W[5][13]:	mock grad = 0.004725903866631409, computed grad = 0.004725903877979679
Iteration: 55, Checking gradient for hidden b[0][42]:	mock grad = 0.101891727760594630, computed grad = 0.101891759615944905
Iteration: 55, Checking gradient for output layer W[7][1]:	mock grad = -0.086300223858803982, computed grad = -0.086300223849173893
Iteration: 56, Checking gradient for words E[435][42]:	mock grad = -0.073706981978616248, computed grad = -0.073706981953233941
Iteration: 56, Checking gradient for hidden W[210][41]:	mock grad = 0.001637988258806899, computed grad = 0.001637988242468412
Iteration: 56, Checking gradient for hidden b[0][37]:	mock grad = -0.090138063700395676, computed grad = -0.090138090101989948
Iteration: 56, Checking gradient for output layer W[16][0]:	mock grad = -0.042430153233019574, computed grad = -0.042430153234676672
Iteration: 57, Checking gradient for words E[225][46]:	mock grad = -0.015652495266860189, computed grad = -0.015652495284505345
Iteration: 57, Checking gradient for hidden W[178][7]:	mock grad = 0.010829259918077039, computed grad = 0.010829259930301718
Iteration: 57, Checking gradient for hidden b[0][34]:	mock grad = -0.034653715839105814, computed grad = -0.034653715428898516
Iteration: 57, Checking gradient for output layer W[65][0]:	mock grad = -0.263728696345832958, computed grad = -0.263728695513268274
Iteration: 58, Checking gradient for words E[44][34]:	mock grad = -0.013064553515740318, computed grad = -0.013064553578460890
Iteration: 58, Checking gradient for hidden W[241][15]:	mock grad = 0.008322415741279787, computed grad = 0.008322415708244430
Iteration: 58, Checking gradient for hidden b[0][48]:	mock grad = -0.138024471710851415, computed grad = -0.138024495062338215
Iteration: 58, Checking gradient for output layer W[23][0]:	mock grad = -0.135496079400498903, computed grad = -0.135496079423474747
current: 60, Cost = 0.709441, Correct(%) = 0.5, time = 4.14332
Iteration: 59, Checking gradient for words E[203][45]:	mock grad = 0.020113208061223187, computed grad = 0.020113208017774532
Iteration: 59, Checking gradient for hidden W[246][35]:	mock grad = -0.001335561547466391, computed grad = -0.001335561545428914
Iteration: 59, Checking gradient for hidden b[0][33]:	mock grad = -0.164075816991904322, computed grad = -0.164075855031649315
Iteration: 59, Checking gradient for output layer W[98][1]:	mock grad = -0.043965819718549959, computed grad = -0.043965819718941125
Iteration: 60, Checking gradient for words E[6][20]:	mock grad = 0.002628124950387889, computed grad = 0.002628124944532326
Iteration: 60, Checking gradient for hidden W[179][48]:	mock grad = -0.003686157350524244, computed grad = -0.003686157505951064
Iteration: 60, Checking gradient for hidden b[0][21]:	mock grad = -0.077400432968943722, computed grad = -0.077400452991663843
Iteration: 60, Checking gradient for output layer W[114][1]:	mock grad = -0.132449184905680806, computed grad = -0.132449184792722718
Iteration: 61, Checking gradient for words E[48][40]:	mock grad = 0.041600040672440386, computed grad = 0.041600040788747850
Iteration: 61, Checking gradient for hidden W[112][18]:	mock grad = 0.034233982420084352, computed grad = 0.034233982443720883
Iteration: 61, Checking gradient for hidden b[0][20]:	mock grad = -0.210908400325282575, computed grad = -0.210908465343879581
Iteration: 61, Checking gradient for output layer W[108][1]:	mock grad = 0.333616551095539116, computed grad = 0.333616552554727719
Iteration: 62, Checking gradient for words E[16][39]:	mock grad = -0.010355461167121938, computed grad = -0.010355461353063160
Iteration: 62, Checking gradient for hidden W[26][13]:	mock grad = -0.004002217943888109, computed grad = -0.004002218007154486
Iteration: 62, Checking gradient for hidden b[0][31]:	mock grad = -0.055485596728965803, computed grad = -0.055485608076911497
Iteration: 62, Checking gradient for output layer W[120][0]:	mock grad = 0.058045300822151535, computed grad = 0.058045300818055603
Iteration: 63, Checking gradient for words E[208][28]:	mock grad = 0.008908061756685015, computed grad = 0.008908061759798826
Iteration: 63, Checking gradient for hidden W[59][45]:	mock grad = -0.004953480333469429, computed grad = -0.004953480364839166
Iteration: 63, Checking gradient for hidden b[0][14]:	mock grad = 0.110338644141227959, computed grad = 0.110338690272866136
Iteration: 63, Checking gradient for output layer W[140][1]:	mock grad = 0.194566309760002554, computed grad = 0.194566309947880156
Iteration: 64, Checking gradient for words E[776][14]:	mock grad = 0.013127847080962596, computed grad = 0.013127847379256811
Iteration: 64, Checking gradient for hidden W[127][39]:	mock grad = 0.008534985398434003, computed grad = 0.008534985518249692
Iteration: 64, Checking gradient for hidden b[0][26]:	mock grad = 0.057603288735497138, computed grad = 0.057603304070291426
Iteration: 64, Checking gradient for output layer W[69][1]:	mock grad = -0.114661270576632557, computed grad = -0.114661270611762012
Iteration: 65, Checking gradient for words E[19][27]:	mock grad = -0.045293553765701766, computed grad = -0.045293553870436987
Iteration: 65, Checking gradient for hidden W[144][45]:	mock grad = -0.000547522677474266, computed grad = -0.000547522712935948
Iteration: 65, Checking gradient for hidden b[0][25]:	mock grad = 0.004467308116262725, computed grad = 0.004467300784268668
Iteration: 65, Checking gradient for output layer W[78][0]:	mock grad = -0.238221307553243555, computed grad = -0.238221306353410239
Iteration: 66, Checking gradient for words E[68][34]:	mock grad = 0.003342863688526876, computed grad = 0.003342863672247923
Iteration: 66, Checking gradient for hidden W[216][28]:	mock grad = 0.017389252681354606, computed grad = 0.017389252683163773
Iteration: 66, Checking gradient for hidden b[0][37]:	mock grad = 0.075453603585295959, computed grad = 0.075453616983640781
Iteration: 66, Checking gradient for output layer W[102][0]:	mock grad = 0.246024768360553203, computed grad = 0.246024767875760553
Iteration: 67, Checking gradient for words E[48][35]:	mock grad = -0.048951836266741822, computed grad = -0.048951836708922815
Iteration: 67, Checking gradient for hidden W[218][45]:	mock grad = 0.001990468901369447, computed grad = 0.001990468906734124
Iteration: 67, Checking gradient for hidden b[0][26]:	mock grad = 0.056062514038446309, computed grad = 0.056062527146169917
Iteration: 67, Checking gradient for output layer W[63][1]:	mock grad = -0.180006859804460184, computed grad = -0.180006859950685827
Iteration: 68, Checking gradient for words E[434][40]:	mock grad = -0.020531783532673487, computed grad = -0.021515350733483339
Iteration: 68, Checking gradient for hidden W[208][5]:	mock grad = 0.010757742546829707, computed grad = 0.010757742545846645
Iteration: 68, Checking gradient for hidden b[0][7]:	mock grad = -0.129542966394113890, computed grad = -0.129542986895328643
Iteration: 68, Checking gradient for output layer W[69][0]:	mock grad = 0.047963634464132099, computed grad = 0.047963634465442467
current: 70, Cost = 0.610835, Correct(%) = 0.5, time = 4.7611
Iteration: 69, Checking gradient for words E[681][5]:	mock grad = -0.038759699045542995, computed grad = -0.038759699241293556
Iteration: 69, Checking gradient for hidden W[172][32]:	mock grad = 0.011478482902727016, computed grad = 0.011478482963467124
Iteration: 69, Checking gradient for hidden b[0][19]:	mock grad = 0.071715308223696539, computed grad = 0.071715318324008126
Iteration: 69, Checking gradient for output layer W[104][0]:	mock grad = -0.073533573585526923, computed grad = -0.073533573570725416
Iteration: 70, Checking gradient for words E[119][47]:	mock grad = -0.015435189373746994, computed grad = -0.015435189360415810
Iteration: 70, Checking gradient for hidden W[81][20]:	mock grad = 0.004071999443300278, computed grad = 0.004071999451647364
Iteration: 70, Checking gradient for hidden b[0][46]:	mock grad = -0.253486288580628827, computed grad = -0.253486358774815068
Iteration: 70, Checking gradient for output layer W[148][1]:	mock grad = 0.230057311987330593, computed grad = 0.230057312206536774
Iteration: 71, Checking gradient for words E[69][7]:	mock grad = -0.001188449274436287, computed grad = -0.001188449169219287
Iteration: 71, Checking gradient for hidden W[75][45]:	mock grad = -0.002321100029401002, computed grad = -0.002321100045764718
Iteration: 71, Checking gradient for hidden b[0][33]:	mock grad = 0.176934076744794666, computed grad = 0.176934123018314571
Iteration: 71, Checking gradient for output layer W[32][0]:	mock grad = 0.150875394233307425, computed grad = 0.150875394320320516
Iteration: 72, Checking gradient for words E[25][13]:	mock grad = 0.040654681092766598, computed grad = 0.040654681188511288
Iteration: 72, Checking gradient for hidden W[90][3]:	mock grad = 0.021775112410471475, computed grad = 0.021775112552298196
Iteration: 72, Checking gradient for hidden b[0][33]:	mock grad = 0.164088012369112413, computed grad = 0.164088054343259848
Iteration: 72, Checking gradient for output layer W[3][1]:	mock grad = 0.068716739911101055, computed grad = 0.068716739914524164
Iteration: 73, Checking gradient for words E[547][17]:	mock grad = 0.106658170850315592, computed grad = 0.106658170973883221
Iteration: 73, Checking gradient for hidden W[148][32]:	mock grad = 0.027116834513341015, computed grad = 0.027116835057529990
Iteration: 73, Checking gradient for hidden b[0][46]:	mock grad = 0.257619779263362769, computed grad = 0.257619859880367508
Iteration: 73, Checking gradient for output layer W[16][1]:	mock grad = -0.009836716113931221, computed grad = -0.009836716113934668
Iteration: 74, Checking gradient for words E[304][49]:	mock grad = 0.050677681046829015, computed grad = 0.050677681214554321
Iteration: 74, Checking gradient for hidden W[198][27]:	mock grad = -0.014801660361496438, computed grad = -0.014801660710377983
Iteration: 74, Checking gradient for hidden b[0][39]:	mock grad = -0.154657681656178436, computed grad = -0.154657744140336201
Iteration: 74, Checking gradient for output layer W[74][1]:	mock grad = -0.279716815391684026, computed grad = -0.279716815893317816
Iteration: 75, Checking gradient for words E[458][24]:	mock grad = 0.000414989260677512, computed grad = 0.000414989313808430
Iteration: 75, Checking gradient for hidden W[127][47]:	mock grad = -0.001730100688879244, computed grad = -0.001730100689875033
Iteration: 75, Checking gradient for hidden b[0][2]:	mock grad = -0.120777789967607685, computed grad = -0.120777846208294556
Iteration: 75, Checking gradient for output layer W[118][0]:	mock grad = 0.007643476099439273, computed grad = 0.007643476099485231
Iteration: 76, Checking gradient for words E[349][43]:	mock grad = 0.016710609649195352, computed grad = 0.016710609714114144
Iteration: 76, Checking gradient for hidden W[246][27]:	mock grad = -0.010718949449206594, computed grad = -0.010718949474422666
Iteration: 76, Checking gradient for hidden b[0][25]:	mock grad = 0.004804551626769182, computed grad = 0.004804542587694137
Iteration: 76, Checking gradient for output layer W[80][0]:	mock grad = -0.099226683580200259, computed grad = -0.099226683581365341
Iteration: 77, Checking gradient for words E[397][34]:	mock grad = -0.076156407220639988, computed grad = -0.076156407281293317
Iteration: 77, Checking gradient for hidden W[208][23]:	mock grad = -0.007122186449159962, computed grad = -0.007122186479782327
Iteration: 77, Checking gradient for hidden b[0][20]:	mock grad = 0.122223568232993074, computed grad = 0.122223596209905694
Iteration: 77, Checking gradient for output layer W[68][1]:	mock grad = 0.153620516388619510, computed grad = 0.153620515759287923
Iteration: 78, Checking gradient for words E[448][5]:	mock grad = -0.005998860074407641, computed grad = -0.005998860106873480
Iteration: 78, Checking gradient for hidden W[4][29]:	mock grad = 0.015162730506224076, computed grad = 0.015162730559830560
Iteration: 78, Checking gradient for hidden b[0][36]:	mock grad = 0.002621680894543399, computed grad = 0.002621665910627809
Iteration: 78, Checking gradient for output layer W[47][0]:	mock grad = 0.055222375711816429, computed grad = 0.055222375711544196
current: 80, Cost = 0.566494, Correct(%) = 0.475, time = 5.25032
Iteration: 79, Checking gradient for words E[104][47]:	mock grad = 0.009802632337907458, computed grad = 0.009802632369703933
Iteration: 79, Checking gradient for hidden W[45][5]:	mock grad = 0.017695306298748381, computed grad = 0.017695306441102933
Iteration: 79, Checking gradient for hidden b[0][41]:	mock grad = -0.042774271207468573, computed grad = -0.042774276865736399
Iteration: 79, Checking gradient for output layer W[143][1]:	mock grad = -0.238431312293119202, computed grad = -0.238431311367637372
Iteration: 80, Checking gradient for words E[10][4]:	mock grad = 0.014477245246280468, computed grad = 0.014477245311291721
Iteration: 80, Checking gradient for hidden W[179][27]:	mock grad = 0.006750740654926091, computed grad = 0.006750740681648741
Iteration: 80, Checking gradient for hidden b[0][35]:	mock grad = -0.019189330087099332, computed grad = -0.019189328344632366
Iteration: 80, Checking gradient for output layer W[58][1]:	mock grad = -0.065342822710312554, computed grad = -0.065342822706931716
Iteration: 81, Checking gradient for words E[444][19]:	mock grad = -0.002190398782020431, computed grad = -0.002190398822734177
Iteration: 81, Checking gradient for hidden W[126][41]:	mock grad = -0.014808466268168008, computed grad = -0.014808466384670945
Iteration: 81, Checking gradient for hidden b[0][24]:	mock grad = -0.140951414876078207, computed grad = -0.140951421780383918
Iteration: 81, Checking gradient for output layer W[112][0]:	mock grad = 0.181376863099880214, computed grad = 0.181376863160533502
Iteration: 82, Checking gradient for words E[249][4]:	mock grad = -0.031968362575518494, computed grad = -0.031968362645550315
Iteration: 82, Checking gradient for hidden W[172][16]:	mock grad = 0.000081317693190197, computed grad = 0.000081317719232425
Iteration: 82, Checking gradient for hidden b[0][14]:	mock grad = -0.115046439146948565, computed grad = -0.115046499466313540
Iteration: 82, Checking gradient for output layer W[89][0]:	mock grad = -0.173019926246131828, computed grad = -0.173019926358847387
Iteration: 83, Checking gradient for words E[217][35]:	mock grad = 0.030316874053637477, computed grad = 0.030316874101027548
Iteration: 83, Checking gradient for hidden W[15][11]:	mock grad = -0.009758418039240713, computed grad = -0.009758418074045904
Iteration: 83, Checking gradient for hidden b[0][37]:	mock grad = 0.075851518334335566, computed grad = 0.075851527746990277
Iteration: 83, Checking gradient for output layer W[120][0]:	mock grad = 0.134142802816872297, computed grad = 0.134142802817901918
Iteration: 84, Checking gradient for words E[7][25]:	mock grad = 0.027131218891307540, computed grad = 0.027131218919678814
Iteration: 84, Checking gradient for hidden W[240][16]:	mock grad = 0.009332170033182763, computed grad = 0.009332170144494502
Iteration: 84, Checking gradient for hidden b[0][12]:	mock grad = 0.115007566217184554, computed grad = 0.115007600188164000
Iteration: 84, Checking gradient for output layer W[127][1]:	mock grad = -0.300040279486601946, computed grad = -0.300040279522845954
Iteration: 85, Checking gradient for words E[107][5]:	mock grad = -0.075818515756553850, computed grad = -0.075818516103632633
Iteration: 85, Checking gradient for hidden W[225][24]:	mock grad = -0.020879227805414846, computed grad = -0.020879227894898992
Iteration: 85, Checking gradient for hidden b[0][38]:	mock grad = -0.112699625451739838, computed grad = -0.112699652617810953
Iteration: 85, Checking gradient for output layer W[4][0]:	mock grad = 0.018413486199519991, computed grad = 0.018413486199650171
Iteration: 86, Checking gradient for words E[25][11]:	mock grad = 0.006606886151228064, computed grad = 0.006606886205026669
Iteration: 86, Checking gradient for hidden W[129][38]:	mock grad = -0.006046583686802887, computed grad = -0.006046583846140165
Iteration: 86, Checking gradient for hidden b[0][31]:	mock grad = -0.062419075776809496, computed grad = -0.062419094670669757
Iteration: 86, Checking gradient for output layer W[45][1]:	mock grad = -0.054790367995660727, computed grad = -0.054790367993641703
Iteration: 87, Checking gradient for words E[184][27]:	mock grad = -0.051382689445489405, computed grad = -0.051382689494198788
Iteration: 87, Checking gradient for hidden W[45][26]:	mock grad = -0.012931280446937254, computed grad = -0.012931280678912513
Iteration: 87, Checking gradient for hidden b[0][4]:	mock grad = 0.027798852402005636, computed grad = 0.027798860120370066
Iteration: 87, Checking gradient for output layer W[40][1]:	mock grad = -0.027656579591028851, computed grad = -0.027656579590521743
Iteration: 88, Checking gradient for words E[124][25]:	mock grad = -0.014651934713671988, computed grad = -0.014651934755452244
Iteration: 88, Checking gradient for hidden W[78][40]:	mock grad = 0.022420580566673287, computed grad = 0.022420580927927106
Iteration: 88, Checking gradient for hidden b[0][5]:	mock grad = 0.022391745201466584, computed grad = 0.022391740334825782
Iteration: 88, Checking gradient for output layer W[29][0]:	mock grad = -0.054694967252078008, computed grad = -0.054694967254509806
current: 90, Cost = 0.744335, Correct(%) = 0.455556, time = 6.20371
Iteration: 89, Checking gradient for words E[652][16]:	mock grad = 0.005082680978762966, computed grad = 0.005082680971256533
Iteration: 89, Checking gradient for hidden W[104][0]:	mock grad = 0.008730353734609508, computed grad = 0.008730353688636507
Iteration: 89, Checking gradient for hidden b[0][20]:	mock grad = -0.159806318480060217, computed grad = -0.159806357091632212
Iteration: 89, Checking gradient for output layer W[146][1]:	mock grad = 0.010730618773935330, computed grad = 0.010730618773903371
Iteration: 90, Checking gradient for words E[289][6]:	mock grad = 0.010554957335928705, computed grad = 0.010554957311514658
Iteration: 90, Checking gradient for hidden W[56][0]:	mock grad = -0.018651029592531287, computed grad = -0.018651029820002822
Iteration: 90, Checking gradient for hidden b[0][44]:	mock grad = -0.023995244438357322, computed grad = -0.023995279736707932
Iteration: 90, Checking gradient for output layer W[45][0]:	mock grad = 0.011440735980861039, computed grad = 0.011440735980854815
Iteration: 91, Checking gradient for words E[97][33]:	mock grad = 0.052529104653864511, computed grad = 0.052529104824724122
Iteration: 91, Checking gradient for hidden W[135][40]:	mock grad = -0.009051873737841998, computed grad = -0.009051873806702439
Iteration: 91, Checking gradient for hidden b[0][23]:	mock grad = 0.192870700764125047, computed grad = 0.192870744660338739
Iteration: 91, Checking gradient for output layer W[132][0]:	mock grad = -0.024308801023109616, computed grad = -0.024308801022167457
Iteration: 92, Checking gradient for words E[752][0]:	mock grad = 0.045206014335497091, computed grad = 0.045206014438898504
Iteration: 92, Checking gradient for hidden W[232][31]:	mock grad = -0.008636161553499111, computed grad = -0.008636161562941537
Iteration: 92, Checking gradient for hidden b[0][43]:	mock grad = -0.016694871972977587, computed grad = -0.016694876522044180
Iteration: 92, Checking gradient for output layer W[18][0]:	mock grad = -0.153641814748584071, computed grad = -0.153641814631508083
Iteration: 93, Checking gradient for words E[594][15]:	mock grad = 0.005650528963840884, computed grad = 0.005650528959538596
Iteration: 93, Checking gradient for hidden W[20][12]:	mock grad = -0.004230994007370370, computed grad = -0.004230994045091101
Iteration: 93, Checking gradient for hidden b[0][27]:	mock grad = -0.065372807409036504, computed grad = -0.065372840390163348
Iteration: 93, Checking gradient for output layer W[56][1]:	mock grad = -0.241535550884408590, computed grad = -0.241535551062887599
Iteration: 94, Checking gradient for words E[439][39]:	mock grad = 0.008685383899809107, computed grad = 0.008685383907352637
Iteration: 94, Checking gradient for hidden W[37][39]:	mock grad = -0.000708587393316584, computed grad = -0.000708587379347402
Iteration: 94, Checking gradient for hidden b[0][24]:	mock grad = 0.151865382501503721, computed grad = 0.151865417266868724
Iteration: 94, Checking gradient for output layer W[61][1]:	mock grad = -0.234575009439297322, computed grad = -0.234575009601466294
Iteration: 95, Checking gradient for words E[354][22]:	mock grad = 0.044996781211559256, computed grad = 0.044996781326723509
Iteration: 95, Checking gradient for hidden W[160][3]:	mock grad = -0.006757400878132369, computed grad = -0.006757400902849257
Iteration: 95, Checking gradient for hidden b[0][20]:	mock grad = -0.177407894381920173, computed grad = -0.177407946335796624
Iteration: 95, Checking gradient for output layer W[140][1]:	mock grad = 0.243497993150876280, computed grad = 0.243497993431999932
Iteration: 96, Checking gradient for words E[683][23]:	mock grad = -0.029678886916295610, computed grad = -0.029678887028550757
Iteration: 96, Checking gradient for hidden W[20][8]:	mock grad = -0.022594229110717290, computed grad = -0.022594229180662274
Iteration: 96, Checking gradient for hidden b[0][16]:	mock grad = 0.027913064336637650, computed grad = 0.027913071349934968
Iteration: 96, Checking gradient for output layer W[27][1]:	mock grad = -0.066843706575892003, computed grad = -0.066843706577166886
Iteration: 97, Checking gradient for words E[31][43]:	mock grad = -0.032100727850759014, computed grad = -0.032100727950883062
Iteration: 97, Checking gradient for hidden W[102][19]:	mock grad = 0.021218387482591083, computed grad = 0.021218387583842153
Iteration: 97, Checking gradient for hidden b[0][1]:	mock grad = 0.199969239624064876, computed grad = 0.199969312094358864
Iteration: 97, Checking gradient for output layer W[15][1]:	mock grad = -0.130500798992072564, computed grad = -0.130500799074691420
Iteration: 98, Checking gradient for words E[177][12]:	mock grad = 0.048665239838929786, computed grad = 0.048665240017312976
Iteration: 98, Checking gradient for hidden W[27][35]:	mock grad = -0.000857556644917601, computed grad = -0.000857556631867377
Iteration: 98, Checking gradient for hidden b[0][2]:	mock grad = -0.106054103712727166, computed grad = -0.106054149609630249
Iteration: 98, Checking gradient for output layer W[2][1]:	mock grad = 0.103269080260814938, computed grad = 0.103269080266863739
current: 100, Cost = 0.684912, Correct(%) = 0.45, time = 6.98832
Iteration: 99, Checking gradient for words E[409][49]:	mock grad = 0.036358496722788125, computed grad = 0.036358496871495297
Iteration: 99, Checking gradient for hidden W[80][8]:	mock grad = 0.010119456866020116, computed grad = 0.010119457213374480
Iteration: 99, Checking gradient for hidden b[0][20]:	mock grad = 0.161897621795525914, computed grad = 0.161897663341059622
Iteration: 99, Checking gradient for output layer W[79][0]:	mock grad = -0.126649390875421819, computed grad = -0.126649390869763318
current: 1, Correct(%) = 0.45, time = 7.03937
Dev start.
Dev finished. Total time taken is: 0.638032
dev:
Accuracy:	P=94/100=0.94
Test start.
Test finished. Total time taken is: 0.551678
test:
Accuracy:	P=52/100=0.52
Exceeds best previous DIS of 0. Saving model file..
open output file error
##### Iteration 1
random: 0, 99
Iteration: 100, Checking gradient for words E[22][19]:	mock grad = 0.039797658481277587, computed grad = 0.039797658446266128
Iteration: 100, Checking gradient for hidden W[34][9]:	mock grad = 0.004496715599072720, computed grad = 0.004496715601216696
Iteration: 100, Checking gradient for hidden b[0][43]:	mock grad = -0.013069620748956678, computed grad = -0.013069624393665482
Iteration: 100, Checking gradient for output layer W[78][0]:	mock grad = -0.200158810173933777, computed grad = -0.200158808354802153
Iteration: 101, Checking gradient for words E[57][1]:	mock grad = -0.001286042599024650, computed grad = -0.001286042611706313
Iteration: 101, Checking gradient for hidden W[120][14]:	mock grad = -0.004430800727539541, computed grad = -0.004430800708473584
Iteration: 101, Checking gradient for hidden b[0][15]:	mock grad = -0.114994670649404274, computed grad = -0.114994694457823246
Iteration: 101, Checking gradient for output layer W[125][1]:	mock grad = 0.011654358849089164, computed grad = 0.011654358848891990
Iteration: 102, Checking gradient for words E[47][1]:	mock grad = 0.005022665152065731, computed grad = 0.005022665151238288
Iteration: 102, Checking gradient for hidden W[1][48]:	mock grad = -0.001020387472017159, computed grad = -0.001020387415405697
Iteration: 102, Checking gradient for hidden b[0][37]:	mock grad = -0.066686079891631334, computed grad = -0.066686098979918659
Iteration: 102, Checking gradient for output layer W[73][1]:	mock grad = -0.050296638506031055, computed grad = -0.050296638490875394
Iteration: 103, Checking gradient for words E[60][13]:	mock grad = -0.007378982691763625, computed grad = -0.007378982707468118
Iteration: 103, Checking gradient for hidden W[25][28]:	mock grad = -0.014847435012288468, computed grad = -0.014847435287161396
Iteration: 103, Checking gradient for hidden b[0][4]:	mock grad = -0.030805710979520118, computed grad = -0.030805721539808300
Iteration: 103, Checking gradient for output layer W[53][1]:	mock grad = -0.163896755883297107, computed grad = -0.163896755910168723
Iteration: 104, Checking gradient for words E[154][33]:	mock grad = -0.009558278859844727, computed grad = -0.009558278970287571
Iteration: 104, Checking gradient for hidden W[124][23]:	mock grad = -0.001982363545594001, computed grad = -0.001982363587841774
Iteration: 104, Checking gradient for hidden b[0][11]:	mock grad = 0.140140660554954888, computed grad = 0.140140697223873595
Iteration: 104, Checking gradient for output layer W[103][1]:	mock grad = 0.184496574550663262, computed grad = 0.184496574468949015
Iteration: 105, Checking gradient for words E[118][13]:	mock grad = 0.008016726731618906, computed grad = 0.008016726796935655
Iteration: 105, Checking gradient for hidden W[52][5]:	mock grad = -0.021043954926192043, computed grad = -0.021043955035669418
Iteration: 105, Checking gradient for hidden b[0][37]:	mock grad = -0.071865822391714573, computed grad = -0.071865846259423077
Iteration: 105, Checking gradient for output layer W[50][1]:	mock grad = -0.247630264017362212, computed grad = -0.247630262666875656
Iteration: 106, Checking gradient for words E[255][9]:	mock grad = -0.006899086655354214, computed grad = -0.006899086661468177
Iteration: 106, Checking gradient for hidden W[148][37]:	mock grad = 0.012533898771399654, computed grad = 0.012533898842235908
Iteration: 106, Checking gradient for hidden b[0][25]:	mock grad = -0.002502039367607845, computed grad = -0.002502032188463045
Iteration: 106, Checking gradient for output layer W[133][0]:	mock grad = -0.053864471104947409, computed grad = -0.053864471073639855
Iteration: 107, Checking gradient for words E[280][44]:	mock grad = -0.004251792333043181, computed grad = -0.004251792372888679
Iteration: 107, Checking gradient for hidden W[157][0]:	mock grad = -0.000053088898943621, computed grad = -0.000053088831371654
Iteration: 107, Checking gradient for hidden b[0][21]:	mock grad = -0.084595681582810833, computed grad = -0.084595706848557745
Iteration: 107, Checking gradient for output layer W[110][1]:	mock grad = -0.153659297903285808, computed grad = -0.153659297828831448
Iteration: 108, Checking gradient for words E[285][9]:	mock grad = -0.062678183603503967, computed grad = -0.062678183705145260
Iteration: 108, Checking gradient for hidden W[165][15]:	mock grad = 0.000160043658936626, computed grad = 0.000160043638892408
Iteration: 108, Checking gradient for hidden b[0][25]:	mock grad = -0.007843473725399619, computed grad = -0.007843469071551944
Iteration: 108, Checking gradient for output layer W[120][0]:	mock grad = -0.102069110700375720, computed grad = -0.102069110644138900
current: 10, Cost = 0.53506, Correct(%) = 0.9, time = 0.771397
Iteration: 109, Checking gradient for words E[25][16]:	mock grad = -0.040863614281239080, computed grad = -0.040863614410696830
Iteration: 109, Checking gradient for hidden W[207][48]:	mock grad = 0.002649321840741958, computed grad = 0.002649321863772410
Iteration: 109, Checking gradient for hidden b[0][18]:	mock grad = 0.079956509947742838, computed grad = 0.079956511427684868
Iteration: 109, Checking gradient for output layer W[127][0]:	mock grad = 0.117583894534367683, computed grad = 0.117583894376074555
Iteration: 110, Checking gradient for words E[273][28]:	mock grad = -0.019928964821974704, computed grad = -0.019928964911024385
Iteration: 110, Checking gradient for hidden W[76][28]:	mock grad = 0.004032707339035602, computed grad = 0.004032707539055112
Iteration: 110, Checking gradient for hidden b[0][1]:	mock grad = 0.127649989224876137, computed grad = 0.127650036497793051
Iteration: 110, Checking gradient for output layer W[43][1]:	mock grad = 0.126887197855696243, computed grad = 0.126887197547565167
Iteration: 111, Checking gradient for words E[325][13]:	mock grad = -0.009086957281523311, computed grad = -0.009086957406619271
Iteration: 111, Checking gradient for hidden W[233][25]:	mock grad = -0.000059251816342254, computed grad = -0.000059251817241115
Iteration: 111, Checking gradient for hidden b[0][43]:	mock grad = -0.015652020941170353, computed grad = -0.015652025033564786
Iteration: 111, Checking gradient for output layer W[18][0]:	mock grad = -0.110521600411306320, computed grad = -0.110521600335136735
Iteration: 112, Checking gradient for words E[350][43]:	mock grad = -0.046795067651927980, computed grad = -0.046795067667104076
Iteration: 112, Checking gradient for hidden W[7][49]:	mock grad = 0.000237555819848634, computed grad = 0.000237555823094505
Iteration: 112, Checking gradient for hidden b[0][40]:	mock grad = -0.123207051096307918, computed grad = -0.123207070465263344
Iteration: 112, Checking gradient for output layer W[93][0]:	mock grad = -0.002009381064205229, computed grad = -0.002009381064205271
Iteration: 113, Checking gradient for words E[363][31]:	mock grad = -0.003828454077092758, computed grad = -0.003828454051726414
Iteration: 113, Checking gradient for hidden W[73][4]:	mock grad = -0.009747734142862008, computed grad = -0.009747734226995392
Iteration: 113, Checking gradient for hidden b[0][45]:	mock grad = -0.011469875537128971, computed grad = -0.011469876204315770
Iteration: 113, Checking gradient for output layer W[134][1]:	mock grad = -0.112387780224709743, computed grad = -0.112387780128299544
Iteration: 114, Checking gradient for words E[156][49]:	mock grad = 0.074366207849674293, computed grad = 0.074366208209019263
Iteration: 114, Checking gradient for hidden W[95][36]:	mock grad = -0.007064681417701113, computed grad = -0.007064681405169624
Iteration: 114, Checking gradient for hidden b[0][13]:	mock grad = 0.205315834360175753, computed grad = 0.205315884850337554
Iteration: 114, Checking gradient for output layer W[140][1]:	mock grad = 0.145804576291319599, computed grad = 0.145804576339195996
Iteration: 115, Checking gradient for words E[102][43]:	mock grad = 0.061914778078986377, computed grad = 0.061914778114910245
Iteration: 115, Checking gradient for hidden W[103][25]:	mock grad = 0.004157840699847704, computed grad = 0.004157840738451714
Iteration: 115, Checking gradient for hidden b[0][46]:	mock grad = 0.194698994268771575, computed grad = 0.194699056015358946
Iteration: 115, Checking gradient for output layer W[70][1]:	mock grad = 0.133944928780882488, computed grad = 0.133944928571675476
Iteration: 116, Checking gradient for words E[97][12]:	mock grad = 0.000450281486041870, computed grad = 0.000450281519779107
Iteration: 116, Checking gradient for hidden W[213][27]:	mock grad = -0.012275399698724598, computed grad = -0.012275399877832159
Iteration: 116, Checking gradient for hidden b[0][0]:	mock grad = 0.245805350704064729, computed grad = 0.245805435496380620
Iteration: 116, Checking gradient for output layer W[40][1]:	mock grad = 0.015580868143683091, computed grad = 0.015580868143615407
Iteration: 117, Checking gradient for words E[166][46]:	mock grad = -0.016016753431880026, computed grad = -0.016016753470607367
Iteration: 117, Checking gradient for hidden W[27][29]:	mock grad = -0.009209026532730480, computed grad = -0.009209026553857885
Iteration: 117, Checking gradient for hidden b[0][7]:	mock grad = -0.095768586271083311, computed grad = -0.095768598595315155
Iteration: 117, Checking gradient for output layer W[38][0]:	mock grad = 0.021514160597424503, computed grad = 0.021514160595976519
Iteration: 118, Checking gradient for words E[198][45]:	mock grad = -0.012406297094091912, computed grad = -0.012406297045512925
Iteration: 118, Checking gradient for hidden W[179][23]:	mock grad = -0.018238994861974334, computed grad = -0.018238995271982094
Iteration: 118, Checking gradient for hidden b[0][30]:	mock grad = -0.001173124326570019, computed grad = -0.001173129605338182
Iteration: 118, Checking gradient for output layer W[137][0]:	mock grad = -0.148109641379734480, computed grad = -0.148109640984638108
current: 20, Cost = 0.546993, Correct(%) = 0.9, time = 1.4293
Iteration: 119, Checking gradient for words E[403][9]:	mock grad = -0.002720826024760203, computed grad = -0.002720826076524768
Iteration: 119, Checking gradient for hidden W[80][30]:	mock grad = -0.003260191178466876, computed grad = -0.003260191336831325
Iteration: 119, Checking gradient for hidden b[0][12]:	mock grad = 0.095552603257720303, computed grad = 0.095552633236801007
Iteration: 119, Checking gradient for output layer W[7][0]:	mock grad = -0.048889392725293135, computed grad = -0.048889392715336419
Iteration: 120, Checking gradient for words E[326][22]:	mock grad = -0.034597965547888965, computed grad = -0.034597965663825919
Iteration: 120, Checking gradient for hidden W[127][25]:	mock grad = 0.000518656715908783, computed grad = 0.000518656722040788
Iteration: 120, Checking gradient for hidden b[0][1]:	mock grad = 0.146621276451097682, computed grad = 0.146621330309585157
Iteration: 120, Checking gradient for output layer W[144][0]:	mock grad = -0.128687890891876489, computed grad = -0.128687890758397927
Iteration: 121, Checking gradient for words E[284][34]:	mock grad = 0.043307502475442572, computed grad = 0.043307502537742959
Iteration: 121, Checking gradient for hidden W[15][29]:	mock grad = -0.004202755184257168, computed grad = -0.004202755184665298
Iteration: 121, Checking gradient for hidden b[0][17]:	mock grad = -0.031743926150795687, computed grad = -0.031743938446873961
Iteration: 121, Checking gradient for output layer W[135][1]:	mock grad = -0.032957199712579932, computed grad = -0.032957199710903613
Iteration: 122, Checking gradient for words E[450][14]:	mock grad = 0.005872543415330611, computed grad = 0.005872543461657755
Iteration: 122, Checking gradient for hidden W[206][17]:	mock grad = -0.002359365100124133, computed grad = -0.002359365120104857
Iteration: 122, Checking gradient for hidden b[0][31]:	mock grad = 0.054026123643302615, computed grad = 0.054026143482335885
Iteration: 122, Checking gradient for output layer W[0][1]:	mock grad = -0.095641842147387024, computed grad = -0.095641842055770740
Iteration: 123, Checking gradient for words E[166][21]:	mock grad = -0.019390031532839913, computed grad = -0.019390031579027581
Iteration: 123, Checking gradient for hidden W[35][15]:	mock grad = -0.011796271817021697, computed grad = -0.011796272034943640
Iteration: 123, Checking gradient for hidden b[0][16]:	mock grad = -0.023144850751444235, computed grad = -0.023144857590150385
Iteration: 123, Checking gradient for output layer W[6][0]:	mock grad = 0.057723515153151883, computed grad = 0.057723515134093274
Iteration: 124, Checking gradient for words E[69][1]:	mock grad = -0.044210844753533873, computed grad = -0.044210844993091677
Iteration: 124, Checking gradient for hidden W[34][13]:	mock grad = -0.024970445852978695, computed grad = -0.024970446047839559
Iteration: 124, Checking gradient for hidden b[0][34]:	mock grad = -0.026949257524278014, computed grad = -0.026949251061970422
Iteration: 124, Checking gradient for output layer W[40][0]:	mock grad = 0.034332003289672119, computed grad = 0.034332003287390646
Iteration: 125, Checking gradient for words E[460][20]:	mock grad = 0.052361886029805849, computed grad = 0.052361886198840232
Iteration: 125, Checking gradient for hidden W[172][41]:	mock grad = 0.003946162140722187, computed grad = 0.003946162151265007
Iteration: 125, Checking gradient for hidden b[0][49]:	mock grad = 0.054800147905487151, computed grad = 0.054800161892596316
Iteration: 125, Checking gradient for output layer W[42][0]:	mock grad = 0.067799995868367269, computed grad = 0.067799995828369722
Iteration: 126, Checking gradient for words E[276][37]:	mock grad = -0.026763465922896579, computed grad = -0.026763465915541663
Iteration: 126, Checking gradient for hidden W[73][1]:	mock grad = -0.042291153601103915, computed grad = -0.042291154281600248
Iteration: 126, Checking gradient for hidden b[0][39]:	mock grad = -0.116427113365458190, computed grad = -0.116427162384544466
Iteration: 126, Checking gradient for output layer W[58][0]:	mock grad = 0.118079159363015940, computed grad = 0.118079159212331267
Iteration: 127, Checking gradient for words E[69][24]:	mock grad = -0.089659457117141628, computed grad = -0.089659457512632434
Iteration: 127, Checking gradient for hidden W[243][10]:	mock grad = 0.018913291146060995, computed grad = 0.018913291292047489
Iteration: 127, Checking gradient for hidden b[0][39]:	mock grad = 0.128309258559444750, computed grad = 0.128309304519155537
Iteration: 127, Checking gradient for output layer W[135][1]:	mock grad = 0.022367423623548266, computed grad = 0.022367423623389979
Iteration: 128, Checking gradient for words E[425][9]:	mock grad = -0.005652832620273163, computed grad = -0.005652832615666824
Iteration: 128, Checking gradient for hidden W[226][39]:	mock grad = 0.009272236056123706, computed grad = 0.009272236098974191
Iteration: 128, Checking gradient for hidden b[0][29]:	mock grad = -0.097691347964390385, computed grad = -0.097691364094732711
Iteration: 128, Checking gradient for output layer W[52][1]:	mock grad = 0.111797523823564671, computed grad = 0.111797523681767888
current: 30, Cost = 0.521989, Correct(%) = 0.933333, time = 2.12308
Iteration: 129, Checking gradient for words E[102][16]:	mock grad = 0.021272486967482962, computed grad = 0.021272486992972080
Iteration: 129, Checking gradient for hidden W[126][36]:	mock grad = 0.009001300237232446, computed grad = 0.009001300193272253
Iteration: 129, Checking gradient for hidden b[0][47]:	mock grad = 0.096689036303332365, computed grad = 0.096689057994069891
Iteration: 129, Checking gradient for output layer W[37][1]:	mock grad = 0.055127881523453492, computed grad = 0.055127881504810690
Iteration: 130, Checking gradient for words E[172][29]:	mock grad = 0.069970949917397274, computed grad = 0.069970950140762184
Iteration: 130, Checking gradient for hidden W[234][14]:	mock grad = 0.000272768085174402, computed grad = 0.000272768147005394
Iteration: 130, Checking gradient for hidden b[0][34]:	mock grad = -0.039100679231962587, computed grad = -0.039100688538107574
Iteration: 130, Checking gradient for output layer W[27][0]:	mock grad = 0.068966155919003569, computed grad = 0.068966155876100152
Iteration: 131, Checking gradient for words E[216][6]:	mock grad = 0.053242539505504727, computed grad = 0.053242539696291474
Iteration: 131, Checking gradient for hidden W[26][42]:	mock grad = 0.003653375284740079, computed grad = 0.003653375311744506
Iteration: 131, Checking gradient for hidden b[0][44]:	mock grad = -0.032152581764588462, computed grad = -0.032152627173815194
Iteration: 131, Checking gradient for output layer W[25][0]:	mock grad = -0.155067830315103183, computed grad = -0.155067830312393795
Iteration: 132, Checking gradient for words E[379][41]:	mock grad = 0.028112938294522216, computed grad = 0.028112938309230704
Iteration: 132, Checking gradient for hidden W[102][10]:	mock grad = 0.001403396184262995, computed grad = 0.001403396186004547
Iteration: 132, Checking gradient for hidden b[0][15]:	mock grad = -0.114208989329012134, computed grad = -0.114209017837235069
Iteration: 132, Checking gradient for output layer W[119][0]:	mock grad = 0.169174125218857219, computed grad = 0.169174124078653454
Iteration: 133, Checking gradient for words E[524][8]:	mock grad = -0.030170169527055091, computed grad = -0.030170169558002696
Iteration: 133, Checking gradient for hidden W[145][44]:	mock grad = 0.021493526236648908, computed grad = 0.021493526377034423
Iteration: 133, Checking gradient for hidden b[0][27]:	mock grad = -0.036601296815763629, computed grad = -0.036601305919925067
Iteration: 133, Checking gradient for output layer W[5][1]:	mock grad = -0.080385625837953167, computed grad = -0.080385625774511832
Iteration: 134, Checking gradient for words E[225][6]:	mock grad = -0.010925421648877442, computed grad = -0.010925421690086477
Iteration: 134, Checking gradient for hidden W[102][17]:	mock grad = -0.003171216886643791, computed grad = -0.003171216896282984
Iteration: 134, Checking gradient for hidden b[0][22]:	mock grad = 0.067955749270487420, computed grad = 0.067955765520597664
Iteration: 134, Checking gradient for output layer W[67][1]:	mock grad = 0.074935938289388826, computed grad = 0.074935938277688685
Iteration: 135, Checking gradient for words E[201][18]:	mock grad = 0.023795817387983753, computed grad = 0.023795817476229345
Iteration: 135, Checking gradient for hidden W[142][43]:	mock grad = -0.000911930494396085, computed grad = -0.000911930494390064
Iteration: 135, Checking gradient for hidden b[0][15]:	mock grad = -0.133044313735286579, computed grad = -0.133044339820968804
Iteration: 135, Checking gradient for output layer W[140][1]:	mock grad = -0.108760101174110169, computed grad = -0.108760101108608176
Iteration: 136, Checking gradient for words E[156][15]:	mock grad = -0.086397744191668568, computed grad = -0.086728010711259615
Iteration: 136, Checking gradient for hidden W[52][40]:	mock grad = 0.010741928808732926, computed grad = 0.010741928925595678
Iteration: 136, Checking gradient for hidden b[0][33]:	mock grad = -0.154169794393643755, computed grad = -0.154169839184907703
Iteration: 136, Checking gradient for output layer W[9][1]:	mock grad = 0.101082791119089244, computed grad = 0.101082791065656430
Iteration: 137, Checking gradient for words E[156][23]:	mock grad = -0.047200018412574263, computed grad = -0.047021623452483598
Iteration: 137, Checking gradient for hidden W[157][1]:	mock grad = -0.000394233537648336, computed grad = -0.000394233568402935
Iteration: 137, Checking gradient for hidden b[0][11]:	mock grad = 0.127766195585521380, computed grad = 0.127766224441808879
Iteration: 137, Checking gradient for output layer W[119][1]:	mock grad = 0.225597762179119421, computed grad = 0.225597761721814477
Iteration: 138, Checking gradient for words E[10][11]:	mock grad = 0.032372021884830016, computed grad = 0.032372021868768440
Iteration: 138, Checking gradient for hidden W[201][18]:	mock grad = -0.001827165717449564, computed grad = -0.001827165758685217
Iteration: 138, Checking gradient for hidden b[0][38]:	mock grad = 0.093186271181322500, computed grad = 0.093186289777420384
Iteration: 138, Checking gradient for output layer W[56][0]:	mock grad = -0.284946960495413570, computed grad = -0.284946959769611874
current: 40, Cost = 0.512073, Correct(%) = 0.95, time = 2.55846
Iteration: 139, Checking gradient for words E[549][41]:	mock grad = 0.001085345779805458, computed grad = 0.001085345786769674
Iteration: 139, Checking gradient for hidden W[127][3]:	mock grad = -0.007182171524344749, computed grad = -0.007182171586551087
Iteration: 139, Checking gradient for hidden b[0][7]:	mock grad = -0.084850652311285746, computed grad = -0.084850656601360111
Iteration: 139, Checking gradient for output layer W[73][1]:	mock grad = -0.079258743086396688, computed grad = -0.079258743024917547
Iteration: 140, Checking gradient for words E[377][46]:	mock grad = -0.029334932041380490, computed grad = -0.029334932163909207
Iteration: 140, Checking gradient for hidden W[9][49]:	mock grad = -0.010704463700605960, computed grad = -0.010704463699056050
Iteration: 140, Checking gradient for hidden b[0][44]:	mock grad = 0.023920209732575248, computed grad = 0.023920236560134436
Iteration: 140, Checking gradient for output layer W[104][1]:	mock grad = 0.118804919608139770, computed grad = 0.118804919413395410
Iteration: 141, Checking gradient for words E[24][12]:	mock grad = -0.066240087152835692, computed grad = -0.066240087377660531
Iteration: 141, Checking gradient for hidden W[162][48]:	mock grad = -0.009994275009250764, computed grad = -0.009994275041441830
Iteration: 141, Checking gradient for hidden b[0][21]:	mock grad = -0.085095396894607767, computed grad = -0.085095419476488895
Iteration: 141, Checking gradient for output layer W[59][0]:	mock grad = -0.242351896807990741, computed grad = -0.242351896728795813
Iteration: 142, Checking gradient for words E[101][21]:	mock grad = 0.005833574143077058, computed grad = 0.005833574165835114
Iteration: 142, Checking gradient for hidden W[31][15]:	mock grad = 0.000494381606430672, computed grad = 0.000494381603518940
Iteration: 142, Checking gradient for hidden b[0][15]:	mock grad = -0.128519559820017282, computed grad = -0.128519573253036562
Iteration: 142, Checking gradient for output layer W[90][1]:	mock grad = -0.020120944610846347, computed grad = -0.020120944610490032
Iteration: 143, Checking gradient for words E[78][33]:	mock grad = -0.016309140048242909, computed grad = -0.016309140076883076
Iteration: 143, Checking gradient for hidden W[230][39]:	mock grad = -0.001085353887597673, computed grad = -0.001085353911586947
Iteration: 143, Checking gradient for hidden b[0][22]:	mock grad = 0.072680003397995740, computed grad = 0.072680019249119127
Iteration: 143, Checking gradient for output layer W[49][1]:	mock grad = 0.068590811670043994, computed grad = 0.068590811671943558
Iteration: 144, Checking gradient for words E[46][26]:	mock grad = 0.005474458736720589, computed grad = 0.005474458740260922
Iteration: 144, Checking gradient for hidden W[50][11]:	mock grad = -0.011445643614715895, computed grad = -0.011445643655523884
Iteration: 144, Checking gradient for hidden b[0][21]:	mock grad = 0.065647208409458369, computed grad = 0.065647220895330663
Iteration: 144, Checking gradient for output layer W[97][0]:	mock grad = 0.187060859195120610, computed grad = 0.187060858424496546
Iteration: 145, Checking gradient for words E[544][35]:	mock grad = -0.009795903723630595, computed grad = -0.009795903729459753
Iteration: 145, Checking gradient for hidden W[231][29]:	mock grad = -0.006252890074331940, computed grad = -0.006252890074164575
Iteration: 145, Checking gradient for hidden b[0][35]:	mock grad = 0.016728192368808958, computed grad = 0.016728190156681935
Iteration: 145, Checking gradient for output layer W[20][0]:	mock grad = -0.112808588274226906, computed grad = -0.112808588236071192
Iteration: 146, Checking gradient for words E[160][38]:	mock grad = -0.015984617595415918, computed grad = -0.015984617634355977
Iteration: 146, Checking gradient for hidden W[76][21]:	mock grad = 0.004333792405053494, computed grad = 0.004333792413293413
Iteration: 146, Checking gradient for hidden b[0][11]:	mock grad = -0.110924868798240039, computed grad = -0.110924899255678833
Iteration: 146, Checking gradient for output layer W[76][0]:	mock grad = -0.108547935083108671, computed grad = -0.108547934890942388
Iteration: 147, Checking gradient for words E[447][48]:	mock grad = 0.015024721195910917, computed grad = 0.015024721198727803
Iteration: 147, Checking gradient for hidden W[86][11]:	mock grad = -0.005461794842731571, computed grad = -0.005461794854801449
Iteration: 147, Checking gradient for hidden b[0][25]:	mock grad = -0.004767040870545802, computed grad = -0.004767034011516495
Iteration: 147, Checking gradient for output layer W[30][0]:	mock grad = -0.020090232451808188, computed grad = -0.020090232451376214
Iteration: 148, Checking gradient for words E[316][48]:	mock grad = -0.008610891314903002, computed grad = -0.008610891317056536
Iteration: 148, Checking gradient for hidden W[148][36]:	mock grad = 0.001482342472741704, computed grad = 0.001482342485273523
Iteration: 148, Checking gradient for hidden b[0][49]:	mock grad = 0.045068479824800223, computed grad = 0.045068489332497935
Iteration: 148, Checking gradient for output layer W[56][0]:	mock grad = -0.201743799354447972, computed grad = -0.201743797669936192
current: 50, Cost = 0.53129, Correct(%) = 0.94, time = 3.44551
Iteration: 149, Checking gradient for words E[38][29]:	mock grad = 0.011800981374210373, computed grad = 0.011800981429562686
Iteration: 149, Checking gradient for hidden W[179][16]:	mock grad = 0.002524971220940753, computed grad = 0.002524971274069696
Iteration: 149, Checking gradient for hidden b[0][41]:	mock grad = -0.046887676993789817, computed grad = -0.046887687294130009
Iteration: 149, Checking gradient for output layer W[62][0]:	mock grad = -0.184738320631983033, computed grad = -0.184738319993019928
Iteration: 150, Checking gradient for words E[48][9]:	mock grad = -0.022245901784789801, computed grad = -0.022245901771965809
Iteration: 150, Checking gradient for hidden W[190][1]:	mock grad = -0.046324610810666300, computed grad = -0.046324611253626427
Iteration: 150, Checking gradient for hidden b[0][40]:	mock grad = 0.156697169826303107, computed grad = 0.156697209383248770
Iteration: 150, Checking gradient for output layer W[119][1]:	mock grad = -0.190106282398794235, computed grad = -0.190106282364844975
Iteration: 151, Checking gradient for words E[131][46]:	mock grad = -0.003595508160814109, computed grad = -0.003595508108297962
Iteration: 151, Checking gradient for hidden W[192][43]:	mock grad = -0.000792919814807114, computed grad = -0.000792919816733125
Iteration: 151, Checking gradient for hidden b[0][44]:	mock grad = 0.047798368622509191, computed grad = 0.047798415127722239
Iteration: 151, Checking gradient for output layer W[50][1]:	mock grad = -0.255289809083070196, computed grad = -0.255289808737966695
Iteration: 152, Checking gradient for words E[107][38]:	mock grad = -0.017260727410706167, computed grad = -0.018952665606970124
Iteration: 152, Checking gradient for hidden W[200][8]:	mock grad = -0.002339354276459371, computed grad = -0.002339354321326359
Iteration: 152, Checking gradient for hidden b[0][4]:	mock grad = -0.035873704047229005, computed grad = -0.035873720725733894
Iteration: 152, Checking gradient for output layer W[101][1]:	mock grad = 0.221166037443598640, computed grad = 0.221166037509623298
Iteration: 153, Checking gradient for words E[624][36]:	mock grad = 0.023051976966115273, computed grad = 0.023051977041729878
Iteration: 153, Checking gradient for hidden W[221][0]:	mock grad = 0.011701305030809017, computed grad = 0.011701305128440994
Iteration: 153, Checking gradient for hidden b[0][26]:	mock grad = -0.042782450490164159, computed grad = -0.042782460512116087
Iteration: 153, Checking gradient for output layer W[1][0]:	mock grad = 0.007113494909394280, computed grad = 0.007113494909298805
Iteration: 154, Checking gradient for words E[354][47]:	mock grad = -0.037899716091693847, computed grad = -0.037899716155149629
Iteration: 154, Checking gradient for hidden W[164][39]:	mock grad = 0.010379340437571827, computed grad = 0.010379340490322174
Iteration: 154, Checking gradient for hidden b[0][6]:	mock grad = -0.054607597052336310, computed grad = -0.054607616681550522
Iteration: 154, Checking gradient for output layer W[131][0]:	mock grad = -0.241725577000362346, computed grad = -0.241725575955867605
Iteration: 155, Checking gradient for words E[58][23]:	mock grad = -0.047139793524442197, computed grad = -0.047139793613740245
Iteration: 155, Checking gradient for hidden W[232][12]:	mock grad = 0.015503038973740946, computed grad = 0.015503039320378977
Iteration: 155, Checking gradient for hidden b[0][44]:	mock grad = 0.022000488754725378, computed grad = 0.022000518257804857
Iteration: 155, Checking gradient for output layer W[30][1]:	mock grad = 0.002476940316520349, computed grad = 0.002476940316496951
Iteration: 156, Checking gradient for words E[48][28]:	mock grad = -0.016933023833043404, computed grad = -0.016933023790775971
Iteration: 156, Checking gradient for hidden W[227][47]:	mock grad = 0.008725175791868267, computed grad = 0.008725175840817643
Iteration: 156, Checking gradient for hidden b[0][9]:	mock grad = 0.087140288800469090, computed grad = 0.087140303683757928
Iteration: 156, Checking gradient for output layer W[16][1]:	mock grad = 0.034236428077705217, computed grad = 0.034236428077454681
Iteration: 157, Checking gradient for words E[109][5]:	mock grad = 0.033637171628742024, computed grad = 0.033637171685862340
Iteration: 157, Checking gradient for hidden W[55][20]:	mock grad = -0.002312884996358289, computed grad = -0.002312885016309531
Iteration: 157, Checking gradient for hidden b[0][44]:	mock grad = -0.015472144884676187, computed grad = -0.015472169737362190
Iteration: 157, Checking gradient for output layer W[103][1]:	mock grad = -0.143678884314657473, computed grad = -0.143678883996159357
Iteration: 158, Checking gradient for words E[58][2]:	mock grad = 0.000355994518197456, computed grad = 0.000355994529532688
Iteration: 158, Checking gradient for hidden W[126][22]:	mock grad = 0.007435597000504668, computed grad = 0.007435597012593758
Iteration: 158, Checking gradient for hidden b[0][45]:	mock grad = 0.013189983373740333, computed grad = 0.013189985708519335
Iteration: 158, Checking gradient for output layer W[89][0]:	mock grad = 0.168393953799361729, computed grad = 0.168393953601327945
current: 60, Cost = 0.613271, Correct(%) = 0.933333, time = 4.13884
Iteration: 159, Checking gradient for words E[225][33]:	mock grad = 0.011319642616247627, computed grad = 0.011319642523408916
Iteration: 159, Checking gradient for hidden W[65][21]:	mock grad = 0.001023712618619665, computed grad = 0.001023712621946540
Iteration: 159, Checking gradient for hidden b[0][36]:	mock grad = -0.012008821605180575, computed grad = -0.012008817104942997
Iteration: 159, Checking gradient for output layer W[138][0]:	mock grad = 0.184621561211961716, computed grad = 0.184621560987208722
Iteration: 160, Checking gradient for words E[97][48]:	mock grad = -0.017056815419247418, computed grad = -0.017056815565439920
Iteration: 160, Checking gradient for hidden W[242][15]:	mock grad = 0.007503031005107186, computed grad = 0.007503031019783657
Iteration: 160, Checking gradient for hidden b[0][24]:	mock grad = -0.118007652036400401, computed grad = -0.118007657421219131
Iteration: 160, Checking gradient for output layer W[57][0]:	mock grad = -0.212072361241710805, computed grad = -0.212072360479753114
Iteration: 161, Checking gradient for words E[656][1]:	mock grad = 0.021446570774852347, computed grad = 0.021446570785465954
Iteration: 161, Checking gradient for hidden W[235][27]:	mock grad = 0.019805290228103178, computed grad = 0.019805290707570523
Iteration: 161, Checking gradient for hidden b[0][43]:	mock grad = 0.020078249653998625, computed grad = 0.020078255578192452
Iteration: 161, Checking gradient for output layer W[65][0]:	mock grad = 0.317969174538634825, computed grad = 0.317969175101600166
Iteration: 162, Checking gradient for words E[377][21]:	mock grad = -0.004847467449498044, computed grad = -0.004847467517366269
Iteration: 162, Checking gradient for hidden W[158][38]:	mock grad = 0.001094006817969895, computed grad = 0.001094006825004273
Iteration: 162, Checking gradient for hidden b[0][44]:	mock grad = -0.009601709208051368, computed grad = -0.009601731423773295
Iteration: 162, Checking gradient for output layer W[52][0]:	mock grad = -0.041645831790115828, computed grad = -0.041645831785675332
Iteration: 163, Checking gradient for words E[41][31]:	mock grad = 0.020355885223310999, computed grad = 0.020355885263584582
Iteration: 163, Checking gradient for hidden W[72][7]:	mock grad = -0.001352553375411070, computed grad = -0.001352553369717085
Iteration: 163, Checking gradient for hidden b[0][37]:	mock grad = -0.078219167860327232, computed grad = -0.078219188211557419
Iteration: 163, Checking gradient for output layer W[63][1]:	mock grad = -0.060225538866809369, computed grad = -0.060225538864100467
Iteration: 164, Checking gradient for words E[423][41]:	mock grad = 0.006332888029758621, computed grad = 0.006332888021381651
Iteration: 164, Checking gradient for hidden W[99][21]:	mock grad = -0.014871621334233698, computed grad = -0.014871621497665719
Iteration: 164, Checking gradient for hidden b[0][45]:	mock grad = 0.013211344040531259, computed grad = 0.013211346242919347
Iteration: 164, Checking gradient for output layer W[65][0]:	mock grad = 0.282185785775046494, computed grad = 0.282185785232194508
Iteration: 165, Checking gradient for words E[166][43]:	mock grad = 0.005673289744434484, computed grad = 0.005673289811003353
Iteration: 165, Checking gradient for hidden W[81][33]:	mock grad = -0.001153021129768339, computed grad = -0.001153021124797662
Iteration: 165, Checking gradient for hidden b[0][38]:	mock grad = 0.080085062103524285, computed grad = 0.080085083561574011
Iteration: 165, Checking gradient for output layer W[145][0]:	mock grad = 0.112441996343165540, computed grad = 0.112441996050398452
Iteration: 166, Checking gradient for words E[216][19]:	mock grad = -0.023541362852541603, computed grad = -0.023541362929492753
Iteration: 166, Checking gradient for hidden W[187][35]:	mock grad = -0.002148537751878177, computed grad = -0.002148537764536947
Iteration: 166, Checking gradient for hidden b[0][17]:	mock grad = 0.023162873859339239, computed grad = 0.023162876986983882
Iteration: 166, Checking gradient for output layer W[125][1]:	mock grad = -0.016880987768197464, computed grad = -0.016880987767790189
Iteration: 167, Checking gradient for words E[64][18]:	mock grad = -0.015392446393702741, computed grad = -0.015392446378661998
Iteration: 167, Checking gradient for hidden W[85][27]:	mock grad = 0.022124047876326269, computed grad = 0.022124048009714833
Iteration: 167, Checking gradient for hidden b[0][41]:	mock grad = 0.055801823684975549, computed grad = 0.055801837895490043
Iteration: 167, Checking gradient for output layer W[145][1]:	mock grad = 0.182092411936007892, computed grad = 0.182092411815129557
Iteration: 168, Checking gradient for words E[480][17]:	mock grad = -0.014722453317850270, computed grad = -0.014722453406307503
Iteration: 168, Checking gradient for hidden W[5][5]:	mock grad = -0.001578821207326442, computed grad = -0.001578821349530484
Iteration: 168, Checking gradient for hidden b[0][0]:	mock grad = 0.225846467965751252, computed grad = 0.225846542730985061
Iteration: 168, Checking gradient for output layer W[115][1]:	mock grad = 0.104668178679490076, computed grad = 0.104668178608540260
current: 70, Cost = 0.487104, Correct(%) = 0.928571, time = 4.74997
Iteration: 169, Checking gradient for words E[25][49]:	mock grad = 0.034255982545694508, computed grad = 0.034255982588728362
Iteration: 169, Checking gradient for hidden W[245][23]:	mock grad = -0.007670806707049671, computed grad = -0.007670806743665907
Iteration: 169, Checking gradient for hidden b[0][32]:	mock grad = -0.069986899270346736, computed grad = -0.069986923222972272
Iteration: 169, Checking gradient for output layer W[18][1]:	mock grad = -0.137528954297083672, computed grad = -0.137528953887215782
Iteration: 170, Checking gradient for words E[184][2]:	mock grad = -0.012326153358499514, computed grad = -0.012326153416319680
Iteration: 170, Checking gradient for hidden W[84][6]:	mock grad = 0.002087713038001482, computed grad = 0.002087713078163674
Iteration: 170, Checking gradient for hidden b[0][48]:	mock grad = -0.138037224660936264, computed grad = -0.138037257447660044
Iteration: 170, Checking gradient for output layer W[46][1]:	mock grad = -0.093713161695818670, computed grad = -0.093713161680531315
Iteration: 171, Checking gradient for words E[686][32]:	mock grad = 0.016925795024436141, computed grad = 0.016925795134585538
Iteration: 171, Checking gradient for hidden W[92][45]:	mock grad = 0.003622970784766810, computed grad = 0.003622970796090098
Iteration: 171, Checking gradient for hidden b[0][32]:	mock grad = -0.082501772431964326, computed grad = -0.082501797019530421
Iteration: 171, Checking gradient for output layer W[43][1]:	mock grad = 0.153621224809052936, computed grad = 0.153621224765038089
Iteration: 172, Checking gradient for words E[403][22]:	mock grad = 0.004369519711611147, computed grad = 0.004369519790436135
Iteration: 172, Checking gradient for hidden W[100][38]:	mock grad = -0.014331847273563181, computed grad = -0.014331847333006676
Iteration: 172, Checking gradient for hidden b[0][45]:	mock grad = 0.012317734460420837, computed grad = 0.012317735688267530
Iteration: 172, Checking gradient for output layer W[38][1]:	mock grad = 0.009214307732485771, computed grad = 0.009214307732489805
Iteration: 173, Checking gradient for words E[166][36]:	mock grad = 0.006009652299632240, computed grad = 0.006009652391517553
Iteration: 173, Checking gradient for hidden W[167][28]:	mock grad = -0.012267290553646948, computed grad = -0.012267290589008155
Iteration: 173, Checking gradient for hidden b[0][23]:	mock grad = 0.228390955848201127, computed grad = 0.228391010059486227
Iteration: 173, Checking gradient for output layer W[5][0]:	mock grad = -0.085684206964253828, computed grad = -0.085684206964609017
Iteration: 174, Checking gradient for words E[68][27]:	mock grad = 0.001636232700552576, computed grad = 0.001636232709360487
Iteration: 174, Checking gradient for hidden W[7][14]:	mock grad = -0.015367555820600121, computed grad = -0.015367555906289739
Iteration: 174, Checking gradient for hidden b[0][36]:	mock grad = 0.009625060690354736, computed grad = 0.009625051757370041
Iteration: 174, Checking gradient for output layer W[108][1]:	mock grad = 0.217884054288075557, computed grad = 0.217884054207461153
Iteration: 175, Checking gradient for words E[694][17]:	mock grad = -0.036268187884214154, computed grad = -0.036268188015114965
Iteration: 175, Checking gradient for hidden W[79][49]:	mock grad = -0.012994186858683587, computed grad = -0.012994186878374741
Iteration: 175, Checking gradient for hidden b[0][37]:	mock grad = -0.078158117767901381, computed grad = -0.078158146689239413
Iteration: 175, Checking gradient for output layer W[97][1]:	mock grad = -0.161068731238389606, computed grad = -0.161068730963158990
Iteration: 176, Checking gradient for words E[557][2]:	mock grad = 0.018885805679380585, computed grad = 0.018885805784726106
Iteration: 176, Checking gradient for hidden W[162][36]:	mock grad = -0.002005480704836415, computed grad = -0.002005480710614308
Iteration: 176, Checking gradient for hidden b[0][10]:	mock grad = 0.126283261881299236, computed grad = 0.126283291472772674
Iteration: 176, Checking gradient for output layer W[149][0]:	mock grad = 0.081528980867051626, computed grad = 0.081528980842851054
Iteration: 177, Checking gradient for words E[94][4]:	mock grad = 0.064277159914277737, computed grad = 0.064277159992701047
Iteration: 177, Checking gradient for hidden W[223][5]:	mock grad = -0.000699137341936273, computed grad = -0.000699137342115353
Iteration: 177, Checking gradient for hidden b[0][46]:	mock grad = 0.166427677075864366, computed grad = 0.166427734257281190
Iteration: 177, Checking gradient for output layer W[143][1]:	mock grad = -0.186911394165906186, computed grad = -0.186911392382630798
Iteration: 178, Checking gradient for words E[320][31]:	mock grad = 0.001231035062609909, computed grad = 0.001231035126302633
Iteration: 178, Checking gradient for hidden W[107][46]:	mock grad = -0.006659706577760716, computed grad = -0.006659706574307731
Iteration: 178, Checking gradient for hidden b[0][3]:	mock grad = 0.053357572699919142, computed grad = 0.053357580526184167
Iteration: 178, Checking gradient for output layer W[63][1]:	mock grad = -0.069754507213581896, computed grad = -0.069754507190313619
current: 80, Cost = 0.49081, Correct(%) = 0.925, time = 5.2353
Iteration: 179, Checking gradient for words E[703][5]:	mock grad = 0.013987887729244308, computed grad = 0.013987887788118266
Iteration: 179, Checking gradient for hidden W[197][7]:	mock grad = -0.004566593526544871, computed grad = -0.004566593535005639
Iteration: 179, Checking gradient for hidden b[0][46]:	mock grad = 0.181519057217027768, computed grad = 0.181519106205522029
Iteration: 179, Checking gradient for output layer W[138][1]:	mock grad = -0.097245396681849838, computed grad = -0.097245396542091397
Iteration: 180, Checking gradient for words E[119][14]:	mock grad = -0.033808350038355695, computed grad = -0.033808349970966142
Iteration: 180, Checking gradient for hidden W[232][21]:	mock grad = -0.002647446632642048, computed grad = -0.002647446641429230
Iteration: 180, Checking gradient for hidden b[0][42]:	mock grad = 0.086720698479203495, computed grad = 0.086720722679450454
Iteration: 180, Checking gradient for output layer W[39][0]:	mock grad = 0.036086359117448730, computed grad = 0.036086359113999926
Iteration: 181, Checking gradient for words E[57][17]:	mock grad = 0.000096529876592033, computed grad = 0.000096529869313436
Iteration: 181, Checking gradient for hidden W[56][17]:	mock grad = -0.004312708363707340, computed grad = -0.004312708379276307
Iteration: 181, Checking gradient for hidden b[0][33]:	mock grad = -0.130462293078836833, computed grad = -0.130462312478092174
Iteration: 181, Checking gradient for output layer W[61][1]:	mock grad = 0.239691855397350384, computed grad = 0.239691854777458024
Iteration: 182, Checking gradient for words E[524][30]:	mock grad = 0.059415066015033879, computed grad = 0.059415066205006473
Iteration: 182, Checking gradient for hidden W[197][19]:	mock grad = -0.002358461451745786, computed grad = -0.002358461475517122
Iteration: 182, Checking gradient for hidden b[0][31]:	mock grad = -0.054294829923973165, computed grad = -0.054294838484099370
Iteration: 182, Checking gradient for output layer W[19][1]:	mock grad = -0.099124433282415936, computed grad = -0.099124433265550857
Iteration: 183, Checking gradient for words E[598][0]:	mock grad = 0.010305953871192486, computed grad = 0.010305953906285224
Iteration: 183, Checking gradient for hidden W[114][18]:	mock grad = 0.015594901259241567, computed grad = 0.015594901436662898
Iteration: 183, Checking gradient for hidden b[0][42]:	mock grad = -0.076144881626105665, computed grad = -0.076144883524786572
Iteration: 183, Checking gradient for output layer W[86][1]:	mock grad = 0.113302322863162441, computed grad = 0.113302322803271377
Iteration: 184, Checking gradient for words E[776][36]:	mock grad = 0.002440434663075752, computed grad = 0.002440434639568059
Iteration: 184, Checking gradient for hidden W[60][44]:	mock grad = -0.038442028964580377, computed grad = -0.038442029572702457
Iteration: 184, Checking gradient for hidden b[0][30]:	mock grad = 0.005529273797710843, computed grad = 0.005529283843728585
Iteration: 184, Checking gradient for output layer W[72][0]:	mock grad = -0.132128390279540042, computed grad = -0.132128390134537282
Iteration: 185, Checking gradient for words E[737][15]:	mock grad = -0.011933878921288610, computed grad = -0.011933878905259264
Iteration: 185, Checking gradient for hidden W[75][29]:	mock grad = 0.005885109943926103, computed grad = 0.005885109951669469
Iteration: 185, Checking gradient for hidden b[0][18]:	mock grad = -0.100738770254016341, computed grad = -0.100738766186785733
Iteration: 185, Checking gradient for output layer W[105][1]:	mock grad = 0.109210798716452562, computed grad = 0.109210798650857727
Iteration: 186, Checking gradient for words E[403][18]:	mock grad = 0.035695654631184848, computed grad = 0.035695654656079240
Iteration: 186, Checking gradient for hidden W[242][44]:	mock grad = 0.032237653028455160, computed grad = 0.032237653254750649
Iteration: 186, Checking gradient for hidden b[0][16]:	mock grad = 0.027678821478416271, computed grad = 0.027678831808315402
Iteration: 186, Checking gradient for output layer W[99][1]:	mock grad = 0.218405146884259604, computed grad = 0.218405146220413821
Iteration: 187, Checking gradient for words E[423][24]:	mock grad = -0.017369420410695557, computed grad = -0.017369420499493967
Iteration: 187, Checking gradient for hidden W[165][35]:	mock grad = -0.003861982526331964, computed grad = -0.003861982540311225
Iteration: 187, Checking gradient for hidden b[0][30]:	mock grad = 0.004890577250671324, computed grad = 0.004890586641667660
Iteration: 187, Checking gradient for output layer W[57][0]:	mock grad = -0.193998339436007861, computed grad = -0.193998338875043808
Iteration: 188, Checking gradient for words E[58][1]:	mock grad = -0.018954168932716620, computed grad = -0.018954169023893255
Iteration: 188, Checking gradient for hidden W[94][40]:	mock grad = -0.009193255699757596, computed grad = -0.009193255817360855
Iteration: 188, Checking gradient for hidden b[0][28]:	mock grad = -0.023160845802838725, computed grad = -0.023160874308800032
Iteration: 188, Checking gradient for output layer W[139][0]:	mock grad = -0.130200736535523998, computed grad = -0.130200736457822402
current: 90, Cost = 0.612999, Correct(%) = 0.933333, time = 6.18849
Iteration: 189, Checking gradient for words E[200][49]:	mock grad = 0.015700520898109804, computed grad = 0.015700520869380424
Iteration: 189, Checking gradient for hidden W[241][0]:	mock grad = 0.006856496161500303, computed grad = 0.006856496147377210
Iteration: 189, Checking gradient for hidden b[0][20]:	mock grad = -0.142600722871588648, computed grad = -0.142600759657584453
Iteration: 189, Checking gradient for output layer W[74][1]:	mock grad = -0.202346485082882666, computed grad = -0.202346484785699832
Iteration: 190, Checking gradient for words E[27][39]:	mock grad = 0.002394402661254347, computed grad = 0.002394402624098867
Iteration: 190, Checking gradient for hidden W[185][28]:	mock grad = 0.017059217916892688, computed grad = 0.017059218064269650
Iteration: 190, Checking gradient for hidden b[0][36]:	mock grad = -0.002224851208210143, computed grad = -0.002224839041017731
Iteration: 190, Checking gradient for output layer W[16][0]:	mock grad = 0.052457964144825286, computed grad = 0.052457964130447919
Iteration: 191, Checking gradient for words E[74][32]:	mock grad = 0.000258474785408414, computed grad = 0.000258474806482297
Iteration: 191, Checking gradient for hidden W[144][49]:	mock grad = 0.003383320413830049, computed grad = 0.003383320433745932
Iteration: 191, Checking gradient for hidden b[0][28]:	mock grad = 0.010046881820879872, computed grad = 0.010046895701618290
Iteration: 191, Checking gradient for output layer W[31][1]:	mock grad = -0.066750615739075148, computed grad = -0.066750615706888880
Iteration: 192, Checking gradient for words E[736][37]:	mock grad = 0.011154679031399883, computed grad = 0.011154679184193555
Iteration: 192, Checking gradient for hidden W[6][3]:	mock grad = -0.016661078615365366, computed grad = -0.016661078866862036
Iteration: 192, Checking gradient for hidden b[0][33]:	mock grad = -0.118538471979179327, computed grad = -0.118538489848332929
Iteration: 192, Checking gradient for output layer W[132][1]:	mock grad = -0.017989405999119246, computed grad = -0.017989405998449344
Iteration: 193, Checking gradient for words E[354][18]:	mock grad = -0.027801216329503831, computed grad = -0.027801216398746637
Iteration: 193, Checking gradient for hidden W[140][30]:	mock grad = 0.002316472910879330, computed grad = 0.002316473039906326
Iteration: 193, Checking gradient for hidden b[0][39]:	mock grad = -0.123512956277493924, computed grad = -0.123513006995619831
Iteration: 193, Checking gradient for output layer W[86][0]:	mock grad = 0.055214367080391735, computed grad = 0.055214367072655500
Iteration: 194, Checking gradient for words E[763][13]:	mock grad = 0.029683236524580270, computed grad = 0.029683236601784842
Iteration: 194, Checking gradient for hidden W[241][48]:	mock grad = 0.006411285620178120, computed grad = 0.006411285670965452
Iteration: 194, Checking gradient for hidden b[0][27]:	mock grad = -0.046224906450287317, computed grad = -0.046224922922382103
Iteration: 194, Checking gradient for output layer W[78][1]:	mock grad = -0.248224858894030564, computed grad = -0.248224857687201456
Iteration: 195, Checking gradient for words E[85][15]:	mock grad = -0.012064056227834019, computed grad = -0.012064056330633911
Iteration: 195, Checking gradient for hidden W[202][34]:	mock grad = 0.021936574773451234, computed grad = 0.021936574997819244
Iteration: 195, Checking gradient for hidden b[0][42]:	mock grad = 0.095467869569654251, computed grad = 0.095467897010797331
Iteration: 195, Checking gradient for output layer W[20][1]:	mock grad = -0.043492384901422376, computed grad = -0.043492384899431608
Iteration: 196, Checking gradient for words E[767][10]:	mock grad = 0.011562706398571265, computed grad = 0.011562706359601218
Iteration: 196, Checking gradient for hidden W[39][14]:	mock grad = -0.010029175157688641, computed grad = -0.010029175180965456
Iteration: 196, Checking gradient for hidden b[0][40]:	mock grad = 0.147498567814485781, computed grad = 0.147498608411769483
Iteration: 196, Checking gradient for output layer W[56][1]:	mock grad = 0.215893383187726329, computed grad = 0.215893382717494364
Iteration: 197, Checking gradient for words E[411][48]:	mock grad = -0.067151540051657577, computed grad = -0.067151540319632969
Iteration: 197, Checking gradient for hidden W[115][27]:	mock grad = 0.044707381397057766, computed grad = 0.044707381688586678
Iteration: 197, Checking gradient for hidden b[0][11]:	mock grad = 0.161565751100767319, computed grad = 0.161565797860632993
Iteration: 197, Checking gradient for output layer W[88][0]:	mock grad = 0.184980785073174392, computed grad = 0.184980785212586651
Iteration: 198, Checking gradient for words E[773][24]:	mock grad = 0.006633976450565182, computed grad = 0.006633976423606231
Iteration: 198, Checking gradient for hidden W[190][36]:	mock grad = -0.014222608816227833, computed grad = -0.014222608892199728
Iteration: 198, Checking gradient for hidden b[0][28]:	mock grad = -0.016110783621237079, computed grad = -0.016110804733801491
Iteration: 198, Checking gradient for output layer W[105][1]:	mock grad = 0.121254737431630844, computed grad = 0.121254737314227631
current: 100, Cost = 0.597632, Correct(%) = 0.93, time = 6.97251
Iteration: 199, Checking gradient for words E[269][38]:	mock grad = 0.009971727842783551, computed grad = 0.009971727823034612
Iteration: 199, Checking gradient for hidden W[184][16]:	mock grad = 0.000380793396870249, computed grad = 0.000380793410302241
Iteration: 199, Checking gradient for hidden b[0][43]:	mock grad = -0.016664802413746127, computed grad = -0.016664807287644922
Iteration: 199, Checking gradient for output layer W[143][0]:	mock grad = 0.275522564191665875, computed grad = 0.275522563242011131
current: 2, Correct(%) = 0.93, time = 7.02412
Dev start.
Dev finished. Total time taken is: 0.63682
dev:
Accuracy:	P=98/100=0.98
Test start.
Test finished. Total time taken is: 0.546361
test:
Accuracy:	P=56/100=0.56
Exceeds best previous DIS of 0.94. Saving model file..
open output file error
##### Iteration 2
random: 0, 99
Iteration: 200, Checking gradient for words E[51][0]:	mock grad = -0.013743755798573121, computed grad = -0.013743755873566724
Iteration: 200, Checking gradient for hidden W[163][16]:	mock grad = -0.006074762177760151, computed grad = -0.006074762214350144
Iteration: 200, Checking gradient for hidden b[0][38]:	mock grad = 0.069051052584201411, computed grad = 0.069051068372943683
Iteration: 200, Checking gradient for output layer W[52][1]:	mock grad = 0.022242981694425845, computed grad = 0.022242981690660069
Iteration: 201, Checking gradient for words E[66][4]:	mock grad = 0.023840297290050882, computed grad = 0.023840297344145098
Iteration: 201, Checking gradient for hidden W[196][32]:	mock grad = -0.013501717397695900, computed grad = -0.013501717584719417
Iteration: 201, Checking gradient for hidden b[0][34]:	mock grad = -0.020798039509800637, computed grad = -0.020798032221532061
Iteration: 201, Checking gradient for output layer W[48][0]:	mock grad = 0.110418963413794202, computed grad = 0.110418963171626583
Iteration: 202, Checking gradient for words E[124][1]:	mock grad = -0.007387231704764075, computed grad = -0.007387231759172512
Iteration: 202, Checking gradient for hidden W[226][18]:	mock grad = 0.007784358470652730, computed grad = 0.007784358503681435
Iteration: 202, Checking gradient for hidden b[0][35]:	mock grad = -0.009757312049107902, computed grad = -0.009757307658714940
Iteration: 202, Checking gradient for output layer W[13][1]:	mock grad = 0.056384356112743061, computed grad = 0.056384356074073098
Iteration: 203, Checking gradient for words E[107][10]:	mock grad = -0.019304685287324652, computed grad = -0.019304685320914502
Iteration: 203, Checking gradient for hidden W[119][14]:	mock grad = 0.020766359981216898, computed grad = 0.020766360053435400
Iteration: 203, Checking gradient for hidden b[0][40]:	mock grad = -0.149018299228254048, computed grad = -0.149018333442547474
Iteration: 203, Checking gradient for output layer W[114][0]:	mock grad = -0.154790242131164213, computed grad = -0.154790242022514568
Iteration: 204, Checking gradient for words E[42][45]:	mock grad = 0.021328542335719014, computed grad = 0.021328542395011300
Iteration: 204, Checking gradient for hidden W[231][8]:	mock grad = -0.013011331010559513, computed grad = -0.013011331160789461
Iteration: 204, Checking gradient for hidden b[0][40]:	mock grad = -0.143524133288386313, computed grad = -0.143524172747629686
Iteration: 204, Checking gradient for output layer W[126][1]:	mock grad = 0.213474463185925334, computed grad = 0.213474462516473590
Iteration: 205, Checking gradient for words E[167][3]:	mock grad = -0.007206405052084808, computed grad = -0.007206405080300254
Iteration: 205, Checking gradient for hidden W[156][26]:	mock grad = -0.008544983551600493, computed grad = -0.008544983578262864
Iteration: 205, Checking gradient for hidden b[0][21]:	mock grad = 0.062246421231981763, computed grad = 0.062246433308105684
Iteration: 205, Checking gradient for output layer W[141][1]:	mock grad = 0.232692812254631232, computed grad = 0.232692810091696178
Iteration: 206, Checking gradient for words E[271][37]:	mock grad = -0.027898030436207533, computed grad = -0.027898030517458428
Iteration: 206, Checking gradient for hidden W[148][40]:	mock grad = -0.008109823530610960, computed grad = -0.008109823557561704
Iteration: 206, Checking gradient for hidden b[0][33]:	mock grad = 0.099809336307543628, computed grad = 0.099809359074009465
Iteration: 206, Checking gradient for output layer W[90][0]:	mock grad = 0.136178012762361300, computed grad = 0.136178011912313268
Iteration: 207, Checking gradient for words E[69][28]:	mock grad = 0.013157581271960872, computed grad = 0.013157581239772159
Iteration: 207, Checking gradient for hidden W[16][17]:	mock grad = 0.002996514256892180, computed grad = 0.002996514292046565
Iteration: 207, Checking gradient for hidden b[0][18]:	mock grad = 0.098864000282250508, computed grad = 0.098864008080727095
Iteration: 207, Checking gradient for output layer W[56][0]:	mock grad = -0.149919895126526814, computed grad = -0.149919895012978172
Iteration: 208, Checking gradient for words E[69][49]:	mock grad = 0.006844499066738230, computed grad = 0.006844499057831552
Iteration: 208, Checking gradient for hidden W[106][24]:	mock grad = -0.010600797429205500, computed grad = -0.010600797398297196
Iteration: 208, Checking gradient for hidden b[0][7]:	mock grad = -0.102316083528175561, computed grad = -0.102316101409238955
Iteration: 208, Checking gradient for output layer W[132][0]:	mock grad = -0.013036887145101428, computed grad = -0.013036887144845197
current: 10, Cost = 0.492941, Correct(%) = 1, time = 0.77117
Iteration: 209, Checking gradient for words E[103][40]:	mock grad = 0.001724857314477912, computed grad = 0.001724857322459914
Iteration: 209, Checking gradient for hidden W[73][7]:	mock grad = 0.003831386062441711, computed grad = 0.003831386066502390
Iteration: 209, Checking gradient for hidden b[0][4]:	mock grad = 0.024653556897458184, computed grad = 0.024653565235753835
Iteration: 209, Checking gradient for output layer W[107][0]:	mock grad = 0.053683659182346410, computed grad = 0.053683659159314404
Iteration: 210, Checking gradient for words E[209][8]:	mock grad = 0.002224093549274553, computed grad = 0.002224093553399313
Iteration: 210, Checking gradient for hidden W[78][28]:	mock grad = 0.000073749614665219, computed grad = 0.000073749586009795
Iteration: 210, Checking gradient for hidden b[0][15]:	mock grad = 0.104437009551316962, computed grad = 0.104437033368635482
Iteration: 210, Checking gradient for output layer W[101][1]:	mock grad = 0.121526837459029480, computed grad = 0.121526836985215561
Iteration: 211, Checking gradient for words E[48][25]:	mock grad = -0.027736087431839973, computed grad = -0.029863316973805055
Iteration: 211, Checking gradient for hidden W[239][6]:	mock grad = 0.005018488802188692, computed grad = 0.005018488830572886
Iteration: 211, Checking gradient for hidden b[0][44]:	mock grad = -0.019330279581730725, computed grad = -0.019330311074912686
Iteration: 211, Checking gradient for output layer W[66][1]:	mock grad = 0.076551818407433192, computed grad = 0.076551818370131419
Iteration: 212, Checking gradient for words E[359][12]:	mock grad = 0.016103740783812714, computed grad = 0.016103740895796647
Iteration: 212, Checking gradient for hidden W[169][17]:	mock grad = 0.002643607674907011, computed grad = 0.002643607685596271
Iteration: 212, Checking gradient for hidden b[0][11]:	mock grad = 0.107678361108032306, computed grad = 0.107678385661135273
Iteration: 212, Checking gradient for output layer W[9][0]:	mock grad = 0.065559911267998316, computed grad = 0.065559911220878536
Iteration: 213, Checking gradient for words E[373][9]:	mock grad = 0.000794972555839824, computed grad = 0.000794972609795837
Iteration: 213, Checking gradient for hidden W[16][47]:	mock grad = -0.009525488213757427, computed grad = -0.009525488228475277
Iteration: 213, Checking gradient for hidden b[0][49]:	mock grad = 0.055527958101708741, computed grad = 0.055527974264445043
Iteration: 213, Checking gradient for output layer W[129][1]:	mock grad = -0.183797137756069873, computed grad = -0.183797136932930261
Iteration: 214, Checking gradient for words E[156][0]:	mock grad = -0.042012966809168439, computed grad = -0.042012966862011641
Iteration: 214, Checking gradient for hidden W[120][48]:	mock grad = 0.003648682402601633, computed grad = 0.003648682482829773
Iteration: 214, Checking gradient for hidden b[0][2]:	mock grad = -0.108549009776581773, computed grad = -0.108549063163133702
Iteration: 214, Checking gradient for output layer W[131][0]:	mock grad = -0.130550966620346376, computed grad = -0.130550966625578607
Iteration: 215, Checking gradient for words E[383][2]:	mock grad = 0.001707567390984988, computed grad = 0.001707567331001795
Iteration: 215, Checking gradient for hidden W[228][17]:	mock grad = -0.001523952101178949, computed grad = -0.001523952103256638
Iteration: 215, Checking gradient for hidden b[0][18]:	mock grad = 0.076364338239931495, computed grad = 0.076364339611224186
Iteration: 215, Checking gradient for output layer W[89][1]:	mock grad = 0.077018192601086000, computed grad = 0.077018192540895411
Iteration: 216, Checking gradient for words E[166][12]:	mock grad = -0.001748292040748556, computed grad = -0.001748292059302534
Iteration: 216, Checking gradient for hidden W[239][4]:	mock grad = 0.008690960978774420, computed grad = 0.008690961069365653
Iteration: 216, Checking gradient for hidden b[0][17]:	mock grad = -0.024271214160986876, computed grad = -0.024271218959060696
Iteration: 216, Checking gradient for output layer W[17][0]:	mock grad = -0.038967011498303172, computed grad = -0.038967011493851296
Iteration: 217, Checking gradient for words E[53][7]:	mock grad = -0.009224352378323086, computed grad = -0.009224352394917170
Iteration: 217, Checking gradient for hidden W[25][2]:	mock grad = -0.006567794199718868, computed grad = -0.006567794272811585
Iteration: 217, Checking gradient for hidden b[0][41]:	mock grad = 0.034393984132852218, computed grad = 0.034393988059266696
Iteration: 217, Checking gradient for output layer W[44][1]:	mock grad = -0.047914829670092640, computed grad = -0.047914829640682013
Iteration: 218, Checking gradient for words E[274][34]:	mock grad = -0.001578397587964631, computed grad = -0.001578397570590698
Iteration: 218, Checking gradient for hidden W[127][1]:	mock grad = 0.002209368538258483, computed grad = 0.002209368568538014
Iteration: 218, Checking gradient for hidden b[0][2]:	mock grad = -0.085072986330064770, computed grad = -0.085073023082235258
Iteration: 218, Checking gradient for output layer W[70][1]:	mock grad = -0.155212825155515155, computed grad = -0.155212824297188551
current: 20, Cost = 0.533083, Correct(%) = 0.95, time = 1.43164
Iteration: 219, Checking gradient for words E[54][40]:	mock grad = 0.053227372636299730, computed grad = 0.053227372489084837
Iteration: 219, Checking gradient for hidden W[175][27]:	mock grad = -0.001036081661409671, computed grad = -0.001036081667937701
Iteration: 219, Checking gradient for hidden b[0][3]:	mock grad = -0.048059651959975191, computed grad = -0.048059660462889606
Iteration: 219, Checking gradient for output layer W[25][1]:	mock grad = 0.149585685557507286, computed grad = 0.149585685224751741
Iteration: 220, Checking gradient for words E[421][14]:	mock grad = -0.043859175016591756, computed grad = -0.043859175028754457
Iteration: 220, Checking gradient for hidden W[75][39]:	mock grad = -0.002187061247105326, computed grad = -0.002187061260859834
Iteration: 220, Checking gradient for hidden b[0][41]:	mock grad = 0.044108800973996498, computed grad = 0.044108809626127343
Iteration: 220, Checking gradient for output layer W[68][0]:	mock grad = 0.265335981705472346, computed grad = 0.265335979396072730
Iteration: 221, Checking gradient for words E[102][0]:	mock grad = 0.051294612858354682, computed grad = 0.051294613049330098
Iteration: 221, Checking gradient for hidden W[192][27]:	mock grad = 0.007994570427272141, computed grad = 0.007994570443868899
Iteration: 221, Checking gradient for hidden b[0][22]:	mock grad = -0.062248406181075744, computed grad = -0.062248422295946244
Iteration: 221, Checking gradient for output layer W[89][0]:	mock grad = 0.152069956608213985, computed grad = 0.152069956370904480
Iteration: 222, Checking gradient for words E[215][33]:	mock grad = 0.033579430308877667, computed grad = 0.033579430448898058
Iteration: 222, Checking gradient for hidden W[135][8]:	mock grad = -0.006526219326064009, computed grad = -0.006526219339660118
Iteration: 222, Checking gradient for hidden b[0][49]:	mock grad = -0.041751551934227082, computed grad = -0.041751550559417420
Iteration: 222, Checking gradient for output layer W[71][1]:	mock grad = -0.049141916730804924, computed grad = -0.049141916708454081
Iteration: 223, Checking gradient for words E[119][42]:	mock grad = 0.036714787892438983, computed grad = 0.036714787967660117
Iteration: 223, Checking gradient for hidden W[0][3]:	mock grad = -0.013014034541097841, computed grad = -0.013014034591365081
Iteration: 223, Checking gradient for hidden b[0][0]:	mock grad = 0.196500965764706548, computed grad = 0.196501030785768527
Iteration: 223, Checking gradient for output layer W[17][0]:	mock grad = -0.041414535435602140, computed grad = -0.041414535423492882
Iteration: 224, Checking gradient for words E[285][5]:	mock grad = 0.013108447302567772, computed grad = 0.013108447383829025
Iteration: 224, Checking gradient for hidden W[107][32]:	mock grad = -0.006954808426995474, computed grad = -0.006954808448228768
Iteration: 224, Checking gradient for hidden b[0][44]:	mock grad = -0.023741074990979438, computed grad = -0.023741109743045764
Iteration: 224, Checking gradient for output layer W[25][0]:	mock grad = -0.122551533591697126, computed grad = -0.122551533403417068
Iteration: 225, Checking gradient for words E[316][46]:	mock grad = -0.041376724514430308, computed grad = -0.041376724599813453
Iteration: 225, Checking gradient for hidden W[46][45]:	mock grad = 0.000945433347110392, computed grad = 0.000945433341198380
Iteration: 225, Checking gradient for hidden b[0][43]:	mock grad = -0.013272519498203073, computed grad = -0.013272522753914703
Iteration: 225, Checking gradient for output layer W[120][1]:	mock grad = -0.093526697338447429, computed grad = -0.093526697197600428
Iteration: 226, Checking gradient for words E[426][29]:	mock grad = -0.000624151679645690, computed grad = -0.000624151673113467
Iteration: 226, Checking gradient for hidden W[165][38]:	mock grad = 0.001601997266120758, computed grad = 0.001601997257434063
Iteration: 226, Checking gradient for hidden b[0][45]:	mock grad = 0.009820013770006630, computed grad = 0.009820014707008043
Iteration: 226, Checking gradient for output layer W[52][1]:	mock grad = -0.101797766425382186, computed grad = -0.101797766217670380
Iteration: 227, Checking gradient for words E[487][18]:	mock grad = -0.018932812965222201, computed grad = -0.018932812983152345
Iteration: 227, Checking gradient for hidden W[110][10]:	mock grad = 0.002103624286764472, computed grad = 0.002103624261135319
Iteration: 227, Checking gradient for hidden b[0][36]:	mock grad = -0.019652383602131174, computed grad = -0.019652386508706619
Iteration: 227, Checking gradient for output layer W[70][1]:	mock grad = 0.115788100996938592, computed grad = 0.115788100947505274
Iteration: 228, Checking gradient for words E[425][22]:	mock grad = -0.004865300956946594, computed grad = -0.004865300975849592
Iteration: 228, Checking gradient for hidden W[78][2]:	mock grad = -0.037430132950544426, computed grad = -0.037430133507085281
Iteration: 228, Checking gradient for hidden b[0][1]:	mock grad = -0.120378291660311332, computed grad = -0.120378313076959648
Iteration: 228, Checking gradient for output layer W[99][0]:	mock grad = -0.203529214842484985, computed grad = -0.203529213107991352
current: 30, Cost = 0.468485, Correct(%) = 0.966667, time = 2.12357
Iteration: 229, Checking gradient for words E[118][0]:	mock grad = 0.007573800556992527, computed grad = 0.007573800569315441
Iteration: 229, Checking gradient for hidden W[16][3]:	mock grad = 0.015784622882486499, computed grad = 0.015784622955102908
Iteration: 229, Checking gradient for hidden b[0][17]:	mock grad = -0.023550787080078850, computed grad = -0.023550794115996414
Iteration: 229, Checking gradient for output layer W[31][0]:	mock grad = -0.050262468401784943, computed grad = -0.050262468377905732
Iteration: 230, Checking gradient for words E[172][29]:	mock grad = 0.019246980331816266, computed grad = 0.019246980430286862
Iteration: 230, Checking gradient for hidden W[59][14]:	mock grad = 0.006310041086060592, computed grad = 0.006310041190200864
Iteration: 230, Checking gradient for hidden b[0][16]:	mock grad = 0.021348316224162334, computed grad = 0.021348322404752880
Iteration: 230, Checking gradient for output layer W[142][0]:	mock grad = 0.106817631191941098, computed grad = 0.106817630959223184
Iteration: 231, Checking gradient for words E[102][33]:	mock grad = -0.006520539846510687, computed grad = -0.006520539833293755
Iteration: 231, Checking gradient for hidden W[16][22]:	mock grad = 0.006337253548205357, computed grad = 0.006337253585562431
Iteration: 231, Checking gradient for hidden b[0][28]:	mock grad = 0.016794793482100001, computed grad = 0.016794814741949805
Iteration: 231, Checking gradient for output layer W[56][1]:	mock grad = 0.157147576237803843, computed grad = 0.157147576150154955
Iteration: 232, Checking gradient for words E[515][36]:	mock grad = -0.005765221019138345, computed grad = -0.005765221036095881
Iteration: 232, Checking gradient for hidden W[151][39]:	mock grad = -0.006528866543065170, computed grad = -0.006528866591515621
Iteration: 232, Checking gradient for hidden b[0][22]:	mock grad = 0.050735072588375951, computed grad = 0.050735084790816266
Iteration: 232, Checking gradient for output layer W[87][0]:	mock grad = -0.060999111661713679, computed grad = -0.060999111595997101
Iteration: 233, Checking gradient for words E[48][43]:	mock grad = -0.097411141225078879, computed grad = -0.097411141598645001
Iteration: 233, Checking gradient for hidden W[197][14]:	mock grad = -0.008780620802617545, computed grad = -0.008780620798285291
Iteration: 233, Checking gradient for hidden b[0][36]:	mock grad = -0.006137399092887019, computed grad = -0.006137413461853213
Iteration: 233, Checking gradient for output layer W[63][1]:	mock grad = -0.040861474158965239, computed grad = -0.040861474143567771
Iteration: 234, Checking gradient for words E[21][40]:	mock grad = -0.071474769717083664, computed grad = -0.071474769905809935
Iteration: 234, Checking gradient for hidden W[238][16]:	mock grad = -0.005035016427623606, computed grad = -0.005035016449214339
Iteration: 234, Checking gradient for hidden b[0][41]:	mock grad = -0.053947716483981889, computed grad = -0.053947731797916364
Iteration: 234, Checking gradient for output layer W[113][1]:	mock grad = -0.159227806079687007, computed grad = -0.159227805854263299
Iteration: 235, Checking gradient for words E[383][21]:	mock grad = 0.016555693805153648, computed grad = 0.016555693883814806
Iteration: 235, Checking gradient for hidden W[2][6]:	mock grad = -0.002181421546454398, computed grad = -0.002181421575465764
Iteration: 235, Checking gradient for hidden b[0][47]:	mock grad = -0.114848711410320270, computed grad = -0.114848743928916808
Iteration: 235, Checking gradient for output layer W[37][1]:	mock grad = -0.034262743156487918, computed grad = -0.034262743153218235
Iteration: 236, Checking gradient for words E[156][41]:	mock grad = 0.015366224349666613, computed grad = 0.015366224338263788
Iteration: 236, Checking gradient for hidden W[191][11]:	mock grad = 0.000335408121165948, computed grad = 0.000335408165393886
Iteration: 236, Checking gradient for hidden b[0][30]:	mock grad = -0.002983898040043176, computed grad = -0.002983897176409634
Iteration: 236, Checking gradient for output layer W[134][0]:	mock grad = 0.149051919871023841, computed grad = 0.149051919645914493
Iteration: 237, Checking gradient for words E[25][23]:	mock grad = 0.002609060755998094, computed grad = 0.002609060726060612
Iteration: 237, Checking gradient for hidden W[205][38]:	mock grad = -0.012524790410972653, computed grad = -0.012524790449806393
Iteration: 237, Checking gradient for hidden b[0][12]:	mock grad = -0.096068712298780223, computed grad = -0.096068745009787793
Iteration: 237, Checking gradient for output layer W[108][1]:	mock grad = 0.254671217037483189, computed grad = 0.254671215327516687
Iteration: 238, Checking gradient for words E[48][16]:	mock grad = -0.040546983131273251, computed grad = -0.040546983199959419
Iteration: 238, Checking gradient for hidden W[25][30]:	mock grad = 0.006743842925316379, computed grad = 0.006743843087631630
Iteration: 238, Checking gradient for hidden b[0][9]:	mock grad = -0.080836002168238608, computed grad = -0.080836011244554862
Iteration: 238, Checking gradient for output layer W[16][1]:	mock grad = -0.021116517168073834, computed grad = -0.021116517167477152
current: 40, Cost = 0.445195, Correct(%) = 0.975, time = 2.55789
Iteration: 239, Checking gradient for words E[95][4]:	mock grad = -0.002530733604538815, computed grad = -0.002530733625305855
Iteration: 239, Checking gradient for hidden W[73][7]:	mock grad = -0.013631409494840874, computed grad = -0.013631409612815676
Iteration: 239, Checking gradient for hidden b[0][27]:	mock grad = -0.044683594546707228, computed grad = -0.044683613585752562
Iteration: 239, Checking gradient for output layer W[62][1]:	mock grad = -0.140680732965664523, computed grad = -0.140680732317669033
Iteration: 240, Checking gradient for words E[156][13]:	mock grad = 0.008756116707753225, computed grad = 0.008756116708159541
Iteration: 240, Checking gradient for hidden W[118][24]:	mock grad = 0.011245626074102288, computed grad = 0.011245626087163476
Iteration: 240, Checking gradient for hidden b[0][20]:	mock grad = -0.121012160127886848, computed grad = -0.121012196294323165
Iteration: 240, Checking gradient for output layer W[133][1]:	mock grad = 0.086510717886234545, computed grad = 0.086510717747412522
Iteration: 241, Checking gradient for words E[25][23]:	mock grad = 0.060497839253215169, computed grad = 0.060497839263400063
Iteration: 241, Checking gradient for hidden W[211][40]:	mock grad = -0.006726046489602400, computed grad = -0.006726046536922253
Iteration: 241, Checking gradient for hidden b[0][49]:	mock grad = 0.036886479049536014, computed grad = 0.036886470434763202
Iteration: 241, Checking gradient for output layer W[47][1]:	mock grad = 0.115955553061708461, computed grad = 0.115955553027219591
Iteration: 242, Checking gradient for words E[6][21]:	mock grad = 0.116759945699096424, computed grad = 0.116759945927991521
Iteration: 242, Checking gradient for hidden W[114][26]:	mock grad = -0.010927666789450896, computed grad = -0.010927666868058024
Iteration: 242, Checking gradient for hidden b[0][37]:	mock grad = 0.079657871302984962, computed grad = 0.079657893226617899
Iteration: 242, Checking gradient for output layer W[119][1]:	mock grad = -0.163291253302777495, computed grad = -0.163291253033443690
Iteration: 243, Checking gradient for words E[560][27]:	mock grad = -0.003051737079651229, computed grad = -0.003051737076207614
Iteration: 243, Checking gradient for hidden W[123][9]:	mock grad = -0.020220466493525713, computed grad = -0.020220466535861382
Iteration: 243, Checking gradient for hidden b[0][39]:	mock grad = 0.133500803012942537, computed grad = 0.133500848825596402
Iteration: 243, Checking gradient for output layer W[12][1]:	mock grad = 0.075324883725969016, computed grad = 0.075324883720842617
Iteration: 244, Checking gradient for words E[25][10]:	mock grad = 0.013748115466283295, computed grad = 0.013748115484919231
Iteration: 244, Checking gradient for hidden W[134][16]:	mock grad = -0.004373059411616476, computed grad = -0.004373059446961052
Iteration: 244, Checking gradient for hidden b[0][23]:	mock grad = -0.154244130266628643, computed grad = -0.154244175224850705
Iteration: 244, Checking gradient for output layer W[10][1]:	mock grad = 0.024850759825195690, computed grad = 0.024850759821088319
Iteration: 245, Checking gradient for words E[102][44]:	mock grad = 0.005262381117532300, computed grad = 0.005262381124719811
Iteration: 245, Checking gradient for hidden W[30][35]:	mock grad = -0.001239172826184021, computed grad = -0.001239172836853606
Iteration: 245, Checking gradient for hidden b[0][28]:	mock grad = 0.018706168580517879, computed grad = 0.018706191296132317
Iteration: 245, Checking gradient for output layer W[82][0]:	mock grad = -0.223366919387069096, computed grad = -0.223366918762521405
Iteration: 246, Checking gradient for words E[109][2]:	mock grad = -0.009104037259205500, computed grad = -0.009104037274400813
Iteration: 246, Checking gradient for hidden W[206][27]:	mock grad = 0.003695612316223507, computed grad = 0.003695612336671957
Iteration: 246, Checking gradient for hidden b[0][29]:	mock grad = -0.087324700153573742, computed grad = -0.087324718144110677
Iteration: 246, Checking gradient for output layer W[24][0]:	mock grad = -0.063653706978250568, computed grad = -0.063653706911874303
Iteration: 247, Checking gradient for words E[48][45]:	mock grad = -0.036909789960376527, computed grad = -0.036909789976325484
Iteration: 247, Checking gradient for hidden W[200][38]:	mock grad = 0.005430349397095302, computed grad = 0.005430349415011701
Iteration: 247, Checking gradient for hidden b[0][45]:	mock grad = 0.008790524455321025, computed grad = 0.008790523518181572
Iteration: 247, Checking gradient for output layer W[124][0]:	mock grad = -0.077463480563338960, computed grad = -0.077463480496756415
Iteration: 248, Checking gradient for words E[166][39]:	mock grad = 0.017005650945728368, computed grad = 0.017005650938001739
Iteration: 248, Checking gradient for hidden W[80][36]:	mock grad = -0.006120725356578705, computed grad = -0.006120725368718181
Iteration: 248, Checking gradient for hidden b[0][13]:	mock grad = -0.126035033160304399, computed grad = -0.126035060614974798
Iteration: 248, Checking gradient for output layer W[122][1]:	mock grad = -0.203104562854744541, computed grad = -0.203104560357642405
current: 50, Cost = 0.464761, Correct(%) = 0.98, time = 3.4475
Iteration: 249, Checking gradient for words E[38][8]:	mock grad = -0.016081064198758543, computed grad = -0.015265321817935294
Iteration: 249, Checking gradient for hidden W[217][15]:	mock grad = 0.007226955744427110, computed grad = 0.007226955753830314
Iteration: 249, Checking gradient for hidden b[0][32]:	mock grad = 0.065639067952594221, computed grad = 0.065639086690797807
Iteration: 249, Checking gradient for output layer W[98][0]:	mock grad = -0.029149822765750510, computed grad = -0.029149822760837642
Iteration: 250, Checking gradient for words E[102][28]:	mock grad = 0.036762265926193471, computed grad = 0.036762265917697580
Iteration: 250, Checking gradient for hidden W[70][14]:	mock grad = -0.019989645619944429, computed grad = -0.019989645727212405
Iteration: 250, Checking gradient for hidden b[0][28]:	mock grad = 0.031060610657052123, computed grad = 0.031060643475338084
Iteration: 250, Checking gradient for output layer W[48][0]:	mock grad = 0.124391481663710390, computed grad = 0.124391481621649619
Iteration: 251, Checking gradient for words E[374][21]:	mock grad = 0.025104768713757331, computed grad = 0.025104768821179627
Iteration: 251, Checking gradient for hidden W[180][17]:	mock grad = -0.000895346518203777, computed grad = -0.000895346522512882
Iteration: 251, Checking gradient for hidden b[0][26]:	mock grad = 0.039156655835814735, computed grad = 0.039156661132468788
Iteration: 251, Checking gradient for output layer W[44][0]:	mock grad = 0.088992009747823619, computed grad = 0.088992009696509292
Iteration: 252, Checking gradient for words E[622][20]:	mock grad = 0.047734954186307021, computed grad = 0.047734954239257471
Iteration: 252, Checking gradient for hidden W[61][27]:	mock grad = -0.025373250258808877, computed grad = -0.025373250348000888
Iteration: 252, Checking gradient for hidden b[0][36]:	mock grad = 0.010442750990269367, computed grad = 0.010442743827065346
Iteration: 252, Checking gradient for output layer W[79][1]:	mock grad = -0.025688524004752189, computed grad = -0.025688524004310317
Iteration: 253, Checking gradient for words E[315][38]:	mock grad = -0.006467739242177473, computed grad = -0.006467739287808835
Iteration: 253, Checking gradient for hidden W[126][36]:	mock grad = -0.001945930620250724, computed grad = -0.001945930575304610
Iteration: 253, Checking gradient for hidden b[0][37]:	mock grad = 0.061752265170744725, computed grad = 0.061752274113813101
Iteration: 253, Checking gradient for output layer W[38][1]:	mock grad = -0.008766625261585892, computed grad = -0.008766625261558662
Iteration: 254, Checking gradient for words E[102][47]:	mock grad = 0.015222454809643571, computed grad = 0.015222454821620046
Iteration: 254, Checking gradient for hidden W[207][18]:	mock grad = -0.022041107156728357, computed grad = -0.022041107315540008
Iteration: 254, Checking gradient for hidden b[0][42]:	mock grad = 0.079362807122562717, computed grad = 0.079362831901247655
Iteration: 254, Checking gradient for output layer W[142][0]:	mock grad = -0.148111046149063119, computed grad = -0.148111045561177684
Iteration: 255, Checking gradient for words E[58][20]:	mock grad = 0.016812364234702892, computed grad = 0.016812364349162293
Iteration: 255, Checking gradient for hidden W[205][7]:	mock grad = -0.011633271756544827, computed grad = -0.011633271788060173
Iteration: 255, Checking gradient for hidden b[0][28]:	mock grad = -0.008215836082403616, computed grad = -0.008215848644653521
Iteration: 255, Checking gradient for output layer W[98][0]:	mock grad = -0.053402959374226899, computed grad = -0.053402959342242776
Iteration: 256, Checking gradient for words E[102][0]:	mock grad = -0.015852642161418018, computed grad = -0.015852642263115533
Iteration: 256, Checking gradient for hidden W[41][12]:	mock grad = 0.004916407912169962, computed grad = 0.004916407910748160
Iteration: 256, Checking gradient for hidden b[0][11]:	mock grad = 0.135195481633876291, computed grad = 0.135195516315356007
Iteration: 256, Checking gradient for output layer W[80][0]:	mock grad = 0.080071228372446956, computed grad = 0.080071228356797350
Iteration: 257, Checking gradient for words E[371][23]:	mock grad = 0.000701914260881065, computed grad = 0.000701914262638016
Iteration: 257, Checking gradient for hidden W[134][38]:	mock grad = -0.017478691894667886, computed grad = -0.017478692025102258
Iteration: 257, Checking gradient for hidden b[0][45]:	mock grad = -0.009036073350149332, computed grad = -0.009036073237635486
Iteration: 257, Checking gradient for output layer W[43][0]:	mock grad = 0.141419674215825797, computed grad = 0.141419673702374404
Iteration: 258, Checking gradient for words E[530][31]:	mock grad = 0.018728759778707804, computed grad = 0.018728759818803255
Iteration: 258, Checking gradient for hidden W[175][4]:	mock grad = 0.006583710459084902, computed grad = 0.006583710510270917
Iteration: 258, Checking gradient for hidden b[0][22]:	mock grad = -0.055229274924739613, computed grad = -0.055229283125378364
Iteration: 258, Checking gradient for output layer W[2][1]:	mock grad = 0.044465714624020158, computed grad = 0.044465714616269886
current: 60, Cost = 0.556006, Correct(%) = 0.983333, time = 4.14371
Iteration: 259, Checking gradient for words E[203][13]:	mock grad = 0.000634248847108942, computed grad = 0.000634248865880175
Iteration: 259, Checking gradient for hidden W[70][43]:	mock grad = 0.000537652432353042, computed grad = 0.000537652432165028
Iteration: 259, Checking gradient for hidden b[0][14]:	mock grad = -0.075018570937446238, computed grad = -0.075018603514976523
Iteration: 259, Checking gradient for output layer W[88][1]:	mock grad = 0.123932236106294447, computed grad = 0.123932235959243173
Iteration: 260, Checking gradient for words E[653][25]:	mock grad = -0.029354646319867950, computed grad = -0.029354646426292017
Iteration: 260, Checking gradient for hidden W[27][28]:	mock grad = 0.007789633422350395, computed grad = 0.007789633401568860
Iteration: 260, Checking gradient for hidden b[0][21]:	mock grad = -0.070201353047916193, computed grad = -0.070201370921273945
Iteration: 260, Checking gradient for output layer W[120][1]:	mock grad = -0.163294882105313732, computed grad = -0.163294881636061678
Iteration: 261, Checking gradient for words E[269][37]:	mock grad = -0.034931353243006225, computed grad = -0.034931353426339698
Iteration: 261, Checking gradient for hidden W[62][1]:	mock grad = 0.016563294019833474, computed grad = 0.016563294139607936
Iteration: 261, Checking gradient for hidden b[0][33]:	mock grad = 0.145248929701524698, computed grad = 0.145248959948635292
Iteration: 261, Checking gradient for output layer W[72][1]:	mock grad = -0.075577356033973420, computed grad = -0.075577356029031068
Iteration: 262, Checking gradient for words E[400][49]:	mock grad = -0.045812945595857801, computed grad = -0.047155143165350341
Iteration: 262, Checking gradient for hidden W[243][44]:	mock grad = -0.025676228797288925, computed grad = -0.025676229154610219
Iteration: 262, Checking gradient for hidden b[0][46]:	mock grad = 0.197844900981736238, computed grad = 0.197844962856989742
Iteration: 262, Checking gradient for output layer W[38][0]:	mock grad = 0.000753791756791244, computed grad = 0.000753791756812706
Iteration: 263, Checking gradient for words E[160][41]:	mock grad = 0.008373091542390743, computed grad = 0.008373091536208792
Iteration: 263, Checking gradient for hidden W[206][12]:	mock grad = 0.009264645300766094, computed grad = 0.009264645357259172
Iteration: 263, Checking gradient for hidden b[0][19]:	mock grad = 0.054657035688132627, computed grad = 0.054657031326160239
Iteration: 263, Checking gradient for output layer W[49][1]:	mock grad = -0.088577246954213429, computed grad = -0.088577246915742966
Iteration: 264, Checking gradient for words E[18][24]:	mock grad = -0.029671928534480418, computed grad = -0.029671928607263399
Iteration: 264, Checking gradient for hidden W[86][23]:	mock grad = -0.000031100199193723, computed grad = -0.000031100179160876
Iteration: 264, Checking gradient for hidden b[0][23]:	mock grad = -0.192040705341678386, computed grad = -0.192040761348505729
Iteration: 264, Checking gradient for output layer W[127][1]:	mock grad = 0.169170502518578569, computed grad = 0.169170502198128597
Iteration: 265, Checking gradient for words E[668][27]:	mock grad = -0.005236369937017260, computed grad = -0.005236369919643171
Iteration: 265, Checking gradient for hidden W[27][40]:	mock grad = -0.013461625203492478, computed grad = -0.015419052424719202
Iteration: 265, Checking gradient for hidden b[0][28]:	mock grad = 0.027893430581987788, computed grad = 0.027893456713506792
Iteration: 265, Checking gradient for output layer W[33][1]:	mock grad = 0.020610106642698911, computed grad = 0.020610106639943886
Iteration: 266, Checking gradient for words E[296][15]:	mock grad = 0.008630996651520784, computed grad = 0.008630996660590976
Iteration: 266, Checking gradient for hidden W[201][43]:	mock grad = 0.003782223649217986, computed grad = 0.003782223740887218
Iteration: 266, Checking gradient for hidden b[0][45]:	mock grad = -0.011376145313629626, computed grad = -0.011376147302218347
Iteration: 266, Checking gradient for output layer W[124][0]:	mock grad = 0.056581967203955807, computed grad = 0.056581967177708928
Iteration: 267, Checking gradient for words E[69][46]:	mock grad = 0.026128560082661068, computed grad = 0.026128560155780297
Iteration: 267, Checking gradient for hidden W[219][15]:	mock grad = -0.007685985315086796, computed grad = -0.007685985329741835
Iteration: 267, Checking gradient for hidden b[0][30]:	mock grad = -0.001910788342773806, computed grad = -0.001910794712601857
Iteration: 267, Checking gradient for output layer W[40][0]:	mock grad = -0.035290967738133006, computed grad = -0.035290967735306898
Iteration: 268, Checking gradient for words E[664][18]:	mock grad = -0.032873873321487279, computed grad = -0.032873873401763169
Iteration: 268, Checking gradient for hidden W[136][48]:	mock grad = 0.014578654238289745, computed grad = 0.014578654335148981
Iteration: 268, Checking gradient for hidden b[0][11]:	mock grad = 0.113182529829863876, computed grad = 0.113182558065106922
Iteration: 268, Checking gradient for output layer W[58][1]:	mock grad = -0.048516572823897874, computed grad = -0.048516572807814254
current: 70, Cost = 0.428421, Correct(%) = 0.985714, time = 4.75817
Iteration: 269, Checking gradient for words E[118][0]:	mock grad = 0.004298609349556726, computed grad = 0.004298609289414659
Iteration: 269, Checking gradient for hidden W[207][43]:	mock grad = 0.002474632242122077, computed grad = 0.002474632262365913
Iteration: 269, Checking gradient for hidden b[0][44]:	mock grad = 0.009170985456136149, computed grad = 0.009170999046992315
Iteration: 269, Checking gradient for output layer W[135][0]:	mock grad = 0.016128598900405633, computed grad = 0.016128598899320740
Iteration: 270, Checking gradient for words E[335][17]:	mock grad = -0.036330565750197774, computed grad = -0.036330565682322541
Iteration: 270, Checking gradient for hidden W[55][16]:	mock grad = -0.001137808990858691, computed grad = -0.001137808946896968
Iteration: 270, Checking gradient for hidden b[0][2]:	mock grad = -0.093486933035291653, computed grad = -0.093486973720999567
Iteration: 270, Checking gradient for output layer W[117][0]:	mock grad = -0.198324171304331554, computed grad = -0.198324170873479116
Iteration: 271, Checking gradient for words E[2][5]:	mock grad = 0.000884438081993899, computed grad = -0.000975127712974076
Iteration: 271, Checking gradient for hidden W[51][39]:	mock grad = -0.013781777493115932, computed grad = -0.013781777699290659
Iteration: 271, Checking gradient for hidden b[0][34]:	mock grad = 0.035623373165594163, computed grad = 0.035623377398228434
Iteration: 271, Checking gradient for output layer W[4][0]:	mock grad = 0.032400188014369835, computed grad = 0.032400188012568186
Iteration: 272, Checking gradient for words E[689][46]:	mock grad = 0.012035036532687116, computed grad = 0.012035036515485244
Iteration: 272, Checking gradient for hidden W[212][23]:	mock grad = 0.014528066796082317, computed grad = 0.014528066896250790
Iteration: 272, Checking gradient for hidden b[0][4]:	mock grad = -0.034958029428822091, computed grad = -0.034958048111753966
Iteration: 272, Checking gradient for output layer W[19][1]:	mock grad = 0.040915758991344475, computed grad = 0.040915758986472830
Iteration: 273, Checking gradient for words E[547][13]:	mock grad = -0.024475198072637649, computed grad = -0.024475198158616568
Iteration: 273, Checking gradient for hidden W[174][19]:	mock grad = -0.006567424667869304, computed grad = -0.006567424679006094
Iteration: 273, Checking gradient for hidden b[0][4]:	mock grad = 0.031652880323806976, computed grad = 0.031652892705947705
Iteration: 273, Checking gradient for output layer W[0][0]:	mock grad = -0.140589231180987539, computed grad = -0.140589231124930214
Iteration: 274, Checking gradient for words E[34][43]:	mock grad = -0.038894140567902191, computed grad = -0.038894140562186617
Iteration: 274, Checking gradient for hidden W[138][43]:	mock grad = 0.002261606793896309, computed grad = 0.002261606795308400
Iteration: 274, Checking gradient for hidden b[0][25]:	mock grad = -0.006929412375511479, computed grad = -0.006929405985932836
Iteration: 274, Checking gradient for output layer W[52][0]:	mock grad = -0.014903001785027126, computed grad = -0.014903001784954118
Iteration: 275, Checking gradient for words E[306][38]:	mock grad = -0.006419608789109965, computed grad = -0.006419608827474006
Iteration: 275, Checking gradient for hidden W[158][2]:	mock grad = -0.025791106549100107, computed grad = -0.025791106907241360
Iteration: 275, Checking gradient for hidden b[0][31]:	mock grad = 0.052362604396449797, computed grad = 0.052362623895155710
Iteration: 275, Checking gradient for output layer W[28][0]:	mock grad = 0.097113018417083197, computed grad = 0.097113018292093484
Iteration: 276, Checking gradient for words E[82][7]:	mock grad = -0.003642585763352635, computed grad = -0.003642585796449655
Iteration: 276, Checking gradient for hidden W[118][48]:	mock grad = -0.010208143553736004, computed grad = -0.010208143651807241
Iteration: 276, Checking gradient for hidden b[0][21]:	mock grad = -0.073222927576277463, computed grad = -0.073222947059221888
Iteration: 276, Checking gradient for output layer W[128][1]:	mock grad = -0.044689424569643865, computed grad = -0.044689424561839150
Iteration: 277, Checking gradient for words E[94][45]:	mock grad = 0.045491753602716622, computed grad = 0.045491753509466243
Iteration: 277, Checking gradient for hidden W[248][12]:	mock grad = 0.000030469509082476, computed grad = 0.000030469509174330
Iteration: 277, Checking gradient for hidden b[0][20]:	mock grad = 0.105507784490033174, computed grad = 0.105507807903829562
Iteration: 277, Checking gradient for output layer W[14][1]:	mock grad = -0.003074058566032711, computed grad = -0.003074058566008050
Iteration: 278, Checking gradient for words E[215][29]:	mock grad = -0.005268694801330298, computed grad = -0.005268694813527626
Iteration: 278, Checking gradient for hidden W[241][16]:	mock grad = 0.005461810052009852, computed grad = 0.005461810128198581
Iteration: 278, Checking gradient for hidden b[0][27]:	mock grad = -0.045160421490331171, computed grad = -0.045160439140023968
Iteration: 278, Checking gradient for output layer W[108][0]:	mock grad = -0.220402023523080803, computed grad = -0.220402022110462420
current: 80, Cost = 0.444846, Correct(%) = 0.9875, time = 5.24284
Iteration: 279, Checking gradient for words E[396][24]:	mock grad = -0.017099910362361470, computed grad = -0.017099910433852585
Iteration: 279, Checking gradient for hidden W[34][13]:	mock grad = 0.016779284491508228, computed grad = 0.016779284563958961
Iteration: 279, Checking gradient for hidden b[0][12]:	mock grad = 0.082797910458676238, computed grad = 0.082797934640839219
Iteration: 279, Checking gradient for output layer W[4][0]:	mock grad = -0.026233593213548234, computed grad = -0.026233593209304750
Iteration: 280, Checking gradient for words E[306][15]:	mock grad = 0.009965670734912724, computed grad = 0.009965670766171593
Iteration: 280, Checking gradient for hidden W[149][19]:	mock grad = -0.016752365026906357, computed grad = -0.016752365034326509
Iteration: 280, Checking gradient for hidden b[0][16]:	mock grad = -0.023365263654706592, computed grad = -0.023365271789773157
Iteration: 280, Checking gradient for output layer W[3][1]:	mock grad = 0.029066905640751095, computed grad = 0.029066905637401063
Iteration: 281, Checking gradient for words E[338][38]:	mock grad = -0.012242605880952162, computed grad = -0.012242606107750884
Iteration: 281, Checking gradient for hidden W[167][27]:	mock grad = 0.003456713738747563, computed grad = -0.000874464690521204
Iteration: 281, Checking gradient for hidden b[0][14]:	mock grad = -0.092675207547021632, computed grad = -0.092675261245452598
Iteration: 281, Checking gradient for output layer W[49][1]:	mock grad = 0.065814234059713428, computed grad = 0.065814234030401667
Iteration: 282, Checking gradient for words E[10][27]:	mock grad = -0.037859662336714450, computed grad = -0.037859662468009384
Iteration: 282, Checking gradient for hidden W[73][17]:	mock grad = -0.003720003134144445, computed grad = -0.003720003169105440
Iteration: 282, Checking gradient for hidden b[0][41]:	mock grad = -0.047109952524770904, computed grad = -0.047109961190102516
Iteration: 282, Checking gradient for output layer W[121][1]:	mock grad = -0.171111814909197246, computed grad = -0.171111814590193895
Iteration: 283, Checking gradient for words E[34][39]:	mock grad = -0.006042199250488878, computed grad = -0.006042199298167059
Iteration: 283, Checking gradient for hidden W[82][27]:	mock grad = -0.003793343499092217, computed grad = -0.003793343533264174
Iteration: 283, Checking gradient for hidden b[0][22]:	mock grad = 0.047395885622691925, computed grad = 0.047395885390745435
Iteration: 283, Checking gradient for output layer W[74][1]:	mock grad = 0.198202096859056986, computed grad = 0.198202096289220059
Iteration: 284, Checking gradient for words E[464][14]:	mock grad = -0.015217511274978612, computed grad = -0.015217511300978242
Iteration: 284, Checking gradient for hidden W[5][27]:	mock grad = 0.004735542161016237, computed grad = 0.004735542161606010
Iteration: 284, Checking gradient for hidden b[0][6]:	mock grad = 0.051030049657341259, computed grad = 0.051030067955859744
Iteration: 284, Checking gradient for output layer W[128][0]:	mock grad = 0.065616060478446148, computed grad = 0.065616060442503579
Iteration: 285, Checking gradient for words E[48][12]:	mock grad = -0.028664778600695495, computed grad = -0.028664778547425933
Iteration: 285, Checking gradient for hidden W[41][8]:	mock grad = -0.020447767712428178, computed grad = -0.020447767700076072
Iteration: 285, Checking gradient for hidden b[0][28]:	mock grad = -0.018351025283691591, computed grad = -0.018351048131815005
Iteration: 285, Checking gradient for output layer W[82][0]:	mock grad = 0.260033703137318994, computed grad = 0.260033701158671637
Iteration: 286, Checking gradient for words E[10][8]:	mock grad = 0.040966367652439661, computed grad = 0.040966367658420336
Iteration: 286, Checking gradient for hidden W[234][38]:	mock grad = -0.011347626315716397, computed grad = -0.011347626436315185
Iteration: 286, Checking gradient for hidden b[0][19]:	mock grad = -0.060362987690132375, computed grad = -0.060362998748589733
Iteration: 286, Checking gradient for output layer W[59][0]:	mock grad = -0.159077881228597562, computed grad = -0.159077880763023954
Iteration: 287, Checking gradient for words E[517][44]:	mock grad = 0.039919238794483558, computed grad = 0.039919238871769486
Iteration: 287, Checking gradient for hidden W[238][39]:	mock grad = -0.023593550452727374, computed grad = -0.023593550652645547
Iteration: 287, Checking gradient for hidden b[0][42]:	mock grad = -0.080111606533150859, computed grad = -0.080111623385082889
Iteration: 287, Checking gradient for output layer W[94][1]:	mock grad = 0.210239128996980718, computed grad = 0.210239127810566806
Iteration: 288, Checking gradient for words E[95][13]:	mock grad = 0.001360281169815458, computed grad = 0.001360281184219142
Iteration: 288, Checking gradient for hidden W[2][16]:	mock grad = -0.005723749641028952, computed grad = -0.005723749698056091
Iteration: 288, Checking gradient for hidden b[0][20]:	mock grad = -0.139262388936689518, computed grad = -0.139262430078915367
Iteration: 288, Checking gradient for output layer W[83][0]:	mock grad = 0.164504900076467564, computed grad = 0.164504899672285265
current: 90, Cost = 0.540905, Correct(%) = 0.988889, time = 6.19115
Iteration: 289, Checking gradient for words E[79][8]:	mock grad = 0.025962064053919320, computed grad = 0.025962064112959930
Iteration: 289, Checking gradient for hidden W[116][41]:	mock grad = -0.001521709095786417, computed grad = -0.001521709120157228
Iteration: 289, Checking gradient for hidden b[0][24]:	mock grad = 0.124787017619254481, computed grad = 0.124787049321255075
Iteration: 289, Checking gradient for output layer W[47][0]:	mock grad = 0.083421492421187349, computed grad = 0.083421492368047037
Iteration: 290, Checking gradient for words E[166][41]:	mock grad = 0.022760917877506781, computed grad = 0.022760917911774557
Iteration: 290, Checking gradient for hidden W[141][6]:	mock grad = -0.000276491290418512, computed grad = -0.000276491281441545
Iteration: 290, Checking gradient for hidden b[0][5]:	mock grad = 0.003593686195790813, computed grad = 0.003593707829115316
Iteration: 290, Checking gradient for output layer W[147][1]:	mock grad = -0.092819558899004395, computed grad = -0.092819558763304277
Iteration: 291, Checking gradient for words E[48][16]:	mock grad = -0.055211840412877633, computed grad = -0.055211840614490387
Iteration: 291, Checking gradient for hidden W[117][35]:	mock grad = -0.004516223228306915, computed grad = -0.004516223248575596
Iteration: 291, Checking gradient for hidden b[0][43]:	mock grad = -0.014795659020849783, computed grad = -0.014795663490437337
Iteration: 291, Checking gradient for output layer W[72][0]:	mock grad = -0.074482347123761006, computed grad = -0.074482347060917789
Iteration: 292, Checking gradient for words E[109][11]:	mock grad = -0.024482621756388712, computed grad = -0.024482621803236751
Iteration: 292, Checking gradient for hidden W[59][30]:	mock grad = -0.007101066950898627, computed grad = -0.007101067083494066
Iteration: 292, Checking gradient for hidden b[0][29]:	mock grad = -0.090172765443680136, computed grad = -0.090172782632751691
Iteration: 292, Checking gradient for output layer W[83][0]:	mock grad = -0.198078929400519188, computed grad = -0.198078927834766494
Iteration: 293, Checking gradient for words E[257][38]:	mock grad = -0.016418508117677089, computed grad = -0.016418508142347406
Iteration: 293, Checking gradient for hidden W[221][26]:	mock grad = -0.001663664047579783, computed grad = -0.001663664063592416
Iteration: 293, Checking gradient for hidden b[0][36]:	mock grad = 0.003615462467299047, computed grad = 0.003615452330281502
Iteration: 293, Checking gradient for output layer W[14][0]:	mock grad = 0.003886396397756453, computed grad = 0.003886396397761702
Iteration: 294, Checking gradient for words E[444][45]:	mock grad = -0.008126432249677684, computed grad = -0.008126432265634011
Iteration: 294, Checking gradient for hidden W[96][49]:	mock grad = 0.009845524511276071, computed grad = 0.009845524535535659
Iteration: 294, Checking gradient for hidden b[0][18]:	mock grad = -0.055880695166987682, computed grad = -0.055880677247671762
Iteration: 294, Checking gradient for output layer W[7][1]:	mock grad = -0.055015632060173569, computed grad = -0.055015632030381595
Iteration: 295, Checking gradient for words E[472][8]:	mock grad = -0.017345636512067220, computed grad = -0.017345636495571002
Iteration: 295, Checking gradient for hidden W[11][34]:	mock grad = 0.007009419207004708, computed grad = 0.007009419232342119
Iteration: 295, Checking gradient for hidden b[0][26]:	mock grad = 0.046202593728228436, computed grad = 0.046202606467861257
Iteration: 295, Checking gradient for output layer W[104][1]:	mock grad = 0.086431221719973639, computed grad = 0.086431221677382222
Iteration: 296, Checking gradient for words E[460][28]:	mock grad = 0.035164910094753754, computed grad = 0.035164910098844510
Iteration: 296, Checking gradient for hidden W[233][43]:	mock grad = 0.004339773179928841, computed grad = 0.004339773267274773
Iteration: 296, Checking gradient for hidden b[0][47]:	mock grad = -0.079656460338028978, computed grad = -0.079656461845641249
Iteration: 296, Checking gradient for output layer W[81][1]:	mock grad = 0.125341748101637229, computed grad = 0.125341747895841377
Iteration: 297, Checking gradient for words E[102][31]:	mock grad = -0.016204157586019452, computed grad = -0.016204157571549006
Iteration: 297, Checking gradient for hidden W[44][8]:	mock grad = -0.021322865221662379, computed grad = -0.021322865244070288
Iteration: 297, Checking gradient for hidden b[0][29]:	mock grad = 0.135687950207796781, computed grad = 0.135687997227221163
Iteration: 297, Checking gradient for output layer W[30][0]:	mock grad = -0.006527545515944144, computed grad = -0.006527545515922620
Iteration: 298, Checking gradient for words E[166][49]:	mock grad = 0.017638207205100676, computed grad = 0.017638207266277267
Iteration: 298, Checking gradient for hidden W[213][15]:	mock grad = 0.013958671830682778, computed grad = 0.013958672024901400
Iteration: 298, Checking gradient for hidden b[0][16]:	mock grad = -0.024126800042489904, computed grad = -0.024126809188435776
Iteration: 298, Checking gradient for output layer W[112][0]:	mock grad = -0.049466352738086750, computed grad = -0.049466352721041384
current: 100, Cost = 0.540162, Correct(%) = 0.98, time = 6.98061
Iteration: 299, Checking gradient for words E[49][43]:	mock grad = -0.014547148009191879, computed grad = -0.014547148014250388
Iteration: 299, Checking gradient for hidden W[19][41]:	mock grad = -0.005961276649424363, computed grad = -0.005961276683506452
Iteration: 299, Checking gradient for hidden b[0][9]:	mock grad = -0.067197986322764525, computed grad = -0.067197985784146486
Iteration: 299, Checking gradient for output layer W[125][1]:	mock grad = -0.035215972234770021, computed grad = -0.035215972230739183
current: 3, Correct(%) = 0.98, time = 7.03218
Dev start.
Dev finished. Total time taken is: 0.635114
dev:
Accuracy:	P=100/100=1
Test start.
Test finished. Total time taken is: 0.546199
test:
Accuracy:	P=63/100=0.63
Exceeds best previous DIS of 0.98. Saving model file..
open output file error
##### Iteration 3
random: 0, 99
Iteration: 300, Checking gradient for words E[16][36]:	mock grad = 0.080956318227964896, computed grad = 0.080956318040771927
Iteration: 300, Checking gradient for hidden W[227][32]:	mock grad = -0.009664542792831021, computed grad = -0.009664542869291382
Iteration: 300, Checking gradient for hidden b[0][30]:	mock grad = 0.003275409294623000, computed grad = 0.003275415775754672
Iteration: 300, Checking gradient for output layer W[15][0]:	mock grad = -0.062302674595365870, computed grad = -0.062302674479551942
Iteration: 301, Checking gradient for words E[86][19]:	mock grad = -0.027110360697413238, computed grad = -0.027110360745627352
Iteration: 301, Checking gradient for hidden W[151][26]:	mock grad = 0.013911355648849044, computed grad = 0.013911355736164326
Iteration: 301, Checking gradient for hidden b[0][20]:	mock grad = 0.109576287034868214, computed grad = 0.109576307441466556
Iteration: 301, Checking gradient for output layer W[38][0]:	mock grad = -0.013835342262208394, computed grad = -0.013835342261649704
Iteration: 302, Checking gradient for words E[25][46]:	mock grad = -0.004688486507448131, computed grad = -0.004688486511502125
Iteration: 302, Checking gradient for hidden W[150][36]:	mock grad = 0.006568341697099633, computed grad = 0.006568341633112874
Iteration: 302, Checking gradient for hidden b[0][6]:	mock grad = -0.040875184646627094, computed grad = -0.040875197174904530
Iteration: 302, Checking gradient for output layer W[7][0]:	mock grad = 0.071465950788746069, computed grad = 0.071465950681388668
Iteration: 303, Checking gradient for words E[138][14]:	mock grad = -0.008182182747729083, computed grad = -0.008182182846430252
Iteration: 303, Checking gradient for hidden W[96][29]:	mock grad = -0.005712529784207554, computed grad = -0.005712529799317587
Iteration: 303, Checking gradient for hidden b[0][19]:	mock grad = 0.064094581358808966, computed grad = 0.064094586354592739
Iteration: 303, Checking gradient for output layer W[65][0]:	mock grad = 0.227523352672232004, computed grad = 0.227523351917208461
Iteration: 304, Checking gradient for words E[148][42]:	mock grad = 0.013527732975915363, computed grad = 0.013527733088504291
Iteration: 304, Checking gradient for hidden W[108][24]:	mock grad = -0.008089306670677754, computed grad = -0.008089306672857843
Iteration: 304, Checking gradient for hidden b[0][34]:	mock grad = 0.035782574114961996, computed grad = 0.035782581732162019
Iteration: 304, Checking gradient for output layer W[53][0]:	mock grad = 0.128668489116912088, computed grad = 0.128668488856768709
Iteration: 305, Checking gradient for words E[225][9]:	mock grad = -0.001819397436514381, computed grad = -0.001819397425817171
Iteration: 305, Checking gradient for hidden W[51][0]:	mock grad = -0.024365442485685396, computed grad = -0.024365442649839007
Iteration: 305, Checking gradient for hidden b[0][17]:	mock grad = -0.024148644985538814, computed grad = -0.024148654033606005
Iteration: 305, Checking gradient for output layer W[66][0]:	mock grad = 0.102731426136232207, computed grad = 0.102731425883667127
Iteration: 306, Checking gradient for words E[102][0]:	mock grad = -0.013758451106377079, computed grad = -0.013758451107588880
Iteration: 306, Checking gradient for hidden W[141][39]:	mock grad = 0.011197939717322303, computed grad = 0.011197939790617522
Iteration: 306, Checking gradient for hidden b[0][29]:	mock grad = 0.079757618888037785, computed grad = 0.079757645923836162
Iteration: 306, Checking gradient for output layer W[56][0]:	mock grad = 0.124173620440815036, computed grad = 0.124173619554527284
Iteration: 307, Checking gradient for words E[280][1]:	mock grad = 0.055323056235789814, computed grad = 0.055323056383848622
Iteration: 307, Checking gradient for hidden W[37][18]:	mock grad = -0.003424900919313245, computed grad = -0.003424900841725325
Iteration: 307, Checking gradient for hidden b[0][36]:	mock grad = -0.005058238027266793, computed grad = -0.005058227354434447
Iteration: 307, Checking gradient for output layer W[65][1]:	mock grad = 0.263711923132514681, computed grad = 0.263711922291280820
Iteration: 308, Checking gradient for words E[288][14]:	mock grad = -0.000751732077197698, computed grad = -0.000751732081965379
Iteration: 308, Checking gradient for hidden W[194][35]:	mock grad = -0.000782477509786439, computed grad = -0.000782477512871657
Iteration: 308, Checking gradient for hidden b[0][16]:	mock grad = -0.021503250971749432, computed grad = -0.021503258392287616
Iteration: 308, Checking gradient for output layer W[25][1]:	mock grad = -0.085105986150385560, computed grad = -0.085105986025583877
current: 10, Cost = 0.465502, Correct(%) = 1, time = 0.769086
Iteration: 309, Checking gradient for words E[228][33]:	mock grad = -0.005633101175489541, computed grad = -0.005633101140208680
Iteration: 309, Checking gradient for hidden W[59][43]:	mock grad = -0.002463355042725546, computed grad = -0.002463355072691811
Iteration: 309, Checking gradient for hidden b[0][17]:	mock grad = 0.022080471100138777, computed grad = 0.022080475520692429
Iteration: 309, Checking gradient for output layer W[105][0]:	mock grad = 0.076131037966470361, computed grad = 0.076131037881282893
Iteration: 310, Checking gradient for words E[209][37]:	mock grad = 0.014762188530720310, computed grad = 0.014762188571224043
Iteration: 310, Checking gradient for hidden W[178][6]:	mock grad = 0.001902843301898605, computed grad = 0.001902843293622508
Iteration: 310, Checking gradient for hidden b[0][17]:	mock grad = -0.018587170471068282, computed grad = -0.018587174383932984
Iteration: 310, Checking gradient for output layer W[110][1]:	mock grad = 0.120503109361125293, computed grad = 0.120503108708861828
Iteration: 311, Checking gradient for words E[156][22]:	mock grad = -0.033287808514137485, computed grad = -0.033287808610019773
Iteration: 311, Checking gradient for hidden W[245][4]:	mock grad = 0.004196022924618514, computed grad = 0.004196022968630003
Iteration: 311, Checking gradient for hidden b[0][47]:	mock grad = -0.103109706922288602, computed grad = -0.103109729748815257
Iteration: 311, Checking gradient for output layer W[0][0]:	mock grad = -0.092140839475507441, computed grad = -0.092140839388065221
Iteration: 312, Checking gradient for words E[339][19]:	mock grad = 0.017989272790647215, computed grad = 0.017989272803894837
Iteration: 312, Checking gradient for hidden W[247][41]:	mock grad = 0.004625436188870902, computed grad = 0.004625436276691016
Iteration: 312, Checking gradient for hidden b[0][19]:	mock grad = 0.045120403642712192, computed grad = 0.045120401346270232
Iteration: 312, Checking gradient for output layer W[31][1]:	mock grad = 0.041823248773997790, computed grad = 0.041823248754949610
Iteration: 313, Checking gradient for words E[158][19]:	mock grad = 0.024277916184339832, computed grad = 0.024277916175828355
Iteration: 313, Checking gradient for hidden W[22][23]:	mock grad = -0.010747215982709335, computed grad = -0.010747216107661765
Iteration: 313, Checking gradient for hidden b[0][18]:	mock grad = 0.082265932910496886, computed grad = 0.082265942822942806
Iteration: 313, Checking gradient for output layer W[16][0]:	mock grad = 0.001864638093262982, computed grad = 0.001864638093295792
Iteration: 314, Checking gradient for words E[376][14]:	mock grad = -0.034344247571038711, computed grad = -0.034344247643682893
Iteration: 314, Checking gradient for hidden W[1][5]:	mock grad = 0.001398152586096213, computed grad = 0.001398152710273315
Iteration: 314, Checking gradient for hidden b[0][12]:	mock grad = -0.113348545656755118, computed grad = -0.113348583639021935
Iteration: 314, Checking gradient for output layer W[28][0]:	mock grad = 0.126264778470719374, computed grad = 0.126264778449920317
Iteration: 315, Checking gradient for words E[378][14]:	mock grad = 0.043652559586077055, computed grad = 0.043652559428031597
Iteration: 315, Checking gradient for hidden W[54][27]:	mock grad = 0.013493993533036930, computed grad = 0.013493993656868979
Iteration: 315, Checking gradient for hidden b[0][18]:	mock grad = 0.073629171798039739, computed grad = 0.073629174157883476
Iteration: 315, Checking gradient for output layer W[99][0]:	mock grad = -0.105271702700748815, computed grad = -0.105271702489601535
Iteration: 316, Checking gradient for words E[396][9]:	mock grad = -0.005196329825596369, computed grad = -0.005196329840961631
Iteration: 316, Checking gradient for hidden W[195][6]:	mock grad = 0.000816522154167831, computed grad = -0.000202021116726119
Iteration: 316, Checking gradient for hidden b[0][14]:	mock grad = 0.080431877152342679, computed grad = 0.080431907360499016
Iteration: 316, Checking gradient for output layer W[111][0]:	mock grad = -0.086857841544885073, computed grad = -0.086857841463447938
Iteration: 317, Checking gradient for words E[178][14]:	mock grad = 0.023022310003395496, computed grad = 0.023022310044561168
Iteration: 317, Checking gradient for hidden W[6][17]:	mock grad = -0.001575993251928809, computed grad = -0.001575993255604058
Iteration: 317, Checking gradient for hidden b[0][44]:	mock grad = 0.028429314877093104, computed grad = 0.028429336876741759
Iteration: 317, Checking gradient for output layer W[89][0]:	mock grad = 0.130885890975179775, computed grad = 0.130885890053454801
Iteration: 318, Checking gradient for words E[433][44]:	mock grad = -0.011303463711948014, computed grad = -0.011303463687272005
Iteration: 318, Checking gradient for hidden W[236][22]:	mock grad = -0.002152499736896019, computed grad = -0.002152499745784908
Iteration: 318, Checking gradient for hidden b[0][26]:	mock grad = 0.030367788554114261, computed grad = 0.030367793242927939
Iteration: 318, Checking gradient for output layer W[89][0]:	mock grad = 0.111925019157216310, computed grad = 0.111925018697710946
current: 20, Cost = 0.516698, Correct(%) = 1, time = 1.43214
Iteration: 319, Checking gradient for words E[119][20]:	mock grad = -0.030538339584218477, computed grad = -0.030538339743710632
Iteration: 319, Checking gradient for hidden W[73][13]:	mock grad = -0.005404448196633105, computed grad = -0.005404448227089031
Iteration: 319, Checking gradient for hidden b[0][42]:	mock grad = -0.086757779444723848, computed grad = -0.086757804177986064
Iteration: 319, Checking gradient for output layer W[78][0]:	mock grad = -0.240629550613380427, computed grad = -0.240629548971813517
Iteration: 320, Checking gradient for words E[442][42]:	mock grad = -0.000814891195938250, computed grad = -0.000814891231633577
Iteration: 320, Checking gradient for hidden W[83][27]:	mock grad = -0.004648679222479801, computed grad = -0.004648679229905617
Iteration: 320, Checking gradient for hidden b[0][33]:	mock grad = 0.112863987254380183, computed grad = 0.112864010006796536
Iteration: 320, Checking gradient for output layer W[125][1]:	mock grad = -0.022562881063226126, computed grad = -0.022562881060999542
Iteration: 321, Checking gradient for words E[284][6]:	mock grad = 0.023580946517276491, computed grad = 0.023580946503363190
Iteration: 321, Checking gradient for hidden W[48][2]:	mock grad = 0.018795111922931262, computed grad = 0.018795112037069545
Iteration: 321, Checking gradient for hidden b[0][1]:	mock grad = 0.131581019592708071, computed grad = 0.131581059423267588
Iteration: 321, Checking gradient for output layer W[55][1]:	mock grad = -0.105478078046949975, computed grad = -0.105478077947570539
Iteration: 322, Checking gradient for words E[450][26]:	mock grad = 0.012819992147083958, computed grad = 0.012819992201972871
Iteration: 322, Checking gradient for hidden W[158][34]:	mock grad = -0.006440694476805842, computed grad = -0.006440694485651089
Iteration: 322, Checking gradient for hidden b[0][3]:	mock grad = 0.042574323461219699, computed grad = 0.042574328788087923
Iteration: 322, Checking gradient for output layer W[55][1]:	mock grad = -0.151922462168219141, computed grad = -0.151922461214420740
Iteration: 323, Checking gradient for words E[102][48]:	mock grad = 0.005643900963414605, computed grad = 0.005643901050951680
Iteration: 323, Checking gradient for hidden W[217][30]:	mock grad = 0.003552647552057531, computed grad = 0.003552647564777079
Iteration: 323, Checking gradient for hidden b[0][0]:	mock grad = 0.186163274027567605, computed grad = 0.186163334765682864
Iteration: 323, Checking gradient for output layer W[13][1]:	mock grad = 0.066481576126570241, computed grad = 0.066481576060258576
Iteration: 324, Checking gradient for words E[460][31]:	mock grad = -0.002639812219568105, computed grad = -0.002639812243334518
Iteration: 324, Checking gradient for hidden W[113][12]:	mock grad = 0.002764584821984828, computed grad = 0.002764584836696089
Iteration: 324, Checking gradient for hidden b[0][17]:	mock grad = 0.024417925511499572, computed grad = 0.024417931660182544
Iteration: 324, Checking gradient for output layer W[139][1]:	mock grad = -0.110975362023785218, computed grad = -0.110975361820437810
Iteration: 325, Checking gradient for words E[316][28]:	mock grad = 0.039137581437670432, computed grad = 0.039137581494312526
Iteration: 325, Checking gradient for hidden W[196][41]:	mock grad = 0.003751824639325330, computed grad = 0.003751824661881049
Iteration: 325, Checking gradient for hidden b[0][24]:	mock grad = -0.115708168929917754, computed grad = -0.115708187655062009
Iteration: 325, Checking gradient for output layer W[113][0]:	mock grad = 0.158714245065139092, computed grad = 0.158714244180063968
Iteration: 326, Checking gradient for words E[478][38]:	mock grad = 0.007803109760201599, computed grad = 0.007803109748937274
Iteration: 326, Checking gradient for hidden W[3][23]:	mock grad = 0.012447540030768867, computed grad = 0.012447540134013828
Iteration: 326, Checking gradient for hidden b[0][21]:	mock grad = 0.052414394187988433, computed grad = 0.052414402303975656
Iteration: 326, Checking gradient for output layer W[128][0]:	mock grad = -0.056670284236115531, computed grad = -0.056670284178098621
Iteration: 327, Checking gradient for words E[68][47]:	mock grad = -0.051336524605760658, computed grad = -0.051336524748329247
Iteration: 327, Checking gradient for hidden W[195][20]:	mock grad = 0.012588984452166496, computed grad = 0.012588984508366874
Iteration: 327, Checking gradient for hidden b[0][8]:	mock grad = 0.056748356833224989, computed grad = 0.056748396219056613
Iteration: 327, Checking gradient for output layer W[26][0]:	mock grad = 0.101835303920883025, computed grad = 0.101835303868369406
Iteration: 328, Checking gradient for words E[267][45]:	mock grad = -0.014554258018689703, computed grad = -0.014554258072731198
Iteration: 328, Checking gradient for hidden W[240][29]:	mock grad = -0.017986899169486481, computed grad = -0.017986899294094340
Iteration: 328, Checking gradient for hidden b[0][0]:	mock grad = -0.169150521661554087, computed grad = -0.169150571693889529
Iteration: 328, Checking gradient for output layer W[25][0]:	mock grad = -0.094557637437236997, computed grad = -0.094557637175391521
current: 30, Cost = 0.431763, Correct(%) = 1, time = 2.12602
Iteration: 329, Checking gradient for words E[505][46]:	mock grad = -0.001536256933970526, computed grad = -0.001536256881639437
Iteration: 329, Checking gradient for hidden W[46][12]:	mock grad = 0.001736662092605146, computed grad = 0.001736662083337679
Iteration: 329, Checking gradient for hidden b[0][44]:	mock grad = 0.007376185223229026, computed grad = 0.007376199043285292
Iteration: 329, Checking gradient for output layer W[137][0]:	mock grad = -0.137412881790893726, computed grad = -0.137412881108617879
Iteration: 330, Checking gradient for words E[511][12]:	mock grad = -0.042434731396256975, computed grad = -0.042434731391157360
Iteration: 330, Checking gradient for hidden W[62][20]:	mock grad = -0.001861981399287060, computed grad = -0.001861981434505154
Iteration: 330, Checking gradient for hidden b[0][32]:	mock grad = 0.063438748053701310, computed grad = 0.063438769227489547
Iteration: 330, Checking gradient for output layer W[96][1]:	mock grad = 0.163936586080909441, computed grad = 0.163936584965168691
Iteration: 331, Checking gradient for words E[119][37]:	mock grad = 0.020077344717484014, computed grad = 0.020077344741155516
Iteration: 331, Checking gradient for hidden W[159][28]:	mock grad = -0.005625070780612695, computed grad = -0.005625070824180689
Iteration: 331, Checking gradient for hidden b[0][34]:	mock grad = -0.040162180114045043, computed grad = -0.040162185382822554
Iteration: 331, Checking gradient for output layer W[47][0]:	mock grad = -0.073621037016025070, computed grad = -0.073621036998474831
Iteration: 332, Checking gradient for words E[194][6]:	mock grad = -0.015576982838300113, computed grad = -0.015576982827454119
Iteration: 332, Checking gradient for hidden W[55][4]:	mock grad = 0.002237941133548249, computed grad = 0.002237941146706141
Iteration: 332, Checking gradient for hidden b[0][32]:	mock grad = 0.060051636176811085, computed grad = 0.060051656473382467
Iteration: 332, Checking gradient for output layer W[12][1]:	mock grad = 0.056743110012580367, computed grad = 0.056743109949342903
Iteration: 333, Checking gradient for words E[338][10]:	mock grad = -0.005102038287463362, computed grad = -0.005102038274270564
Iteration: 333, Checking gradient for hidden W[106][44]:	mock grad = -0.019372397829953147, computed grad = -0.019372397829815063
Iteration: 333, Checking gradient for hidden b[0][49]:	mock grad = -0.040206900173422877, computed grad = -0.040206901449532845
Iteration: 333, Checking gradient for output layer W[103][1]:	mock grad = 0.135945017147087732, computed grad = 0.135945016334587970
Iteration: 334, Checking gradient for words E[529][36]:	mock grad = -0.022849036902639064, computed grad = -0.022849037044334771
Iteration: 334, Checking gradient for hidden W[75][44]:	mock grad = 0.035963978438746658, computed grad = 0.035963978768644829
Iteration: 334, Checking gradient for hidden b[0][42]:	mock grad = -0.089627940913328263, computed grad = -0.089627966480926191
Iteration: 334, Checking gradient for output layer W[111][0]:	mock grad = 0.104177180187015050, computed grad = 0.104177180090590168
Iteration: 335, Checking gradient for words E[201][16]:	mock grad = -0.078862426430237331, computed grad = -0.078862426639147082
Iteration: 335, Checking gradient for hidden W[170][4]:	mock grad = 0.000916008787432698, computed grad = 0.000916008802503154
Iteration: 335, Checking gradient for hidden b[0][38]:	mock grad = 0.088700234065097749, computed grad = 0.088700257472018262
Iteration: 335, Checking gradient for output layer W[115][0]:	mock grad = -0.064707027233934511, computed grad = -0.064707027202266648
Iteration: 336, Checking gradient for words E[10][44]:	mock grad = 0.051075361231667227, computed grad = 0.051075361466833967
Iteration: 336, Checking gradient for hidden W[136][21]:	mock grad = 0.005203017315102976, computed grad = 0.005203017315222323
Iteration: 336, Checking gradient for hidden b[0][37]:	mock grad = 0.073211095129144255, computed grad = 0.073211113432574138
Iteration: 336, Checking gradient for output layer W[132][1]:	mock grad = 0.046900999317756042, computed grad = 0.046900999309152466
Iteration: 337, Checking gradient for words E[21][47]:	mock grad = -0.000327613522560499, computed grad = -0.000327613565600265
Iteration: 337, Checking gradient for hidden W[162][31]:	mock grad = 0.009275010696890851, computed grad = 0.009275010731145904
Iteration: 337, Checking gradient for hidden b[0][7]:	mock grad = -0.087252749848998334, computed grad = -0.087252757089633912
Iteration: 337, Checking gradient for output layer W[102][0]:	mock grad = -0.214812442470846410, computed grad = -0.214812440871546467
Iteration: 338, Checking gradient for words E[488][4]:	mock grad = 0.002938860804768684, computed grad = 0.002938860805264343
Iteration: 338, Checking gradient for hidden W[60][40]:	mock grad = -0.016260597088757489, computed grad = -0.016260597391720796
Iteration: 338, Checking gradient for hidden b[0][9]:	mock grad = -0.076301371416231945, computed grad = -0.076301379157964866
Iteration: 338, Checking gradient for output layer W[117][1]:	mock grad = -0.170365182399156989, computed grad = -0.170365181948894329
current: 40, Cost = 0.403651, Correct(%) = 1, time = 2.56014
Iteration: 339, Checking gradient for words E[6][19]:	mock grad = 0.008041762745270020, computed grad = 0.008041762821872798
Iteration: 339, Checking gradient for hidden W[154][3]:	mock grad = 0.011168670748434861, computed grad = 0.011168670791347434
Iteration: 339, Checking gradient for hidden b[0][3]:	mock grad = 0.042129106051391751, computed grad = 0.042129112759066077
Iteration: 339, Checking gradient for output layer W[107][0]:	mock grad = -0.017777362536719465, computed grad = -0.017777362534752143
Iteration: 340, Checking gradient for words E[776][22]:	mock grad = 0.003184416061785544, computed grad = 0.003364473827380428
Iteration: 340, Checking gradient for hidden W[81][23]:	mock grad = -0.005316318938908271, computed grad = -0.005316318944580231
Iteration: 340, Checking gradient for hidden b[0][15]:	mock grad = 0.103218874823368667, computed grad = 0.103218899799677272
Iteration: 340, Checking gradient for output layer W[54][0]:	mock grad = 0.120739378918044427, computed grad = 0.120739378354892971
Iteration: 341, Checking gradient for words E[225][48]:	mock grad = -0.005410806726113826, computed grad = -0.005410806793886971
Iteration: 341, Checking gradient for hidden W[79][2]:	mock grad = -0.014381641047112037, computed grad = -0.014381641193349420
Iteration: 341, Checking gradient for hidden b[0][31]:	mock grad = -0.053927145026055889, computed grad = -0.053927156359152142
Iteration: 341, Checking gradient for output layer W[133][1]:	mock grad = -0.129738988924210652, computed grad = -0.129738988836542946
Iteration: 342, Checking gradient for words E[101][10]:	mock grad = -0.029323090523780682, computed grad = -0.029323090578602912
Iteration: 342, Checking gradient for hidden W[77][39]:	mock grad = -0.026655098828265444, computed grad = -0.026655099152432409
Iteration: 342, Checking gradient for hidden b[0][23]:	mock grad = 0.180070728153292059, computed grad = 0.180070754572125458
Iteration: 342, Checking gradient for output layer W[95][1]:	mock grad = 0.006533140990305775, computed grad = 0.006533140990251356
Iteration: 343, Checking gradient for words E[10][21]:	mock grad = -0.016647171412642603, computed grad = -0.016647171427421004
Iteration: 343, Checking gradient for hidden W[127][23]:	mock grad = 0.003001994844709088, computed grad = 0.003001994812367394
Iteration: 343, Checking gradient for hidden b[0][23]:	mock grad = 0.211995753927440855, computed grad = 0.211995808210288295
Iteration: 343, Checking gradient for output layer W[33][1]:	mock grad = 0.039838992543117513, computed grad = 0.039838992541245732
Iteration: 344, Checking gradient for words E[148][40]:	mock grad = -0.017695232766901103, computed grad = -0.017695232785580970
Iteration: 344, Checking gradient for hidden W[167][16]:	mock grad = 0.000147873299827239, computed grad = 0.000147873295138855
Iteration: 344, Checking gradient for hidden b[0][35]:	mock grad = -0.011004481540899391, computed grad = -0.011004479408724087
Iteration: 344, Checking gradient for output layer W[132][0]:	mock grad = -0.059594481118724341, computed grad = -0.059594481030478652
Iteration: 345, Checking gradient for words E[49][16]:	mock grad = -0.055381891590933208, computed grad = -0.055254008083154907
Iteration: 345, Checking gradient for hidden W[43][39]:	mock grad = -0.000877156191181339, computed grad = -0.000877156158626166
Iteration: 345, Checking gradient for hidden b[0][46]:	mock grad = 0.195905899728565647, computed grad = 0.195905958824900389
Iteration: 345, Checking gradient for output layer W[118][1]:	mock grad = 0.062826349877365129, computed grad = 0.062826349855813535
Iteration: 346, Checking gradient for words E[257][25]:	mock grad = -0.003009583276619709, computed grad = -0.003009583282636382
Iteration: 346, Checking gradient for hidden W[39][49]:	mock grad = -0.007886301977094723, computed grad = -0.007886302002555757
Iteration: 346, Checking gradient for hidden b[0][48]:	mock grad = 0.078014522738006908, computed grad = 0.078014516825865507
Iteration: 346, Checking gradient for output layer W[34][1]:	mock grad = 0.002818023977863016, computed grad = 0.002818023977811751
Iteration: 347, Checking gradient for words E[190][36]:	mock grad = -0.012426842837365282, computed grad = -0.012426842856794863
Iteration: 347, Checking gradient for hidden W[115][40]:	mock grad = 0.009160975675520655, computed grad = 0.009160975781487260
Iteration: 347, Checking gradient for hidden b[0][14]:	mock grad = 0.074044727679439815, computed grad = 0.074044756449182958
Iteration: 347, Checking gradient for output layer W[96][0]:	mock grad = 0.186462391292774310, computed grad = 0.186462389780915810
Iteration: 348, Checking gradient for words E[436][41]:	mock grad = -0.002207967786660436, computed grad = -0.002207967791131254
Iteration: 348, Checking gradient for hidden W[78][9]:	mock grad = 0.006696498680763829, computed grad = 0.006696498666463123
Iteration: 348, Checking gradient for hidden b[0][21]:	mock grad = -0.051739107893050695, computed grad = -0.051739117761775415
Iteration: 348, Checking gradient for output layer W[15][0]:	mock grad = -0.088253863687992595, computed grad = -0.088253863424726728
current: 50, Cost = 0.419329, Correct(%) = 1, time = 3.44929
Iteration: 349, Checking gradient for words E[197][19]:	mock grad = -0.020335559337225639, computed grad = -0.020335559373508973
Iteration: 349, Checking gradient for hidden W[75][27]:	mock grad = -0.015782841074452048, computed grad = -0.015782841148636848
Iteration: 349, Checking gradient for hidden b[0][7]:	mock grad = 0.090156825758969106, computed grad = 0.090156847819204355
Iteration: 349, Checking gradient for output layer W[0][0]:	mock grad = -0.056230899236925369, computed grad = -0.056230899184633150
Iteration: 350, Checking gradient for words E[409][29]:	mock grad = -0.000947160112951817, computed grad = -0.003199203620393177
Iteration: 350, Checking gradient for hidden W[83][32]:	mock grad = 0.018214447107367526, computed grad = 0.018214447353423001
Iteration: 350, Checking gradient for hidden b[0][48]:	mock grad = 0.124400690478421172, computed grad = 0.124400695837731723
Iteration: 350, Checking gradient for output layer W[6][0]:	mock grad = -0.062987998910857268, computed grad = -0.062987998901532560
Iteration: 351, Checking gradient for words E[403][40]:	mock grad = 0.001770997920036432, computed grad = 0.001876520623625585
Iteration: 351, Checking gradient for hidden W[69][43]:	mock grad = -0.000040848730564758, computed grad = -0.000040848730433095
Iteration: 351, Checking gradient for hidden b[0][11]:	mock grad = 0.117827110866275397, computed grad = 0.117827143363248038
Iteration: 351, Checking gradient for output layer W[71][1]:	mock grad = -0.133135106394288716, computed grad = -0.133135106098551032
Iteration: 352, Checking gradient for words E[451][39]:	mock grad = 0.016220680969847301, computed grad = 0.016220681090306978
Iteration: 352, Checking gradient for hidden W[150][21]:	mock grad = 0.004062177999863970, computed grad = 0.004062178011431957
Iteration: 352, Checking gradient for hidden b[0][10]:	mock grad = -0.124227445580771256, computed grad = -0.124227487444930007
Iteration: 352, Checking gradient for output layer W[133][1]:	mock grad = 0.077941240421375291, computed grad = 0.077941240389851271
Iteration: 353, Checking gradient for words E[315][16]:	mock grad = 0.022478218987098675, computed grad = 0.022478218943948431
Iteration: 353, Checking gradient for hidden W[97][46]:	mock grad = -0.016467268110942346, computed grad = -0.016467268224028116
Iteration: 353, Checking gradient for hidden b[0][20]:	mock grad = 0.119821161731842318, computed grad = 0.119821191168465180
Iteration: 353, Checking gradient for output layer W[52][1]:	mock grad = 0.062102147257603324, computed grad = 0.062102147208056388
Iteration: 354, Checking gradient for words E[124][40]:	mock grad = -0.010553825691872465, computed grad = -0.010553825674328311
Iteration: 354, Checking gradient for hidden W[68][0]:	mock grad = 0.003618903311797261, computed grad = 0.003618903314261163
Iteration: 354, Checking gradient for hidden b[0][13]:	mock grad = 0.132909971046091258, computed grad = 0.132910003821867129
Iteration: 354, Checking gradient for output layer W[47][0]:	mock grad = 0.073938217410279838, computed grad = 0.073938217293033334
Iteration: 355, Checking gradient for words E[612][21]:	mock grad = -0.004167618285477737, computed grad = -0.004167618298428548
Iteration: 355, Checking gradient for hidden W[138][17]:	mock grad = -0.009477110348848905, computed grad = -0.009477110483702202
Iteration: 355, Checking gradient for hidden b[0][20]:	mock grad = -0.114576977329167162, computed grad = -0.114577012030920120
Iteration: 355, Checking gradient for output layer W[52][1]:	mock grad = -0.091887381065164364, computed grad = -0.091887380829955428
Iteration: 356, Checking gradient for words E[49][5]:	mock grad = -0.011865793283882820, computed grad = -0.011865793295605210
Iteration: 356, Checking gradient for hidden W[106][15]:	mock grad = 0.003907248220691439, computed grad = 0.003907248222948497
Iteration: 356, Checking gradient for hidden b[0][12]:	mock grad = -0.102329802263823044, computed grad = -0.102329833447933533
Iteration: 356, Checking gradient for output layer W[64][0]:	mock grad = 0.194379553089041313, computed grad = 0.194379552719228715
Iteration: 357, Checking gradient for words E[34][29]:	mock grad = -0.029041234575866159, computed grad = -0.029041234622838103
Iteration: 357, Checking gradient for hidden W[124][40]:	mock grad = 0.010960314348856803, computed grad = 0.010960314410697477
Iteration: 357, Checking gradient for hidden b[0][44]:	mock grad = -0.008370863708084819, computed grad = -0.008370880574696244
Iteration: 357, Checking gradient for output layer W[115][1]:	mock grad = -0.019578812152420966, computed grad = -0.019578812150616909
Iteration: 358, Checking gradient for words E[309][9]:	mock grad = 0.010556530015581878, computed grad = 0.010556530114993613
Iteration: 358, Checking gradient for hidden W[36][33]:	mock grad = -0.004218179146808776, computed grad = -0.004218179322526726
Iteration: 358, Checking gradient for hidden b[0][16]:	mock grad = -0.023599477887714659, computed grad = -0.023599486067536486
Iteration: 358, Checking gradient for output layer W[96][1]:	mock grad = -0.220187582141262439, computed grad = -0.220187580758525125
current: 60, Cost = 0.509969, Correct(%) = 1, time = 4.1481
Iteration: 359, Checking gradient for words E[68][11]:	mock grad = 0.049650160036773805, computed grad = 0.049650160099900476
Iteration: 359, Checking gradient for hidden W[15][42]:	mock grad = -0.027488284640053706, computed grad = -0.027488285074561025
Iteration: 359, Checking gradient for hidden b[0][2]:	mock grad = 0.072488067715259508, computed grad = 0.072488105375895409
Iteration: 359, Checking gradient for output layer W[67][0]:	mock grad = -0.117372166945128864, computed grad = -0.117372166741260969
Iteration: 360, Checking gradient for words E[48][28]:	mock grad = 0.041239379018676026, computed grad = 0.041239379425392145
Iteration: 360, Checking gradient for hidden W[73][16]:	mock grad = 0.002731545933887247, computed grad = 0.002731545960004318
Iteration: 360, Checking gradient for hidden b[0][46]:	mock grad = 0.177226529031626612, computed grad = 0.177226575565829686
Iteration: 360, Checking gradient for output layer W[119][1]:	mock grad = -0.157085997572414815, computed grad = -0.157085997035169539
Iteration: 361, Checking gradient for words E[514][46]:	mock grad = -0.038564826202347469, computed grad = -0.038564826311986496
Iteration: 361, Checking gradient for hidden W[224][47]:	mock grad = -0.016266104237494527, computed grad = -0.016266104318771415
Iteration: 361, Checking gradient for hidden b[0][18]:	mock grad = -0.087191674472519853, computed grad = -0.087191660259422071
Iteration: 361, Checking gradient for output layer W[79][1]:	mock grad = -0.142834550294568263, computed grad = -0.142834550167042912
Iteration: 362, Checking gradient for words E[350][35]:	mock grad = -0.001542096034534701, computed grad = -0.001542096005170031
Iteration: 362, Checking gradient for hidden W[74][10]:	mock grad = 0.012539617086665400, computed grad = 0.012539617124333265
Iteration: 362, Checking gradient for hidden b[0][20]:	mock grad = 0.128886384709120394, computed grad = 0.128886421340727114
Iteration: 362, Checking gradient for output layer W[54][0]:	mock grad = -0.081557396480497202, computed grad = -0.081557396400036286
Iteration: 363, Checking gradient for words E[361][25]:	mock grad = 0.011007183932099096, computed grad = 0.011007184046022904
Iteration: 363, Checking gradient for hidden W[67][43]:	mock grad = 0.000974534668962690, computed grad = 0.000974534671266192
Iteration: 363, Checking gradient for hidden b[0][13]:	mock grad = 0.155759047613501789, computed grad = 0.155759083340026722
Iteration: 363, Checking gradient for output layer W[104][0]:	mock grad = -0.115736360648377445, computed grad = -0.115736360495515042
Iteration: 364, Checking gradient for words E[194][2]:	mock grad = 0.007010499576631801, computed grad = 0.007010499608220310
Iteration: 364, Checking gradient for hidden W[190][42]:	mock grad = 0.006749426379404344, computed grad = 0.006749426357853214
Iteration: 364, Checking gradient for hidden b[0][36]:	mock grad = 0.003540930404277720, computed grad = 0.003540920016416189
Iteration: 364, Checking gradient for output layer W[27][1]:	mock grad = 0.077430729530814091, computed grad = 0.077430729482080116
Iteration: 365, Checking gradient for words E[118][45]:	mock grad = 0.024067346980044535, computed grad = 0.024067347056626956
Iteration: 365, Checking gradient for hidden W[94][11]:	mock grad = 0.001392521611326414, computed grad = 0.001392521605446052
Iteration: 365, Checking gradient for hidden b[0][13]:	mock grad = -0.116605004046171912, computed grad = -0.116605031044465776
Iteration: 365, Checking gradient for output layer W[94][0]:	mock grad = -0.138808139626439564, computed grad = -0.138808138449952767
Iteration: 366, Checking gradient for words E[296][20]:	mock grad = 0.001527286918906468, computed grad = 0.001527286945469259
Iteration: 366, Checking gradient for hidden W[166][4]:	mock grad = 0.002762099442221722, computed grad = 0.002762099448800207
Iteration: 366, Checking gradient for hidden b[0][3]:	mock grad = -0.056782178951053641, computed grad = -0.056782203638806962
Iteration: 366, Checking gradient for output layer W[36][0]:	mock grad = 0.079453818118274677, computed grad = 0.079453818014553507
Iteration: 367, Checking gradient for words E[589][40]:	mock grad = -0.010134224761038091, computed grad = -0.010134224749355006
Iteration: 367, Checking gradient for hidden W[103][47]:	mock grad = 0.004221571808404878, computed grad = 0.004221571804063177
Iteration: 367, Checking gradient for hidden b[0][10]:	mock grad = -0.116694248579740645, computed grad = -0.116694286956356405
Iteration: 367, Checking gradient for output layer W[20][1]:	mock grad = -0.022028527089379413, computed grad = -0.022028527088222675
Iteration: 368, Checking gradient for words E[102][46]:	mock grad = -0.005528893010475766, computed grad = -0.005528892999829674
Iteration: 368, Checking gradient for hidden W[211][36]:	mock grad = -0.009349993348173324, computed grad = -0.009349993342510301
Iteration: 368, Checking gradient for hidden b[0][7]:	mock grad = -0.086360679126207263, computed grad = -0.086360689371084903
Iteration: 368, Checking gradient for output layer W[86][1]:	mock grad = -0.058489550427948123, computed grad = -0.058489550382534894
current: 70, Cost = 0.391171, Correct(%) = 1, time = 4.76237
Iteration: 369, Checking gradient for words E[311][45]:	mock grad = -0.019171345884083646, computed grad = -0.019171345902604411
Iteration: 369, Checking gradient for hidden W[225][32]:	mock grad = 0.017023568882612583, computed grad = 0.017023569229670146
Iteration: 369, Checking gradient for hidden b[0][42]:	mock grad = 0.070434407321823267, computed grad = 0.070434430531872261
Iteration: 369, Checking gradient for output layer W[144][0]:	mock grad = -0.125574608023981682, computed grad = -0.125574607273296612
Iteration: 370, Checking gradient for words E[448][42]:	mock grad = 0.016509178500823118, computed grad = 0.016509178582375789
Iteration: 370, Checking gradient for hidden W[167][45]:	mock grad = -0.000085022951845648, computed grad = -0.000085022958308317
Iteration: 370, Checking gradient for hidden b[0][17]:	mock grad = -0.024355773195317010, computed grad = -0.024355778919523879
Iteration: 370, Checking gradient for output layer W[115][0]:	mock grad = -0.076840601819838650, computed grad = -0.076840601779591303
Iteration: 371, Checking gradient for words E[2][35]:	mock grad = -0.008591430661342159, computed grad = -0.008591430619272361
Iteration: 371, Checking gradient for hidden W[85][16]:	mock grad = -0.008159402930474347, computed grad = -0.008159403062706372
Iteration: 371, Checking gradient for hidden b[0][27]:	mock grad = -0.042041141134652182, computed grad = -0.042041155669046169
Iteration: 371, Checking gradient for output layer W[53][1]:	mock grad = -0.066390512797331880, computed grad = -0.066390512770600096
Iteration: 372, Checking gradient for words E[25][29]:	mock grad = -0.000115963174174283, computed grad = -0.000115963229451667
Iteration: 372, Checking gradient for hidden W[160][35]:	mock grad = 0.003461621613187749, computed grad = 0.003461621610184878
Iteration: 372, Checking gradient for hidden b[0][4]:	mock grad = -0.033038274185714922, computed grad = -0.033038291937118595
Iteration: 372, Checking gradient for output layer W[49][0]:	mock grad = 0.055728300589696911, computed grad = 0.055728300570265961
Iteration: 373, Checking gradient for words E[49][8]:	mock grad = -0.026813086413390863, computed grad = -0.026813086422628973
Iteration: 373, Checking gradient for hidden W[156][15]:	mock grad = -0.008613868503037914, computed grad = -0.008613868531018670
Iteration: 373, Checking gradient for hidden b[0][44]:	mock grad = -0.017430338710322513, computed grad = -0.017430369210671676
Iteration: 373, Checking gradient for output layer W[121][0]:	mock grad = 0.113607476531718543, computed grad = 0.113607476469640770
Iteration: 374, Checking gradient for words E[693][28]:	mock grad = 0.071675530754544692, computed grad = 0.071675530943606250
Iteration: 374, Checking gradient for hidden W[205][42]:	mock grad = -0.016300736521590586, computed grad = -0.016300736662632875
Iteration: 374, Checking gradient for hidden b[0][8]:	mock grad = -0.048896563149158911, computed grad = -0.048896592293997135
Iteration: 374, Checking gradient for output layer W[129][0]:	mock grad = -0.109112324322924170, computed grad = -0.109112324237650313
Iteration: 375, Checking gradient for words E[510][27]:	mock grad = 0.034294213610369662, computed grad = 0.034294213736203429
Iteration: 375, Checking gradient for hidden W[201][1]:	mock grad = -0.004025542027358275, computed grad = -0.004025542037930229
Iteration: 375, Checking gradient for hidden b[0][31]:	mock grad = 0.049158442285895942, computed grad = 0.049158460736884443
Iteration: 375, Checking gradient for output layer W[22][0]:	mock grad = -0.024914761132172059, computed grad = -0.024914761129003350
Iteration: 376, Checking gradient for words E[119][24]:	mock grad = -0.045386398440427911, computed grad = -0.045386398591268681
Iteration: 376, Checking gradient for hidden W[176][2]:	mock grad = -0.007804487900830992, computed grad = -0.007804487894531090
Iteration: 376, Checking gradient for hidden b[0][20]:	mock grad = 0.132088967953636338, computed grad = 0.132089008395886498
Iteration: 376, Checking gradient for output layer W[106][1]:	mock grad = -0.102168967763438534, computed grad = -0.102168967622938187
Iteration: 377, Checking gradient for words E[397][39]:	mock grad = -0.005053842957208898, computed grad = -0.005053843006559488
Iteration: 377, Checking gradient for hidden W[101][41]:	mock grad = 0.010930603656089666, computed grad = 0.010930603793762175
Iteration: 377, Checking gradient for hidden b[0][22]:	mock grad = 0.044370183002295782, computed grad = 0.044370192669150890
Iteration: 377, Checking gradient for output layer W[149][0]:	mock grad = -0.026167200101234744, computed grad = -0.026167200093100782
Iteration: 378, Checking gradient for words E[90][20]:	mock grad = -0.037293102138402645, computed grad = -0.037293102291390080
Iteration: 378, Checking gradient for hidden W[241][26]:	mock grad = -0.016137679303862562, computed grad = -0.016137679443146029
Iteration: 378, Checking gradient for hidden b[0][44]:	mock grad = 0.013817528467602314, computed grad = 0.013817550354201902
Iteration: 378, Checking gradient for output layer W[127][0]:	mock grad = -0.179849729260606050, computed grad = -0.179849728140614340
current: 80, Cost = 0.414572, Correct(%) = 1, time = 5.24939
Iteration: 379, Checking gradient for words E[706][35]:	mock grad = -0.003972017695086372, computed grad = -0.003972017704663568
Iteration: 379, Checking gradient for hidden W[36][48]:	mock grad = 0.014918892423748664, computed grad = 0.014918892498281400
Iteration: 379, Checking gradient for hidden b[0][15]:	mock grad = -0.102371553444474683, computed grad = -0.102371577519752968
Iteration: 379, Checking gradient for output layer W[1][1]:	mock grad = -0.001630143856623523, computed grad = -0.001630143856662557
Iteration: 380, Checking gradient for words E[119][2]:	mock grad = 0.002340334302741720, computed grad = 0.002340334271027726
Iteration: 380, Checking gradient for hidden W[22][8]:	mock grad = -0.004217561743291576, computed grad = -0.004217561837418344
Iteration: 380, Checking gradient for hidden b[0][22]:	mock grad = -0.054314220622214693, computed grad = -0.054314233521206033
Iteration: 380, Checking gradient for output layer W[56][0]:	mock grad = 0.181449253242343955, computed grad = 0.181449252113395132
Iteration: 381, Checking gradient for words E[225][37]:	mock grad = 0.002443244928512778, computed grad = 0.002443244946397211
Iteration: 381, Checking gradient for hidden W[230][26]:	mock grad = 0.005306047098474487, computed grad = 0.005306047139621384
Iteration: 381, Checking gradient for hidden b[0][10]:	mock grad = 0.101936780666112004, computed grad = 0.101936798813319796
Iteration: 381, Checking gradient for output layer W[108][0]:	mock grad = 0.216618650446559480, computed grad = 0.216618648753354792
Iteration: 382, Checking gradient for words E[250][24]:	mock grad = 0.018350632858654681, computed grad = 0.018350632763379636
Iteration: 382, Checking gradient for hidden W[40][4]:	mock grad = 0.007288542431371781, computed grad = 0.007288542493755176
Iteration: 382, Checking gradient for hidden b[0][11]:	mock grad = -0.119061229460371010, computed grad = -0.119061264295351590
Iteration: 382, Checking gradient for output layer W[0][1]:	mock grad = 0.098717515505719611, computed grad = 0.098717515390377999
Iteration: 383, Checking gradient for words E[34][9]:	mock grad = 0.004818511232906886, computed grad = 0.004818511254358031
Iteration: 383, Checking gradient for hidden W[117][12]:	mock grad = 0.012360176131154255, computed grad = 0.012360176230975760
Iteration: 383, Checking gradient for hidden b[0][25]:	mock grad = 0.004071218468737481, computed grad = 0.004071211480805886
Iteration: 383, Checking gradient for output layer W[23][0]:	mock grad = 0.112460324026675274, computed grad = 0.112460323868073975
Iteration: 384, Checking gradient for words E[10][27]:	mock grad = -0.023376927550805560, computed grad = -0.023376927563057502
Iteration: 384, Checking gradient for hidden W[154][11]:	mock grad = -0.013576044669005860, computed grad = -0.013576044837296278
Iteration: 384, Checking gradient for hidden b[0][10]:	mock grad = 0.107586356776184777, computed grad = 0.107586380628946487
Iteration: 384, Checking gradient for output layer W[20][1]:	mock grad = 0.012867518131592925, computed grad = 0.012867518131212062
Iteration: 385, Checking gradient for words E[144][31]:	mock grad = -0.003007104418889517, computed grad = -0.003007104412113968
Iteration: 385, Checking gradient for hidden W[223][27]:	mock grad = -0.012140503474034059, computed grad = -0.012140503504653328
Iteration: 385, Checking gradient for hidden b[0][2]:	mock grad = -0.070315543189092278, computed grad = -0.070315563327942615
Iteration: 385, Checking gradient for output layer W[130][1]:	mock grad = 0.131201122195251374, computed grad = 0.131201121769494028
Iteration: 386, Checking gradient for words E[537][3]:	mock grad = -0.011931004447324201, computed grad = -0.011931004420725689
Iteration: 386, Checking gradient for hidden W[88][38]:	mock grad = -0.017487509532981838, computed grad = -0.017487509664136789
Iteration: 386, Checking gradient for hidden b[0][31]:	mock grad = -0.048155759117013419, computed grad = -0.048155772252688848
Iteration: 386, Checking gradient for output layer W[137][1]:	mock grad = -0.165566057491184360, computed grad = -0.165566056711709597
Iteration: 387, Checking gradient for words E[34][10]:	mock grad = 0.000069774065747374, computed grad = 0.000069774122148432
Iteration: 387, Checking gradient for hidden W[189][42]:	mock grad = -0.018689431638213572, computed grad = -0.018689431852721584
Iteration: 387, Checking gradient for hidden b[0][38]:	mock grad = 0.076573186723327691, computed grad = 0.076573202643422419
Iteration: 387, Checking gradient for output layer W[103][1]:	mock grad = -0.105997728582246653, computed grad = -0.105997728364841573
Iteration: 388, Checking gradient for words E[58][49]:	mock grad = 0.038514903238473508, computed grad = 0.038514903317257820
Iteration: 388, Checking gradient for hidden W[99][21]:	mock grad = -0.012889331872018017, computed grad = -0.012889332007034597
Iteration: 388, Checking gradient for hidden b[0][34]:	mock grad = 0.031038645772862861, computed grad = 0.031038649024686862
Iteration: 388, Checking gradient for output layer W[32][0]:	mock grad = 0.116370090711581664, computed grad = 0.116370090482555255
current: 90, Cost = 0.49646, Correct(%) = 1, time = 6.1985
Iteration: 389, Checking gradient for words E[776][24]:	mock grad = 0.003660290051721304, computed grad = 0.003660290100602048
Iteration: 389, Checking gradient for hidden W[212][36]:	mock grad = -0.009317166592304194, computed grad = -0.009317166618335914
Iteration: 389, Checking gradient for hidden b[0][45]:	mock grad = 0.012289702413958992, computed grad = 0.012289705532170269
Iteration: 389, Checking gradient for output layer W[104][0]:	mock grad = -0.106564929039765222, computed grad = -0.106564928865580943
Iteration: 390, Checking gradient for words E[97][18]:	mock grad = 0.000477505980533977, computed grad = 0.000477506016575456
Iteration: 390, Checking gradient for hidden W[14][47]:	mock grad = -0.002176998421171694, computed grad = -0.002176998423209947
Iteration: 390, Checking gradient for hidden b[0][20]:	mock grad = 0.104838279528507439, computed grad = 0.104838296104369635
Iteration: 390, Checking gradient for output layer W[116][1]:	mock grad = -0.161284487174812030, computed grad = -0.161284486137713345
Iteration: 391, Checking gradient for words E[48][17]:	mock grad = 0.046357143622888231, computed grad = 0.046357143716365727
Iteration: 391, Checking gradient for hidden W[52][35]:	mock grad = -0.004834285092403245, computed grad = -0.004834285138501294
Iteration: 391, Checking gradient for hidden b[0][37]:	mock grad = 0.063198342947018427, computed grad = 0.063198355798939190
Iteration: 391, Checking gradient for output layer W[20][1]:	mock grad = 0.026160236441502560, computed grad = 0.026160236438039643
Iteration: 392, Checking gradient for words E[653][30]:	mock grad = 0.027246016812421114, computed grad = 0.023653547961471795
Iteration: 392, Checking gradient for hidden W[89][45]:	mock grad = -0.001074259153382995, computed grad = -0.001074259161983555
Iteration: 392, Checking gradient for hidden b[0][18]:	mock grad = 0.071858884593045680, computed grad = 0.071858890008841034
Iteration: 392, Checking gradient for output layer W[2][1]:	mock grad = -0.068463105671434654, computed grad = -0.068463105576536246
Iteration: 393, Checking gradient for words E[118][49]:	mock grad = 0.008581544178543066, computed grad = 0.008581544206984733
Iteration: 393, Checking gradient for hidden W[120][17]:	mock grad = 0.004953491483689065, computed grad = 0.004953491548319418
Iteration: 393, Checking gradient for hidden b[0][3]:	mock grad = 0.040786069533366165, computed grad = 0.040786069628780494
Iteration: 393, Checking gradient for output layer W[95][1]:	mock grad = -0.144957652602195619, computed grad = -0.144957652104637874
Iteration: 394, Checking gradient for words E[452][33]:	mock grad = -0.001686173592391205, computed grad = -0.001686173593423204
Iteration: 394, Checking gradient for hidden W[216][35]:	mock grad = 0.000213596259651672, computed grad = 0.000213596260400505
Iteration: 394, Checking gradient for hidden b[0][12]:	mock grad = -0.080813674839225191, computed grad = -0.080813701431204046
Iteration: 394, Checking gradient for output layer W[81][0]:	mock grad = 0.060340126205843392, computed grad = 0.060340126144594053
Iteration: 395, Checking gradient for words E[102][34]:	mock grad = -0.028390300556568882, computed grad = -0.028390300711393784
Iteration: 395, Checking gradient for hidden W[160][3]:	mock grad = -0.004136177323754708, computed grad = -0.004136177333752563
Iteration: 395, Checking gradient for hidden b[0][14]:	mock grad = 0.097935340075294697, computed grad = 0.097935383270322057
Iteration: 395, Checking gradient for output layer W[148][0]:	mock grad = -0.232769844427327399, computed grad = -0.232769843036317092
Iteration: 396, Checking gradient for words E[161][18]:	mock grad = 0.002756606662102090, computed grad = 0.002756606658881787
Iteration: 396, Checking gradient for hidden W[173][28]:	mock grad = -0.012262273027113801, computed grad = -0.012262273019543350
Iteration: 396, Checking gradient for hidden b[0][26]:	mock grad = -0.038472208241990824, computed grad = -0.038472215758073698
Iteration: 396, Checking gradient for output layer W[149][0]:	mock grad = 0.109767870085014652, computed grad = 0.109767869868586457
Iteration: 397, Checking gradient for words E[119][31]:	mock grad = 0.033937826853069897, computed grad = 0.033937826994653995
Iteration: 397, Checking gradient for hidden W[66][24]:	mock grad = -0.008002346762403523, computed grad = -0.008002346728975179
Iteration: 397, Checking gradient for hidden b[0][1]:	mock grad = 0.163061787272600345, computed grad = 0.163061850369457806
Iteration: 397, Checking gradient for output layer W[129][1]:	mock grad = 0.201035323241671726, computed grad = 0.201035323137393779
Iteration: 398, Checking gradient for words E[520][40]:	mock grad = -0.013172814217587980, computed grad = -0.013172814255311381
Iteration: 398, Checking gradient for hidden W[122][3]:	mock grad = -0.005615870716346727, computed grad = -0.005615870733173162
Iteration: 398, Checking gradient for hidden b[0][45]:	mock grad = 0.010132594008671614, computed grad = 0.010132595331768855
Iteration: 398, Checking gradient for output layer W[118][1]:	mock grad = 0.006767876042784726, computed grad = 0.006767876042650636
current: 100, Cost = 0.498047, Correct(%) = 1, time = 6.98466
Iteration: 399, Checking gradient for words E[362][3]:	mock grad = 0.000767119992639564, computed grad = 0.000767119994855253
Iteration: 399, Checking gradient for hidden W[49][8]:	mock grad = -0.004300502131793227, computed grad = -0.004300502410693630
Iteration: 399, Checking gradient for hidden b[0][12]:	mock grad = 0.090166655075357705, computed grad = 0.090166682137786197
Iteration: 399, Checking gradient for output layer W[1][1]:	mock grad = -0.024112905582185373, computed grad = -0.024112905580220508
current: 4, Correct(%) = 1, time = 7.03702
Dev start.
Dev finished. Total time taken is: 0.641778
dev:
Accuracy:	P=100/100=1
Test start.
Test finished. Total time taken is: 0.546791
test:
Accuracy:	P=61/100=0.61
##### Iteration 4
random: 0, 99
Iteration: 400, Checking gradient for words E[21][1]:	mock grad = 0.014705371980366921, computed grad = 0.014705371991937964
Iteration: 400, Checking gradient for hidden W[160][40]:	mock grad = 0.004102995489363437, computed grad = 0.004102995498358972
Iteration: 400, Checking gradient for hidden b[0][2]:	mock grad = 0.057580928134126630, computed grad = 0.057580961268642754
Iteration: 400, Checking gradient for output layer W[19][1]:	mock grad = -0.036749084875276061, computed grad = -0.036749084845181419
Iteration: 401, Checking gradient for words E[78][5]:	mock grad = 0.006094386755556291, computed grad = 0.006094386770821185
Iteration: 401, Checking gradient for hidden W[105][18]:	mock grad = -0.007342071116717142, computed grad = -0.007342071142380075
Iteration: 401, Checking gradient for hidden b[0][25]:	mock grad = 0.004558613856703309, computed grad = 0.004558608803605181
Iteration: 401, Checking gradient for output layer W[73][0]:	mock grad = 0.008146055471947022, computed grad = 0.008146055471811582
Iteration: 402, Checking gradient for words E[104][13]:	mock grad = -0.011321617079590451, computed grad = -0.011321617106307386
Iteration: 402, Checking gradient for hidden W[242][24]:	mock grad = -0.009134384321402012, computed grad = -0.009134384348103564
Iteration: 402, Checking gradient for hidden b[0][3]:	mock grad = 0.032841117395016051, computed grad = 0.032841116104428685
Iteration: 402, Checking gradient for output layer W[88][1]:	mock grad = -0.177462873508310581, computed grad = -0.177462871372908754
Iteration: 403, Checking gradient for words E[139][42]:	mock grad = 0.019571787112149241, computed grad = 0.019571787159818495
Iteration: 403, Checking gradient for hidden W[61][3]:	mock grad = -0.003074895920274212, computed grad = -0.003074895989999725
Iteration: 403, Checking gradient for hidden b[0][42]:	mock grad = 0.075847012967478200, computed grad = 0.075847028865360128
Iteration: 403, Checking gradient for output layer W[34][1]:	mock grad = -0.013408011293958300, computed grad = -0.013408011293675092
Iteration: 404, Checking gradient for words E[157][32]:	mock grad = 0.029222475317586261, computed grad = 0.029222475339745826
Iteration: 404, Checking gradient for hidden W[92][37]:	mock grad = -0.013944140327737387, computed grad = -0.013944140417528969
Iteration: 404, Checking gradient for hidden b[0][12]:	mock grad = -0.088905273615408342, computed grad = -0.088905303158619303
Iteration: 404, Checking gradient for output layer W[90][0]:	mock grad = 0.091567374520401534, computed grad = 0.091567374388425563
Iteration: 405, Checking gradient for words E[99][46]:	mock grad = -0.011148757872586623, computed grad = -0.011148757852568488
Iteration: 405, Checking gradient for hidden W[217][49]:	mock grad = 0.003369643472778128, computed grad = 0.003369643486935520
Iteration: 405, Checking gradient for hidden b[0][48]:	mock grad = -0.082377250432502658, computed grad = -0.082377260015876572
Iteration: 405, Checking gradient for output layer W[10][1]:	mock grad = 0.031912332072558947, computed grad = 0.031912332063124535
Iteration: 406, Checking gradient for words E[267][49]:	mock grad = -0.019208971615225501, computed grad = -0.019208971696904338
Iteration: 406, Checking gradient for hidden W[183][35]:	mock grad = -0.001540261461374648, computed grad = -0.001540261453734516
Iteration: 406, Checking gradient for hidden b[0][19]:	mock grad = 0.032047015220915132, computed grad = 0.032047009053435244
Iteration: 406, Checking gradient for output layer W[84][0]:	mock grad = 0.106021204620082576, computed grad = 0.106021203940765571
Iteration: 407, Checking gradient for words E[69][26]:	mock grad = -0.063091397260739424, computed grad = -0.063091397371162886
Iteration: 407, Checking gradient for hidden W[139][40]:	mock grad = 0.012583929726583953, computed grad = 0.012583929808210404
Iteration: 407, Checking gradient for hidden b[0][6]:	mock grad = 0.052348815887726108, computed grad = 0.052348831890451239
Iteration: 407, Checking gradient for output layer W[72][1]:	mock grad = 0.128017525488133277, computed grad = 0.128017525365956730
Iteration: 408, Checking gradient for words E[102][43]:	mock grad = 0.014452289539640395, computed grad = 0.014452289598402401
Iteration: 408, Checking gradient for hidden W[80][7]:	mock grad = 0.019876495135573435, computed grad = 0.019876495225489454
Iteration: 408, Checking gradient for hidden b[0][20]:	mock grad = -0.115348613283217105, computed grad = -0.115348647719609607
Iteration: 408, Checking gradient for output layer W[89][0]:	mock grad = 0.168757589931373131, computed grad = 0.168757588557463034
current: 10, Cost = 0.443613, Correct(%) = 1, time = 0.771024
Iteration: 409, Checking gradient for words E[6][25]:	mock grad = -0.007246476590672124, computed grad = -0.010788352311238893
Iteration: 409, Checking gradient for hidden W[19][12]:	mock grad = -0.017617670706543409, computed grad = -0.017617670899413559
Iteration: 409, Checking gradient for hidden b[0][29]:	mock grad = -0.087563637713505837, computed grad = -0.087563654650970019
Iteration: 409, Checking gradient for output layer W[62][0]:	mock grad = -0.166235719056467834, computed grad = -0.166235717971653457
Iteration: 410, Checking gradient for words E[58][11]:	mock grad = -0.012789919059424948, computed grad = -0.012789919075419147
Iteration: 410, Checking gradient for hidden W[15][20]:	mock grad = -0.000937084186830583, computed grad = -0.000937084150055611
Iteration: 410, Checking gradient for hidden b[0][38]:	mock grad = -0.065433931505209930, computed grad = -0.065433945745154307
Iteration: 410, Checking gradient for output layer W[95][0]:	mock grad = 0.043983416551263144, computed grad = 0.043983416511468365
Iteration: 411, Checking gradient for words E[48][13]:	mock grad = 0.004396309469689719, computed grad = 0.004396309471484280
Iteration: 411, Checking gradient for hidden W[66][28]:	mock grad = 0.000030585287913443, computed grad = 0.000030585288838368
Iteration: 411, Checking gradient for hidden b[0][15]:	mock grad = -0.121438822879016417, computed grad = -0.121438852465098085
Iteration: 411, Checking gradient for output layer W[149][1]:	mock grad = -0.037870906225812018, computed grad = -0.037870906218408523
Iteration: 412, Checking gradient for words E[350][3]:	mock grad = -0.022726290616253975, computed grad = -0.022726290618133652
Iteration: 412, Checking gradient for hidden W[62][27]:	mock grad = 0.004429543540129810, computed grad = 0.004429543544288140
Iteration: 412, Checking gradient for hidden b[0][17]:	mock grad = -0.018629054485769236, computed grad = -0.018629058507223340
Iteration: 412, Checking gradient for output layer W[127][0]:	mock grad = -0.180319271075535248, computed grad = -0.180319269016055300
Iteration: 413, Checking gradient for words E[309][20]:	mock grad = 0.011715135696616086, computed grad = 0.011469544463839815
Iteration: 413, Checking gradient for hidden W[98][15]:	mock grad = 0.007382365307229133, computed grad = 0.007382365328439159
Iteration: 413, Checking gradient for hidden b[0][16]:	mock grad = 0.021459852711264160, computed grad = 0.021459860521728828
Iteration: 413, Checking gradient for output layer W[130][0]:	mock grad = 0.106131156558331163, computed grad = 0.106131156225043349
Iteration: 414, Checking gradient for words E[376][45]:	mock grad = -0.007899358988150418, computed grad = -0.007899359076445327
Iteration: 414, Checking gradient for hidden W[89][46]:	mock grad = -0.032780933271969737, computed grad = -0.032780933984818426
Iteration: 414, Checking gradient for hidden b[0][30]:	mock grad = 0.001817143010895972, computed grad = 0.001817140226732560
Iteration: 414, Checking gradient for output layer W[77][0]:	mock grad = 0.126977754241919261, computed grad = 0.126977754196737375
Iteration: 415, Checking gradient for words E[382][27]:	mock grad = 0.015504342741112787, computed grad = 0.015504342607544051
Iteration: 415, Checking gradient for hidden W[150][11]:	mock grad = 0.013898200120210369, computed grad = 0.013898200245771963
Iteration: 415, Checking gradient for hidden b[0][4]:	mock grad = 0.023283222960968075, computed grad = 0.023283231660712160
Iteration: 415, Checking gradient for output layer W[67][0]:	mock grad = 0.019887890463560209, computed grad = 0.019887890461746868
Iteration: 416, Checking gradient for words E[387][14]:	mock grad = -0.031138993060286779, computed grad = -0.031138993211722258
Iteration: 416, Checking gradient for hidden W[208][39]:	mock grad = 0.021657460563900832, computed grad = 0.021657460707816226
Iteration: 416, Checking gradient for hidden b[0][19]:	mock grad = 0.044935885758601302, computed grad = 0.044935880076634627
Iteration: 416, Checking gradient for output layer W[36][0]:	mock grad = -0.045616205519544151, computed grad = -0.045616205503371893
Iteration: 417, Checking gradient for words E[416][14]:	mock grad = 0.008882300453538772, computed grad = 0.008882300475762330
Iteration: 417, Checking gradient for hidden W[93][15]:	mock grad = 0.004396963431546608, computed grad = 0.004396963438510296
Iteration: 417, Checking gradient for hidden b[0][6]:	mock grad = -0.036882323948772555, computed grad = -0.036882336776721272
Iteration: 417, Checking gradient for output layer W[72][0]:	mock grad = 0.098711914872012807, computed grad = 0.098711914338702633
Iteration: 418, Checking gradient for words E[427][38]:	mock grad = 0.012007897303684123, computed grad = 0.012007897348392119
Iteration: 418, Checking gradient for hidden W[93][14]:	mock grad = -0.016017380383009927, computed grad = -0.016017380432747780
Iteration: 418, Checking gradient for hidden b[0][16]:	mock grad = -0.017946743364721796, computed grad = -0.017946749093486942
Iteration: 418, Checking gradient for output layer W[100][0]:	mock grad = -0.121111832433046240, computed grad = -0.121111831675261766
current: 20, Cost = 0.505535, Correct(%) = 1, time = 1.43215
Iteration: 419, Checking gradient for words E[58][12]:	mock grad = -0.020257936246770303, computed grad = -0.020257936373282680
Iteration: 419, Checking gradient for hidden W[150][48]:	mock grad = 0.002579816844594518, computed grad = 0.002579816809063370
Iteration: 419, Checking gradient for hidden b[0][19]:	mock grad = -0.066544082148456418, computed grad = -0.066544100740771037
Iteration: 419, Checking gradient for output layer W[50][1]:	mock grad = 0.219371875489426937, computed grad = 0.219371874098582220
Iteration: 420, Checking gradient for words E[124][32]:	mock grad = 0.007877804938827282, computed grad = 0.007877804959181205
Iteration: 420, Checking gradient for hidden W[48][5]:	mock grad = -0.000663090365893293, computed grad = -0.000663090502047961
Iteration: 420, Checking gradient for hidden b[0][17]:	mock grad = -0.019367574151274436, computed grad = -0.019367577998421415
Iteration: 420, Checking gradient for output layer W[114][1]:	mock grad = 0.141136352108106600, computed grad = 0.141136351375234781
Iteration: 421, Checking gradient for words E[102][8]:	mock grad = 0.006929963111967918, computed grad = 0.006929963165193704
Iteration: 421, Checking gradient for hidden W[62][5]:	mock grad = -0.009179667901648703, computed grad = -0.009179667976046413
Iteration: 421, Checking gradient for hidden b[0][37]:	mock grad = -0.072302830822990050, computed grad = -0.072302853840963688
Iteration: 421, Checking gradient for output layer W[37][1]:	mock grad = 0.033037268400204489, computed grad = 0.033037268396487650
Iteration: 422, Checking gradient for words E[447][26]:	mock grad = 0.014200344749221161, computed grad = 0.014200344808922447
Iteration: 422, Checking gradient for hidden W[27][9]:	mock grad = 0.005306897726681248, computed grad = 0.005306897727611317
Iteration: 422, Checking gradient for hidden b[0][30]:	mock grad = -0.003371897651038847, computed grad = -0.003371903932925461
Iteration: 422, Checking gradient for output layer W[31][1]:	mock grad = 0.056582983641001450, computed grad = 0.056582983576380863
Iteration: 423, Checking gradient for words E[455][27]:	mock grad = -0.016382243035489852, computed grad = -0.016382242915620328
Iteration: 423, Checking gradient for hidden W[96][30]:	mock grad = -0.003801633524563863, computed grad = -0.003801633562391282
Iteration: 423, Checking gradient for hidden b[0][30]:	mock grad = 0.001665346903961806, computed grad = 0.001665345363736383
Iteration: 423, Checking gradient for output layer W[13][1]:	mock grad = 0.063287503266812317, computed grad = 0.063287503194038752
Iteration: 424, Checking gradient for words E[433][33]:	mock grad = -0.031203501085208529, computed grad = -0.031203501160775273
Iteration: 424, Checking gradient for hidden W[227][14]:	mock grad = -0.017004319653485256, computed grad = -0.017004319866403758
Iteration: 424, Checking gradient for hidden b[0][13]:	mock grad = -0.142487794861451134, computed grad = -0.142487833768050243
Iteration: 424, Checking gradient for output layer W[32][0]:	mock grad = -0.109850621839824303, computed grad = -0.109850621576653790
Iteration: 425, Checking gradient for words E[49][31]:	mock grad = 0.007800986905670548, computed grad = 0.007800986856100355
Iteration: 425, Checking gradient for hidden W[246][23]:	mock grad = -0.019353255718529416, computed grad = -0.019353255871996728
Iteration: 425, Checking gradient for hidden b[0][45]:	mock grad = -0.011442086390506256, computed grad = -0.011442089642855990
Iteration: 425, Checking gradient for output layer W[85][0]:	mock grad = -0.182897570405043952, computed grad = -0.182897568798881749
Iteration: 426, Checking gradient for words E[152][40]:	mock grad = 0.005893283364616364, computed grad = 0.005893283386180467
Iteration: 426, Checking gradient for hidden W[77][29]:	mock grad = 0.009845303536981653, computed grad = 0.009845303578542127
Iteration: 426, Checking gradient for hidden b[0][3]:	mock grad = 0.032605032854821081, computed grad = 0.032605032562369504
Iteration: 426, Checking gradient for output layer W[46][0]:	mock grad = 0.091439315268976307, computed grad = 0.091439314934975535
Iteration: 427, Checking gradient for words E[485][34]:	mock grad = 0.018968997414547939, computed grad = 0.018968997486442516
Iteration: 427, Checking gradient for hidden W[235][25]:	mock grad = -0.005994843988832432, computed grad = -0.005994844060824055
Iteration: 427, Checking gradient for hidden b[0][41]:	mock grad = -0.054743393959721942, computed grad = -0.054743411193139677
Iteration: 427, Checking gradient for output layer W[54][1]:	mock grad = 0.116069220435499609, computed grad = 0.116069220329256401
Iteration: 428, Checking gradient for words E[35][36]:	mock grad = -0.010820682239343071, computed grad = -0.010820682207434002
Iteration: 428, Checking gradient for hidden W[225][49]:	mock grad = 0.002277905171071959, computed grad = 0.002277905192478755
Iteration: 428, Checking gradient for hidden b[0][16]:	mock grad = 0.019899361070579991, computed grad = 0.019899368176710907
Iteration: 428, Checking gradient for output layer W[92][1]:	mock grad = 0.095516166265852132, computed grad = 0.095516165916044643
current: 30, Cost = 0.407828, Correct(%) = 1, time = 2.12641
Iteration: 429, Checking gradient for words E[316][24]:	mock grad = 0.017812819549489767, computed grad = 0.017812819699658797
Iteration: 429, Checking gradient for hidden W[243][44]:	mock grad = 0.016715636175279602, computed grad = 0.016715636319472833
Iteration: 429, Checking gradient for hidden b[0][43]:	mock grad = 0.012107917802595125, computed grad = 0.012107921261928710
Iteration: 429, Checking gradient for output layer W[69][1]:	mock grad = -0.029645069585304551, computed grad = -0.029645069576844985
Iteration: 430, Checking gradient for words E[156][37]:	mock grad = -0.015168026910261645, computed grad = -0.015168026938996574
Iteration: 430, Checking gradient for hidden W[118][21]:	mock grad = -0.006860992257523568, computed grad = -0.006860992292969156
Iteration: 430, Checking gradient for hidden b[0][35]:	mock grad = 0.014824182792250173, computed grad = 0.014824183467892500
Iteration: 430, Checking gradient for output layer W[91][0]:	mock grad = -0.024444759945119188, computed grad = -0.024444759940507450
Iteration: 431, Checking gradient for words E[102][38]:	mock grad = 0.003462860236669307, computed grad = 0.003462860268261862
Iteration: 431, Checking gradient for hidden W[200][0]:	mock grad = 0.006165784390999018, computed grad = 0.006165784367270505
Iteration: 431, Checking gradient for hidden b[0][12]:	mock grad = 0.102977177183494195, computed grad = 0.102977212341670579
Iteration: 431, Checking gradient for output layer W[127][0]:	mock grad = 0.194439358786990812, computed grad = 0.194439358300120269
Iteration: 432, Checking gradient for words E[515][49]:	mock grad = -0.031776395694599335, computed grad = -0.031776395702293986
Iteration: 432, Checking gradient for hidden W[109][30]:	mock grad = -0.011741139934157152, computed grad = -0.011741140117183821
Iteration: 432, Checking gradient for hidden b[0][35]:	mock grad = 0.016336801940824985, computed grad = 0.016336804821313194
Iteration: 432, Checking gradient for output layer W[55][1]:	mock grad = 0.197993121069744982, computed grad = 0.197993118015152736
Iteration: 433, Checking gradient for words E[25][29]:	mock grad = 0.035121779988989132, computed grad = 0.035121780064126008
Iteration: 433, Checking gradient for hidden W[124][44]:	mock grad = 0.024876033929782659, computed grad = 0.024876033817353937
Iteration: 433, Checking gradient for hidden b[0][14]:	mock grad = 0.064326653056279159, computed grad = 0.064326677039196117
Iteration: 433, Checking gradient for output layer W[7][1]:	mock grad = -0.067521209674903382, computed grad = -0.067521209547030184
Iteration: 434, Checking gradient for words E[776][14]:	mock grad = -0.026587112292142390, computed grad = -0.026587112446003855
Iteration: 434, Checking gradient for hidden W[182][43]:	mock grad = 0.000150104507101734, computed grad = 0.000150104506482092
Iteration: 434, Checking gradient for hidden b[0][28]:	mock grad = 0.026130744459174071, computed grad = 0.026130771894382357
Iteration: 434, Checking gradient for output layer W[1][0]:	mock grad = 0.005330641307477180, computed grad = 0.005330641307516149
Iteration: 435, Checking gradient for words E[34][32]:	mock grad = -0.031639650526910090, computed grad = -0.031639650517612701
Iteration: 435, Checking gradient for hidden W[115][39]:	mock grad = -0.004560708857459517, computed grad = -0.004560708861111944
Iteration: 435, Checking gradient for hidden b[0][2]:	mock grad = 0.077646823453625391, computed grad = 0.077646866945304205
Iteration: 435, Checking gradient for output layer W[131][1]:	mock grad = -0.109861174475783807, computed grad = -0.109861174280226223
Iteration: 436, Checking gradient for words E[531][49]:	mock grad = -0.086154872453003684, computed grad = -0.086154872583905001
Iteration: 436, Checking gradient for hidden W[16][22]:	mock grad = -0.000040686616631369, computed grad = -0.000040686613310138
Iteration: 436, Checking gradient for hidden b[0][46]:	mock grad = 0.199034230047634431, computed grad = 0.199034296170270436
Iteration: 436, Checking gradient for output layer W[90][0]:	mock grad = -0.038322791348577301, computed grad = -0.038322791342892119
Iteration: 437, Checking gradient for words E[225][41]:	mock grad = -0.004976110132420830, computed grad = -0.004976110123677510
Iteration: 437, Checking gradient for hidden W[9][26]:	mock grad = 0.005623705838231352, computed grad = 0.005623705849658226
Iteration: 437, Checking gradient for hidden b[0][11]:	mock grad = 0.101366749540754419, computed grad = 0.101366771312638923
Iteration: 437, Checking gradient for output layer W[34][1]:	mock grad = 0.000909568428464347, computed grad = 0.000909568428399525
Iteration: 438, Checking gradient for words E[25][28]:	mock grad = -0.012090559874722295, computed grad = -0.012090559930582521
Iteration: 438, Checking gradient for hidden W[68][43]:	mock grad = -0.000194766020811254, computed grad = -0.000194766020912435
Iteration: 438, Checking gradient for hidden b[0][31]:	mock grad = -0.051677908057323751, computed grad = -0.051677922572000205
Iteration: 438, Checking gradient for output layer W[88][1]:	mock grad = 0.142155510037877963, computed grad = 0.142155509686550302
current: 40, Cost = 0.373869, Correct(%) = 1, time = 2.56521
Iteration: 439, Checking gradient for words E[31][35]:	mock grad = -0.022916611893630812, computed grad = -0.022916611948951785
Iteration: 439, Checking gradient for hidden W[197][26]:	mock grad = -0.011420195811739919, computed grad = -0.011420195912487179
Iteration: 439, Checking gradient for hidden b[0][40]:	mock grad = -0.104099313401218030, computed grad = -0.104099343562566782
Iteration: 439, Checking gradient for output layer W[97][1]:	mock grad = -0.160749424183620482, computed grad = -0.160749422342166465
Iteration: 440, Checking gradient for words E[25][34]:	mock grad = -0.005916627997121227, computed grad = -0.005916628038988327
Iteration: 440, Checking gradient for hidden W[101][17]:	mock grad = 0.001144965840332857, computed grad = 0.001144965856541861
Iteration: 440, Checking gradient for hidden b[0][6]:	mock grad = -0.038651798885624977, computed grad = -0.038651811538509599
Iteration: 440, Checking gradient for output layer W[71][0]:	mock grad = 0.073695957612224250, computed grad = 0.073695957441301155
Iteration: 441, Checking gradient for words E[25][25]:	mock grad = -0.062039486855902659, computed grad = -0.062039486916504023
Iteration: 441, Checking gradient for hidden W[18][24]:	mock grad = -0.001984628567097690, computed grad = -0.001984628564889537
Iteration: 441, Checking gradient for hidden b[0][41]:	mock grad = -0.048658198595563640, computed grad = -0.048658208452444056
Iteration: 441, Checking gradient for output layer W[131][0]:	mock grad = 0.059632508886908742, computed grad = 0.059632508874598970
Iteration: 442, Checking gradient for words E[6][22]:	mock grad = -0.045194817705818124, computed grad = -0.045194817989644809
Iteration: 442, Checking gradient for hidden W[137][28]:	mock grad = 0.023344932410351849, computed grad = 0.023344932497518043
Iteration: 442, Checking gradient for hidden b[0][23]:	mock grad = 0.173809402402580471, computed grad = 0.173809426691823976
Iteration: 442, Checking gradient for output layer W[65][0]:	mock grad = -0.214795063847306178, computed grad = -0.214795062745832571
Iteration: 443, Checking gradient for words E[560][30]:	mock grad = -0.006523523286006139, computed grad = -0.006523523292786337
Iteration: 443, Checking gradient for hidden W[97][13]:	mock grad = 0.012941119661935563, computed grad = 0.012941119706872110
Iteration: 443, Checking gradient for hidden b[0][46]:	mock grad = 0.205824222360373454, computed grad = 0.205824278557838858
Iteration: 443, Checking gradient for output layer W[37][1]:	mock grad = -0.051300268229692580, computed grad = -0.051300268223368001
Iteration: 444, Checking gradient for words E[119][30]:	mock grad = 0.007838012426930607, computed grad = 0.007838012408548711
Iteration: 444, Checking gradient for hidden W[69][33]:	mock grad = -0.009410177371715900, computed grad = -0.009410177401901944
Iteration: 444, Checking gradient for hidden b[0][27]:	mock grad = -0.038039101076026149, computed grad = -0.038039116866461102
Iteration: 444, Checking gradient for output layer W[29][1]:	mock grad = 0.024392841463050097, computed grad = 0.024392841454748727
Iteration: 445, Checking gradient for words E[544][21]:	mock grad = -0.003779061153064145, computed grad = -0.003779061196216085
Iteration: 445, Checking gradient for hidden W[106][18]:	mock grad = 0.013555750173588343, computed grad = 0.013555750311566667
Iteration: 445, Checking gradient for hidden b[0][41]:	mock grad = -0.047279350732676306, computed grad = -0.047279363624652557
Iteration: 445, Checking gradient for output layer W[8][0]:	mock grad = 0.093633031984452497, computed grad = 0.093633031886218118
Iteration: 446, Checking gradient for words E[124][23]:	mock grad = 0.002873230297489870, computed grad = 0.002873230279800120
Iteration: 446, Checking gradient for hidden W[96][31]:	mock grad = -0.009372472088126083, computed grad = -0.009372472114710135
Iteration: 446, Checking gradient for hidden b[0][14]:	mock grad = -0.052762003913853217, computed grad = -0.052762026012365411
Iteration: 446, Checking gradient for output layer W[149][0]:	mock grad = 0.035390180654043535, computed grad = 0.035390180633419387
Iteration: 447, Checking gradient for words E[600][39]:	mock grad = 0.016903104521182044, computed grad = 0.016903104555876791
Iteration: 447, Checking gradient for hidden W[65][38]:	mock grad = 0.002876911542576410, computed grad = 0.002876911572936328
Iteration: 447, Checking gradient for hidden b[0][48]:	mock grad = -0.094805897787819671, computed grad = -0.094805919148019571
Iteration: 447, Checking gradient for output layer W[137][0]:	mock grad = -0.167237449796159288, computed grad = -0.167237448322737875
Iteration: 448, Checking gradient for words E[31][3]:	mock grad = 0.014475889336756920, computed grad = 0.014475889378035381
Iteration: 448, Checking gradient for hidden W[214][44]:	mock grad = 0.008132318185000154, computed grad = 0.008132318201350987
Iteration: 448, Checking gradient for hidden b[0][10]:	mock grad = 0.083359656893211564, computed grad = 0.083359672330195173
Iteration: 448, Checking gradient for output layer W[7][1]:	mock grad = 0.054306377517576720, computed grad = 0.054306377441823810
current: 50, Cost = 0.385143, Correct(%) = 1, time = 3.45569
Iteration: 449, Checking gradient for words E[506][48]:	mock grad = 0.010964594745060952, computed grad = 0.010964594813597609
Iteration: 449, Checking gradient for hidden W[128][44]:	mock grad = 0.012115928418293676, computed grad = 0.012115928510418660
Iteration: 449, Checking gradient for hidden b[0][45]:	mock grad = -0.008574692735352496, computed grad = -0.008574693428494328
Iteration: 449, Checking gradient for output layer W[102][1]:	mock grad = -0.162668591881159896, computed grad = -0.162668590158097343
Iteration: 450, Checking gradient for words E[776][21]:	mock grad = -0.012529838324670806, computed grad = -0.012529838333725229
Iteration: 450, Checking gradient for hidden W[65][2]:	mock grad = -0.013657969234692757, computed grad = -0.013657969327824837
Iteration: 450, Checking gradient for hidden b[0][2]:	mock grad = 0.077602560416445421, computed grad = 0.077602597704536369
Iteration: 450, Checking gradient for output layer W[140][0]:	mock grad = 0.195247824423372407, computed grad = 0.195247824030967382
Iteration: 451, Checking gradient for words E[69][17]:	mock grad = 0.019730042763399513, computed grad = 0.019730042851742575
Iteration: 451, Checking gradient for hidden W[246][29]:	mock grad = 0.004513533066602404, computed grad = 0.004513533066392505
Iteration: 451, Checking gradient for hidden b[0][19]:	mock grad = 0.044641906263048758, computed grad = 0.044641900525600485
Iteration: 451, Checking gradient for output layer W[79][0]:	mock grad = 0.066561979151308348, computed grad = 0.066561979100698304
Iteration: 452, Checking gradient for words E[119][40]:	mock grad = -0.007546018603921123, computed grad = -0.007546018655981140
Iteration: 452, Checking gradient for hidden W[51][24]:	mock grad = -0.020373355375968849, computed grad = -0.020373355524763097
Iteration: 452, Checking gradient for hidden b[0][26]:	mock grad = 0.044360835372680985, computed grad = 0.044360848275867844
Iteration: 452, Checking gradient for output layer W[118][1]:	mock grad = 0.018211029550019298, computed grad = 0.018211029549381374
Iteration: 453, Checking gradient for words E[627][43]:	mock grad = 0.015757459916637684, computed grad = 0.015757459927678470
Iteration: 453, Checking gradient for hidden W[141][21]:	mock grad = 0.001389243702049070, computed grad = 0.001389243721232573
Iteration: 453, Checking gradient for hidden b[0][33]:	mock grad = -0.114425788859423561, computed grad = -0.114425814297355335
Iteration: 453, Checking gradient for output layer W[110][0]:	mock grad = 0.179438129745329578, computed grad = 0.179438128189350010
Iteration: 454, Checking gradient for words E[632][49]:	mock grad = -0.016649732417522989, computed grad = -0.016649732404960840
Iteration: 454, Checking gradient for hidden W[249][24]:	mock grad = 0.005904042543747323, computed grad = 0.005904042620352717
Iteration: 454, Checking gradient for hidden b[0][29]:	mock grad = 0.085337086278919339, computed grad = 0.085337116287459394
Iteration: 454, Checking gradient for output layer W[11][0]:	mock grad = 0.028604204691085044, computed grad = 0.028604204681375933
Iteration: 455, Checking gradient for words E[513][9]:	mock grad = -0.020350264420904152, computed grad = -0.019860545629858943
Iteration: 455, Checking gradient for hidden W[34][9]:	mock grad = -0.009224863977419506, computed grad = -0.009224864011556670
Iteration: 455, Checking gradient for hidden b[0][16]:	mock grad = -0.020661093754037463, computed grad = -0.020661102415222378
Iteration: 455, Checking gradient for output layer W[114][0]:	mock grad = -0.106891569150835419, computed grad = -0.106891568664691641
Iteration: 456, Checking gradient for words E[460][32]:	mock grad = -0.092329078575981161, computed grad = -0.088020249454498489
Iteration: 456, Checking gradient for hidden W[133][35]:	mock grad = 0.001827502972284734, computed grad = 0.001827502991923490
Iteration: 456, Checking gradient for hidden b[0][38]:	mock grad = -0.094550283979144556, computed grad = -0.094550308486486434
Iteration: 456, Checking gradient for output layer W[49][1]:	mock grad = -0.102801888062276081, computed grad = -0.102801887989271284
Iteration: 457, Checking gradient for words E[102][29]:	mock grad = 0.008474414120995810, computed grad = 0.009223788967973700
Iteration: 457, Checking gradient for hidden W[195][15]:	mock grad = -0.004700240565536085, computed grad = -0.004700240556941382
Iteration: 457, Checking gradient for hidden b[0][30]:	mock grad = 0.002375570204798949, computed grad = 0.002375576132685389
Iteration: 457, Checking gradient for output layer W[9][1]:	mock grad = 0.050083815497259376, computed grad = 0.050083815459046242
Iteration: 458, Checking gradient for words E[541][42]:	mock grad = 0.026012282701687317, computed grad = 0.026012282774954404
Iteration: 458, Checking gradient for hidden W[38][37]:	mock grad = -0.011339515346153739, computed grad = -0.011339515432102886
Iteration: 458, Checking gradient for hidden b[0][32]:	mock grad = -0.067115091501757096, computed grad = -0.067115110944693607
Iteration: 458, Checking gradient for output layer W[52][1]:	mock grad = -0.137563608735374876, computed grad = -0.137563608286350225
current: 60, Cost = 0.477694, Correct(%) = 1, time = 4.15327
Iteration: 459, Checking gradient for words E[648][7]:	mock grad = 0.019275071919705455, computed grad = 0.019275071942024071
Iteration: 459, Checking gradient for hidden W[207][47]:	mock grad = -0.007341543609207246, computed grad = -0.007341543674544831
Iteration: 459, Checking gradient for hidden b[0][29]:	mock grad = -0.090980925654127187, computed grad = -0.090980941470912666
Iteration: 459, Checking gradient for output layer W[60][0]:	mock grad = -0.139242428138924845, computed grad = -0.139242427673780367
Iteration: 460, Checking gradient for words E[148][11]:	mock grad = -0.034057435439299732, computed grad = -0.034057435496808522
Iteration: 460, Checking gradient for hidden W[171][17]:	mock grad = 0.004265180258783419, computed grad = 0.004265180314170017
Iteration: 460, Checking gradient for hidden b[0][40]:	mock grad = 0.128903895427329163, computed grad = 0.128903932763618084
Iteration: 460, Checking gradient for output layer W[48][0]:	mock grad = 0.123145062472268529, computed grad = 0.123145062164203567
Iteration: 461, Checking gradient for words E[137][10]:	mock grad = -0.005743966826621261, computed grad = -0.005743966827419240
Iteration: 461, Checking gradient for hidden W[180][8]:	mock grad = -0.011165358290654570, computed grad = -0.011165358345611446
Iteration: 461, Checking gradient for hidden b[0][6]:	mock grad = -0.051965204681037047, computed grad = -0.051965221714624713
Iteration: 461, Checking gradient for output layer W[145][0]:	mock grad = -0.111388554795366357, computed grad = -0.111388554687594926
Iteration: 462, Checking gradient for words E[58][10]:	mock grad = 0.021065142999709963, computed grad = 0.021065143164871930
Iteration: 462, Checking gradient for hidden W[88][0]:	mock grad = -0.011733428796123491, computed grad = -0.011733428826613903
Iteration: 462, Checking gradient for hidden b[0][14]:	mock grad = -0.055409387383187347, computed grad = -0.055409401357097036
Iteration: 462, Checking gradient for output layer W[141][1]:	mock grad = -0.113494623865229016, computed grad = -0.113494623584121879
Iteration: 463, Checking gradient for words E[434][15]:	mock grad = -0.009844139838788291, computed grad = -0.009844139805395469
Iteration: 463, Checking gradient for hidden W[26][34]:	mock grad = -0.002224169455999903, computed grad = -0.002224169454888296
Iteration: 463, Checking gradient for hidden b[0][27]:	mock grad = -0.043972798097813293, computed grad = -0.043972814931660109
Iteration: 463, Checking gradient for output layer W[106][0]:	mock grad = -0.065574218219177638, computed grad = -0.065574218180368210
Iteration: 464, Checking gradient for words E[316][36]:	mock grad = -0.006035156787626006, computed grad = -0.006035156783839941
Iteration: 464, Checking gradient for hidden W[116][41]:	mock grad = -0.006096216323830950, computed grad = -0.006096216334672587
Iteration: 464, Checking gradient for hidden b[0][23]:	mock grad = -0.172682078163149955, computed grad = -0.172682127847108069
Iteration: 464, Checking gradient for output layer W[93][1]:	mock grad = -0.010155878372025695, computed grad = -0.010155878371905874
Iteration: 465, Checking gradient for words E[118][16]:	mock grad = 0.031853240465645749, computed grad = 0.031853240535754043
Iteration: 465, Checking gradient for hidden W[91][14]:	mock grad = 0.012793769554653167, computed grad = 0.012793769600496307
Iteration: 465, Checking gradient for hidden b[0][39]:	mock grad = 0.079843325501233187, computed grad = 0.079843347808930543
Iteration: 465, Checking gradient for output layer W[143][1]:	mock grad = -0.160328555920519200, computed grad = -0.160328553586780664
Iteration: 466, Checking gradient for words E[102][25]:	mock grad = -0.005775165794391635, computed grad = -0.005775165808671127
Iteration: 466, Checking gradient for hidden W[168][31]:	mock grad = 0.001307258703703296, computed grad = 0.001307258708628334
Iteration: 466, Checking gradient for hidden b[0][35]:	mock grad = 0.013242873762947527, computed grad = 0.013242872746432065
Iteration: 466, Checking gradient for output layer W[115][1]:	mock grad = 0.029018740535707854, computed grad = 0.029018740528863756
Iteration: 467, Checking gradient for words E[34][13]:	mock grad = -0.012491212505905125, computed grad = -0.012491212469318072
Iteration: 467, Checking gradient for hidden W[80][19]:	mock grad = -0.021576205744028298, computed grad = -0.021576205838401197
Iteration: 467, Checking gradient for hidden b[0][28]:	mock grad = -0.012660206807513230, computed grad = -0.012660224400630343
Iteration: 467, Checking gradient for output layer W[12][1]:	mock grad = -0.069763857611981894, computed grad = -0.069763857556073491
Iteration: 468, Checking gradient for words E[616][1]:	mock grad = -0.014713775529057171, computed grad = -0.014713775540024194
Iteration: 468, Checking gradient for hidden W[232][2]:	mock grad = 0.025446498589348776, computed grad = 0.025446498678281124
Iteration: 468, Checking gradient for hidden b[0][38]:	mock grad = -0.066729329919884561, computed grad = -0.066729340520666133
Iteration: 468, Checking gradient for output layer W[80][1]:	mock grad = -0.105679239004535797, computed grad = -0.105679238632419042
current: 70, Cost = 0.362675, Correct(%) = 1, time = 4.76673
Iteration: 469, Checking gradient for words E[118][9]:	mock grad = -0.006567958969139198, computed grad = -0.006567958985285488
Iteration: 469, Checking gradient for hidden W[98][34]:	mock grad = -0.003924200916161080, computed grad = -0.003924200978333444
Iteration: 469, Checking gradient for hidden b[0][36]:	mock grad = 0.006544941379721392, computed grad = 0.006544936273201681
Iteration: 469, Checking gradient for output layer W[102][1]:	mock grad = 0.151000692565406158, computed grad = 0.151000690875528087
Iteration: 470, Checking gradient for words E[448][46]:	mock grad = -0.002590515321698739, computed grad = -0.002590515339356260
Iteration: 470, Checking gradient for hidden W[230][28]:	mock grad = -0.004183377775912600, computed grad = -0.004183377839460568
Iteration: 470, Checking gradient for hidden b[0][8]:	mock grad = -0.049690559927706435, computed grad = -0.049690589390842664
Iteration: 470, Checking gradient for output layer W[45][1]:	mock grad = 0.024057101380037249, computed grad = 0.024057101378401721
Iteration: 471, Checking gradient for words E[102][8]:	mock grad = 0.032431831796853050, computed grad = 0.032431831904024960
Iteration: 471, Checking gradient for hidden W[144][45]:	mock grad = 0.000076271722715404, computed grad = 0.000076271733903832
Iteration: 471, Checking gradient for hidden b[0][25]:	mock grad = -0.005333785877303931, computed grad = -0.005333780014575475
Iteration: 471, Checking gradient for output layer W[3][1]:	mock grad = 0.019059449647806925, computed grad = 0.019059449646831796
Iteration: 472, Checking gradient for words E[60][17]:	mock grad = -0.009494731929043931, computed grad = -0.009494731934380222
Iteration: 472, Checking gradient for hidden W[198][11]:	mock grad = -0.000762701033746316, computed grad = -0.000762701016631692
Iteration: 472, Checking gradient for hidden b[0][8]:	mock grad = -0.049162721736994985, computed grad = -0.049162750242059974
Iteration: 472, Checking gradient for output layer W[64][1]:	mock grad = -0.147118700237119926, computed grad = -0.147118699766638050
Iteration: 473, Checking gradient for words E[49][9]:	mock grad = 0.007840802150438364, computed grad = 0.007840802218870499
Iteration: 473, Checking gradient for hidden W[110][41]:	mock grad = 0.006524990180056811, computed grad = 0.006524990201212803
Iteration: 473, Checking gradient for hidden b[0][1]:	mock grad = -0.138209845353731708, computed grad = -0.138209870692676706
Iteration: 473, Checking gradient for output layer W[41][0]:	mock grad = 0.055570748521527324, computed grad = 0.055570748509993120
Iteration: 474, Checking gradient for words E[73][10]:	mock grad = -0.016914979290516552, computed grad = -0.016914979401772320
Iteration: 474, Checking gradient for hidden W[1][36]:	mock grad = -0.013876089364761590, computed grad = -0.013876089442672873
Iteration: 474, Checking gradient for hidden b[0][42]:	mock grad = 0.088757660305138764, computed grad = 0.088757688047026606
Iteration: 474, Checking gradient for output layer W[143][1]:	mock grad = 0.264689722103084168, computed grad = 0.264689720351399205
Iteration: 475, Checking gradient for words E[206][39]:	mock grad = -0.014121051735732637, computed grad = -0.014121051773892281
Iteration: 475, Checking gradient for hidden W[161][35]:	mock grad = -0.001226848561519667, computed grad = -0.001226848562816981
Iteration: 475, Checking gradient for hidden b[0][26]:	mock grad = 0.034234020325873971, computed grad = 0.034234028180152240
Iteration: 475, Checking gradient for output layer W[83][1]:	mock grad = -0.152902566881607527, computed grad = -0.152902565888430070
Iteration: 476, Checking gradient for words E[49][36]:	mock grad = 0.026641910135571933, computed grad = 0.025947339962291854
Iteration: 476, Checking gradient for hidden W[149][29]:	mock grad = 0.007960684746427527, computed grad = 0.007960684765790354
Iteration: 476, Checking gradient for hidden b[0][15]:	mock grad = -0.114002649179517634, computed grad = -0.114002673475952596
Iteration: 476, Checking gradient for output layer W[62][0]:	mock grad = -0.198805436679022618, computed grad = -0.198805435268348318
Iteration: 477, Checking gradient for words E[94][39]:	mock grad = 0.063407855411257152, computed grad = 0.063407855355773618
Iteration: 477, Checking gradient for hidden W[95][21]:	mock grad = 0.004158489160882572, computed grad = 0.004158489201777462
Iteration: 477, Checking gradient for hidden b[0][18]:	mock grad = 0.078500239441725572, computed grad = 0.078500259670126582
Iteration: 477, Checking gradient for output layer W[55][1]:	mock grad = 0.104947607117933384, computed grad = 0.104947606493093107
Iteration: 478, Checking gradient for words E[32][6]:	mock grad = 0.012491533671193933, computed grad = 0.012491533659407054
Iteration: 478, Checking gradient for hidden W[173][2]:	mock grad = -0.020968864699461021, computed grad = -0.020968865042477341
Iteration: 478, Checking gradient for hidden b[0][5]:	mock grad = 0.020829082041540214, computed grad = 0.020829084721773288
Iteration: 478, Checking gradient for output layer W[50][1]:	mock grad = -0.176432853135871648, computed grad = -0.176432851779518451
current: 80, Cost = 0.391321, Correct(%) = 1, time = 5.25238
Iteration: 479, Checking gradient for words E[102][38]:	mock grad = 0.016407370332732762, computed grad = 0.016407370394493710
Iteration: 479, Checking gradient for hidden W[98][40]:	mock grad = -0.006853838126069656, computed grad = -0.006853838147785047
Iteration: 479, Checking gradient for hidden b[0][42]:	mock grad = -0.060404685731568764, computed grad = -0.060404693177314700
Iteration: 479, Checking gradient for output layer W[0][1]:	mock grad = 0.062100285724342452, computed grad = 0.062100285633691729
Iteration: 480, Checking gradient for words E[156][38]:	mock grad = -0.025074297173355298, computed grad = -0.025074297241081851
Iteration: 480, Checking gradient for hidden W[22][8]:	mock grad = -0.003927478624071723, computed grad = -0.003927478709788923
Iteration: 480, Checking gradient for hidden b[0][36]:	mock grad = -0.000854028432056841, computed grad = -0.000854040510263440
Iteration: 480, Checking gradient for output layer W[90][1]:	mock grad = -0.086679150770874180, computed grad = -0.086679150610267541
Iteration: 481, Checking gradient for words E[225][6]:	mock grad = -0.009427131352213181, computed grad = -0.009427131372431372
Iteration: 481, Checking gradient for hidden W[197][9]:	mock grad = -0.004821266759186260, computed grad = -0.004821266751076167
Iteration: 481, Checking gradient for hidden b[0][11]:	mock grad = -0.108035007180429243, computed grad = -0.108035039350693182
Iteration: 481, Checking gradient for output layer W[98][0]:	mock grad = -0.083791479901923482, computed grad = -0.083791479764333252
Iteration: 482, Checking gradient for words E[701][2]:	mock grad = 0.009640703933111450, computed grad = 0.009640703938696382
Iteration: 482, Checking gradient for hidden W[108][34]:	mock grad = -0.011777209764474161, computed grad = -0.011777209781294946
Iteration: 482, Checking gradient for hidden b[0][40]:	mock grad = 0.122891313327211371, computed grad = 0.122891345383334571
Iteration: 482, Checking gradient for output layer W[6][0]:	mock grad = -0.036349228244664245, computed grad = -0.036349228236291352
Iteration: 483, Checking gradient for words E[148][10]:	mock grad = 0.024813959620839654, computed grad = 0.024813959633553706
Iteration: 483, Checking gradient for hidden W[105][19]:	mock grad = 0.002489472786887381, computed grad = 0.002489472789442790
Iteration: 483, Checking gradient for hidden b[0][36]:	mock grad = -0.001458624643374540, computed grad = -0.001458612010878657
Iteration: 483, Checking gradient for output layer W[66][1]:	mock grad = 0.067134564658039020, computed grad = 0.067134564614854481
Iteration: 484, Checking gradient for words E[732][23]:	mock grad = 0.003221738578473854, computed grad = 0.003221738621171689
Iteration: 484, Checking gradient for hidden W[66][6]:	mock grad = 0.001407270599523835, computed grad = 0.001407270609634179
Iteration: 484, Checking gradient for hidden b[0][6]:	mock grad = 0.045320398454645749, computed grad = 0.045320414810892044
Iteration: 484, Checking gradient for output layer W[114][1]:	mock grad = -0.151942273693250751, computed grad = -0.151942272785701132
Iteration: 485, Checking gradient for words E[571][21]:	mock grad = -0.000017929963097574, computed grad = -0.000017929960064986
Iteration: 485, Checking gradient for hidden W[195][28]:	mock grad = -0.003610270120191217, computed grad = -0.003610270165412411
Iteration: 485, Checking gradient for hidden b[0][29]:	mock grad = 0.091439822274941296, computed grad = 0.091439853218556771
Iteration: 485, Checking gradient for output layer W[20][1]:	mock grad = -0.040318683493800744, computed grad = -0.040318683476864924
Iteration: 486, Checking gradient for words E[201][4]:	mock grad = 0.021859850078825849, computed grad = 0.021859850183164855
Iteration: 486, Checking gradient for hidden W[97][19]:	mock grad = -0.013510127395782723, computed grad = -0.013510127520619577
Iteration: 486, Checking gradient for hidden b[0][30]:	mock grad = -0.003768986990226875, computed grad = -0.003768987986642484
Iteration: 486, Checking gradient for output layer W[81][0]:	mock grad = -0.063681056114067713, computed grad = -0.063681056055854085
Iteration: 487, Checking gradient for words E[194][30]:	mock grad = -0.038535625248969918, computed grad = -0.038535625283968929
Iteration: 487, Checking gradient for hidden W[183][14]:	mock grad = -0.002155468802278460, computed grad = -0.002155468830565152
Iteration: 487, Checking gradient for hidden b[0][44]:	mock grad = -0.005604534756564750, computed grad = -0.005604547839455657
Iteration: 487, Checking gradient for output layer W[43][0]:	mock grad = 0.126064534440251119, computed grad = 0.126064533967182757
Iteration: 488, Checking gradient for words E[75][48]:	mock grad = -0.006886078890183667, computed grad = -0.006886078892244019
Iteration: 488, Checking gradient for hidden W[39][19]:	mock grad = -0.010558849513869761, computed grad = -0.010558849471612215
Iteration: 488, Checking gradient for hidden b[0][10]:	mock grad = -0.106945387623319510, computed grad = -0.106945423720946189
Iteration: 488, Checking gradient for output layer W[49][1]:	mock grad = -0.062553922893532343, computed grad = -0.062553922843494744
current: 90, Cost = 0.461977, Correct(%) = 1, time = 6.1993
Iteration: 489, Checking gradient for words E[476][15]:	mock grad = -0.009166116384573542, computed grad = -0.009166116397451352
Iteration: 489, Checking gradient for hidden W[38][3]:	mock grad = 0.003788153851630449, computed grad = 0.003788153867705765
Iteration: 489, Checking gradient for hidden b[0][46]:	mock grad = -0.167234158893692086, computed grad = -0.167234195584768180
Iteration: 489, Checking gradient for output layer W[95][1]:	mock grad = -0.046045544115774772, computed grad = -0.046045544096334677
Iteration: 490, Checking gradient for words E[148][47]:	mock grad = -0.016338566984547276, computed grad = -0.016338567006959022
Iteration: 490, Checking gradient for hidden W[23][16]:	mock grad = -0.000504956015945357, computed grad = -0.000504956036713836
Iteration: 490, Checking gradient for hidden b[0][18]:	mock grad = 0.038948536182109583, computed grad = 0.038948526323830834
Iteration: 490, Checking gradient for output layer W[13][0]:	mock grad = 0.041477338936946051, computed grad = 0.041477338914762220
Iteration: 491, Checking gradient for words E[102][31]:	mock grad = -0.000559438591035688, computed grad = -0.000559438624260157
Iteration: 491, Checking gradient for hidden W[2][25]:	mock grad = 0.013510728250176740, computed grad = 0.013510728479653487
Iteration: 491, Checking gradient for hidden b[0][2]:	mock grad = 0.065098261817136960, computed grad = 0.065098295414781682
Iteration: 491, Checking gradient for output layer W[31][0]:	mock grad = 0.059814998227381011, computed grad = 0.059814998178003349
Iteration: 492, Checking gradient for words E[102][36]:	mock grad = -0.005783315756818785, computed grad = -0.005783315735874413
Iteration: 492, Checking gradient for hidden W[173][10]:	mock grad = 0.000291645751937519, computed grad = 0.000291645756261528
Iteration: 492, Checking gradient for hidden b[0][16]:	mock grad = 0.019644767477983160, computed grad = 0.019644774283389375
Iteration: 492, Checking gradient for output layer W[36][1]:	mock grad = -0.057358508389115936, computed grad = -0.057358508314995928
Iteration: 493, Checking gradient for words E[645][9]:	mock grad = -0.005094311070458213, computed grad = -0.005094311048643861
Iteration: 493, Checking gradient for hidden W[229][23]:	mock grad = -0.021116954593891624, computed grad = -0.021116954851127610
Iteration: 493, Checking gradient for hidden b[0][12]:	mock grad = -0.081303172330848827, computed grad = -0.081303197622656753
Iteration: 493, Checking gradient for output layer W[125][0]:	mock grad = -0.059264786652357682, computed grad = -0.059264786605726795
Iteration: 494, Checking gradient for words E[25][43]:	mock grad = -0.003862831840922842, computed grad = -0.003862831856641184
Iteration: 494, Checking gradient for hidden W[53][4]:	mock grad = 0.001953174831315074, computed grad = 0.001953174839978787
Iteration: 494, Checking gradient for hidden b[0][28]:	mock grad = -0.021639469098694297, computed grad = -0.021639493616082528
Iteration: 494, Checking gradient for output layer W[25][0]:	mock grad = 0.081294176311086375, computed grad = 0.081294176100082327
Iteration: 495, Checking gradient for words E[533][26]:	mock grad = -0.012070840098271640, computed grad = -0.012070840136831955
Iteration: 495, Checking gradient for hidden W[158][42]:	mock grad = -0.021499956176340707, computed grad = -0.021499956477021478
Iteration: 495, Checking gradient for hidden b[0][19]:	mock grad = 0.046740972120817847, computed grad = 0.046740967202144913
Iteration: 495, Checking gradient for output layer W[19][1]:	mock grad = 0.059183063879336695, computed grad = 0.059183063847640452
Iteration: 496, Checking gradient for words E[201][34]:	mock grad = -0.012115495181652669, computed grad = -0.012115495199136531
Iteration: 496, Checking gradient for hidden W[54][27]:	mock grad = -0.018321465350468191, computed grad = -0.018321465464152493
Iteration: 496, Checking gradient for hidden b[0][24]:	mock grad = -0.095230438632021297, computed grad = -0.095230439139957296
Iteration: 496, Checking gradient for output layer W[3][1]:	mock grad = -0.029304968474030968, computed grad = -0.029304968468174718
Iteration: 497, Checking gradient for words E[473][31]:	mock grad = -0.038064531688086767, computed grad = -0.038064531744344779
Iteration: 497, Checking gradient for hidden W[68][22]:	mock grad = -0.013568140579844901, computed grad = -0.013568140645089828
Iteration: 497, Checking gradient for hidden b[0][24]:	mock grad = 0.129703544993975406, computed grad = 0.129703571061294709
Iteration: 497, Checking gradient for output layer W[56][1]:	mock grad = -0.167830820670178404, computed grad = -0.167830820514409590
Iteration: 498, Checking gradient for words E[757][31]:	mock grad = 0.025323012538186473, computed grad = 0.025323012600008580
Iteration: 498, Checking gradient for hidden W[147][21]:	mock grad = -0.006403356988454734, computed grad = -0.006403357031332512
Iteration: 498, Checking gradient for hidden b[0][28]:	mock grad = -0.010353318075306817, computed grad = -0.010353332563594745
Iteration: 498, Checking gradient for output layer W[131][0]:	mock grad = -0.170744963463281829, computed grad = -0.170744961976643989
current: 100, Cost = 0.466489, Correct(%) = 1, time = 6.98141
Iteration: 499, Checking gradient for words E[25][25]:	mock grad = -0.072229637369719546, computed grad = -0.072229637957406168
Iteration: 499, Checking gradient for hidden W[243][13]:	mock grad = -0.000557100862441651, computed grad = -0.000557100854894485
Iteration: 499, Checking gradient for hidden b[0][4]:	mock grad = 0.025398984412478010, computed grad = 0.025398994550517768
Iteration: 499, Checking gradient for output layer W[100][1]:	mock grad = -0.030966178616337325, computed grad = -0.030966178610652689
current: 5, Correct(%) = 1, time = 7.03261
Dev start.
Dev finished. Total time taken is: 0.636864
dev:
Accuracy:	P=100/100=1
Test start.
Test finished. Total time taken is: 0.544639
test:
Accuracy:	P=61/100=0.61
##### Iteration 5
random: 0, 99
Iteration: 500, Checking gradient for words E[34][1]:	mock grad = 0.010041942070410093, computed grad = 0.010041942092160819
Iteration: 500, Checking gradient for hidden W[236][3]:	mock grad = -0.001582319853393743, computed grad = -0.001582319878480063
Iteration: 500, Checking gradient for hidden b[0][9]:	mock grad = -0.040559975359399925, computed grad = -0.040559971802241607
Iteration: 500, Checking gradient for output layer W[15][1]:	mock grad = 0.056615879389182444, computed grad = 0.056615879257672121
Iteration: 501, Checking gradient for words E[86][22]:	mock grad = -0.021849482711666646, computed grad = -0.021849482747360975
Iteration: 501, Checking gradient for hidden W[61][30]:	mock grad = -0.001195375684576172, computed grad = -0.001195375693822750
Iteration: 501, Checking gradient for hidden b[0][36]:	mock grad = -0.007940968389075209, computed grad = -0.007940964264346618
Iteration: 501, Checking gradient for output layer W[90][1]:	mock grad = 0.021527403652576993, computed grad = 0.021527403648967335
Iteration: 502, Checking gradient for words E[102][19]:	mock grad = 0.008383846893639246, computed grad = 0.008383846910810939
Iteration: 502, Checking gradient for hidden W[185][27]:	mock grad = 0.011930784044628107, computed grad = 0.011930784135030872
Iteration: 502, Checking gradient for hidden b[0][7]:	mock grad = -0.067873161816500716, computed grad = -0.067873165806030394
Iteration: 502, Checking gradient for output layer W[22][1]:	mock grad = 0.043283936232541009, computed grad = 0.043283936195593176
Iteration: 503, Checking gradient for words E[127][25]:	mock grad = 0.017547421863689516, computed grad = 0.017547421988149962
Iteration: 503, Checking gradient for hidden W[98][7]:	mock grad = -0.007350004226164275, computed grad = -0.007350004267246850
Iteration: 503, Checking gradient for hidden b[0][46]:	mock grad = -0.174212050423033782, computed grad = -0.174212092502828181
Iteration: 503, Checking gradient for output layer W[94][0]:	mock grad = 0.181053332153457980, computed grad = 0.181053331284287966
Iteration: 504, Checking gradient for words E[162][36]:	mock grad = 0.012594930335479981, computed grad = 0.012594930352441506
Iteration: 504, Checking gradient for hidden W[144][10]:	mock grad = 0.006141560777911392, computed grad = 0.006141560824352060
Iteration: 504, Checking gradient for hidden b[0][33]:	mock grad = 0.120958970099754870, computed grad = 0.120959004133383050
Iteration: 504, Checking gradient for output layer W[21][1]:	mock grad = -0.014963686454816383, computed grad = -0.014963686454095139
Iteration: 505, Checking gradient for words E[210][39]:	mock grad = -0.007714566122224431, computed grad = -0.007714566142129271
Iteration: 505, Checking gradient for hidden W[85][17]:	mock grad = -0.001648043791852016, computed grad = -0.001648043792960068
Iteration: 505, Checking gradient for hidden b[0][45]:	mock grad = 0.008118158913689166, computed grad = 0.008118158857165571
Iteration: 505, Checking gradient for output layer W[56][1]:	mock grad = -0.184090680374199289, computed grad = -0.184090678257803231
Iteration: 506, Checking gradient for words E[149][13]:	mock grad = -0.003240250459807248, computed grad = -0.003240250467232475
Iteration: 506, Checking gradient for hidden W[19][20]:	mock grad = -0.013300598390386664, computed grad = -0.013300598645478902
Iteration: 506, Checking gradient for hidden b[0][29]:	mock grad = 0.072264381665770872, computed grad = 0.072264406600576980
Iteration: 506, Checking gradient for output layer W[138][0]:	mock grad = -0.044333747958091863, computed grad = -0.044333747897311426
Iteration: 507, Checking gradient for words E[25][9]:	mock grad = 0.099467705873990120, computed grad = 0.099467706039857162
Iteration: 507, Checking gradient for hidden W[189][25]:	mock grad = 0.000188549389323889, computed grad = 0.000188549356110964
Iteration: 507, Checking gradient for hidden b[0][3]:	mock grad = -0.059943488064595485, computed grad = -0.059943509105038595
Iteration: 507, Checking gradient for output layer W[28][1]:	mock grad = 0.109688049084288863, computed grad = 0.109688048990788517
Iteration: 508, Checking gradient for words E[296][21]:	mock grad = -0.030852982265100470, computed grad = -0.030852982344750604
Iteration: 508, Checking gradient for hidden W[204][46]:	mock grad = 0.006851429702081635, computed grad = 0.006851429721940227
Iteration: 508, Checking gradient for hidden b[0][25]:	mock grad = -0.006234955546496890, computed grad = -0.006234952562337356
Iteration: 508, Checking gradient for output layer W[122][0]:	mock grad = -0.156669835865652640, computed grad = -0.156669834491572318
current: 10, Cost = 0.424887, Correct(%) = 1, time = 0.770497
Iteration: 509, Checking gradient for words E[103][39]:	mock grad = 0.047742276563067509, computed grad = 0.047742276648496763
Iteration: 509, Checking gradient for hidden W[33][18]:	mock grad = -0.007903995692504218, computed grad = -0.007903995749331226
Iteration: 509, Checking gradient for hidden b[0][35]:	mock grad = 0.013394340208888744, computed grad = 0.013394339594824965
Iteration: 509, Checking gradient for output layer W[113][1]:	mock grad = -0.122615401834863524, computed grad = -0.122615401318961481
Iteration: 510, Checking gradient for words E[209][43]:	mock grad = -0.001335415400371964, computed grad = -0.001044514926035695
Iteration: 510, Checking gradient for hidden W[85][15]:	mock grad = -0.006986047721008193, computed grad = -0.006986047803903107
Iteration: 510, Checking gradient for hidden b[0][36]:	mock grad = 0.004185666074435490, computed grad = 0.004185659280631484
Iteration: 510, Checking gradient for output layer W[31][0]:	mock grad = -0.053244103861121994, computed grad = -0.053244103774923848
Iteration: 511, Checking gradient for words E[34][48]:	mock grad = -0.033329679107158006, computed grad = -0.033329679293370308
Iteration: 511, Checking gradient for hidden W[176][45]:	mock grad = -0.003099099243286663, computed grad = -0.003099099255363889
Iteration: 511, Checking gradient for hidden b[0][45]:	mock grad = -0.009305421378336121, computed grad = -0.009305421002194895
Iteration: 511, Checking gradient for output layer W[8][1]:	mock grad = -0.077800880169787145, computed grad = -0.077800880093425104
Iteration: 512, Checking gradient for words E[63][23]:	mock grad = -0.015096172675171937, computed grad = -0.015096172726460497
Iteration: 512, Checking gradient for hidden W[98][44]:	mock grad = -0.017081959829107207, computed grad = -0.017081959836982106
Iteration: 512, Checking gradient for hidden b[0][3]:	mock grad = 0.033886810027361491, computed grad = 0.033886810548564100
Iteration: 512, Checking gradient for output layer W[137][1]:	mock grad = 0.121998363446590430, computed grad = 0.121998362660274057
Iteration: 513, Checking gradient for words E[216][3]:	mock grad = -0.038987634054299392, computed grad = -0.038987634088993632
Iteration: 513, Checking gradient for hidden W[207][12]:	mock grad = -0.003335023922884650, computed grad = -0.003335023936795575
Iteration: 513, Checking gradient for hidden b[0][32]:	mock grad = 0.057194145989036826, computed grad = 0.057194161696063901
Iteration: 513, Checking gradient for output layer W[84][1]:	mock grad = 0.100100149953846751, computed grad = 0.100100149589693502
Iteration: 514, Checking gradient for words E[102][17]:	mock grad = -0.065175058191202506, computed grad = -0.065175058403450295
Iteration: 514, Checking gradient for hidden W[85][44]:	mock grad = 0.023324513764466648, computed grad = 0.023324513806440652
Iteration: 514, Checking gradient for hidden b[0][30]:	mock grad = 0.001940081645945835, computed grad = 0.001940079113803733
Iteration: 514, Checking gradient for output layer W[106][1]:	mock grad = 0.042071676315591500, computed grad = 0.042071676313345685
Iteration: 515, Checking gradient for words E[49][26]:	mock grad = 0.015978793836246563, computed grad = 0.015978793886136815
Iteration: 515, Checking gradient for hidden W[108][31]:	mock grad = -0.010081739551470736, computed grad = -0.010081739562267560
Iteration: 515, Checking gradient for hidden b[0][13]:	mock grad = -0.141272175864570748, computed grad = -0.141272218773060926
Iteration: 515, Checking gradient for output layer W[90][1]:	mock grad = -0.001070184314555522, computed grad = -0.001070184314564098
Iteration: 516, Checking gradient for words E[392][7]:	mock grad = 0.019891484451278751, computed grad = 0.019891484421346098
Iteration: 516, Checking gradient for hidden W[115][28]:	mock grad = -0.011166724654787519, computed grad = -0.011166724780669223
Iteration: 516, Checking gradient for hidden b[0][36]:	mock grad = 0.006261652723771327, computed grad = 0.006261644822511353
Iteration: 516, Checking gradient for output layer W[74][1]:	mock grad = -0.129784403171129270, computed grad = -0.129784402692999096
Iteration: 517, Checking gradient for words E[412][3]:	mock grad = -0.002274646399569313, computed grad = -0.002274646384287542
Iteration: 517, Checking gradient for hidden W[57][28]:	mock grad = -0.010924357440350585, computed grad = -0.010924357472508400
Iteration: 517, Checking gradient for hidden b[0][47]:	mock grad = 0.056414711931795924, computed grad = 0.056414715629477002
Iteration: 517, Checking gradient for output layer W[99][0]:	mock grad = 0.139963334411102869, computed grad = 0.139963332528356854
Iteration: 518, Checking gradient for words E[42][16]:	mock grad = 0.013107850810878396, computed grad = 0.013107850823984069
Iteration: 518, Checking gradient for hidden W[193][49]:	mock grad = 0.004419246049353065, computed grad = 0.004419246056401452
Iteration: 518, Checking gradient for hidden b[0][17]:	mock grad = -0.018216550906891005, computed grad = -0.018216555719547944
Iteration: 518, Checking gradient for output layer W[114][0]:	mock grad = -0.099263614592282057, computed grad = -0.099263614092133481
current: 20, Cost = 0.49276, Correct(%) = 1, time = 1.42846
Iteration: 519, Checking gradient for words E[403][4]:	mock grad = -0.025131081999585358, computed grad = -0.025131082030080784
Iteration: 519, Checking gradient for hidden W[42][32]:	mock grad = 0.012217983578616254, computed grad = 0.012217983675501561
Iteration: 519, Checking gradient for hidden b[0][14]:	mock grad = -0.078450064562418653, computed grad = -0.078450105010467397
Iteration: 519, Checking gradient for output layer W[90][0]:	mock grad = -0.020806152555896684, computed grad = -0.020806152554565860
Iteration: 520, Checking gradient for words E[444][2]:	mock grad = -0.037903390867899400, computed grad = -0.037903390970091350
Iteration: 520, Checking gradient for hidden W[185][38]:	mock grad = -0.006956017340936649, computed grad = -0.006956017392063543
Iteration: 520, Checking gradient for hidden b[0][23]:	mock grad = -0.145138522733345710, computed grad = -0.145138562948659400
Iteration: 520, Checking gradient for output layer W[20][0]:	mock grad = 0.013036707134955750, computed grad = 0.013036707134148557
Iteration: 521, Checking gradient for words E[284][24]:	mock grad = 0.040985617546784869, computed grad = 0.040985617490596746
Iteration: 521, Checking gradient for hidden W[221][3]:	mock grad = 0.010367798888022417, computed grad = 0.010367799060956681
Iteration: 521, Checking gradient for hidden b[0][10]:	mock grad = -0.116997859872236543, computed grad = -0.116997898348459639
Iteration: 521, Checking gradient for output layer W[106][1]:	mock grad = -0.025276324940148864, computed grad = -0.025276324938217562
Iteration: 522, Checking gradient for words E[215][18]:	mock grad = -0.023805822045752656, computed grad = -0.023805822110873245
Iteration: 522, Checking gradient for hidden W[200][25]:	mock grad = -0.003669538488126900, computed grad = -0.005436746334578984
Iteration: 522, Checking gradient for hidden b[0][24]:	mock grad = 0.089601266936328861, computed grad = 0.089601289075309842
Iteration: 522, Checking gradient for output layer W[13][1]:	mock grad = 0.030241323943525078, computed grad = 0.030241323931235430
Iteration: 523, Checking gradient for words E[69][8]:	mock grad = 0.027809464525851979, computed grad = 0.027809464556082554
Iteration: 523, Checking gradient for hidden W[95][19]:	mock grad = 0.014073726701063771, computed grad = 0.014073726698478716
Iteration: 523, Checking gradient for hidden b[0][31]:	mock grad = 0.045071472020596914, computed grad = 0.045071489710459441
Iteration: 523, Checking gradient for output layer W[16][0]:	mock grad = -0.037964921921151973, computed grad = -0.037964921902704049
Iteration: 524, Checking gradient for words E[128][27]:	mock grad = 0.036699833166981000, computed grad = 0.036699833217401577
Iteration: 524, Checking gradient for hidden W[233][2]:	mock grad = 0.019857811401402525, computed grad = 0.019857811484294781
Iteration: 524, Checking gradient for hidden b[0][12]:	mock grad = 0.079153754260513631, computed grad = 0.079153775333161813
Iteration: 524, Checking gradient for output layer W[110][1]:	mock grad = -0.127429727160649486, computed grad = -0.127429726649038844
Iteration: 525, Checking gradient for words E[465][16]:	mock grad = -0.019210696887039802, computed grad = -0.019210696943534420
Iteration: 525, Checking gradient for hidden W[179][23]:	mock grad = 0.001817187168684509, computed grad = 0.001817187067570405
Iteration: 525, Checking gradient for hidden b[0][49]:	mock grad = 0.047697580654010308, computed grad = 0.047697593810469299
Iteration: 525, Checking gradient for output layer W[115][0]:	mock grad = 0.011764406874420485, computed grad = 0.011764406874012936
Iteration: 526, Checking gradient for words E[0][45]:	mock grad = 0.016252620976459076, computed grad = 0.015989510919467848
Iteration: 526, Checking gradient for hidden W[192][11]:	mock grad = -0.001809901374549838, computed grad = -0.001809901370074681
Iteration: 526, Checking gradient for hidden b[0][21]:	mock grad = 0.045584275375043504, computed grad = 0.045584282263069591
Iteration: 526, Checking gradient for output layer W[31][0]:	mock grad = -0.056830909795274609, computed grad = -0.056830909692857971
Iteration: 527, Checking gradient for words E[69][16]:	mock grad = 0.019717801801322210, computed grad = 0.019717801879101403
Iteration: 527, Checking gradient for hidden W[200][37]:	mock grad = 0.011334554221253956, computed grad = 0.011334554325064108
Iteration: 527, Checking gradient for hidden b[0][30]:	mock grad = -0.005236126732421909, computed grad = -0.005236128961379385
Iteration: 527, Checking gradient for output layer W[147][0]:	mock grad = 0.073820673188229602, computed grad = 0.073820673153100314
Iteration: 528, Checking gradient for words E[49][7]:	mock grad = -0.004481492232560891, computed grad = -0.004481492227675587
Iteration: 528, Checking gradient for hidden W[42][42]:	mock grad = 0.003032474783087880, computed grad = 0.003032474769101698
Iteration: 528, Checking gradient for hidden b[0][27]:	mock grad = 0.030475528994294843, computed grad = 0.030475543379690086
Iteration: 528, Checking gradient for output layer W[147][1]:	mock grad = -0.043533150507868434, computed grad = -0.043533150465966411
current: 30, Cost = 0.389525, Correct(%) = 1, time = 2.11874
Iteration: 529, Checking gradient for words E[509][16]:	mock grad = 0.010486445679164236, computed grad = 0.010486445693050099
Iteration: 529, Checking gradient for hidden W[115][22]:	mock grad = -0.000202012719663358, computed grad = -0.000202012719424623
Iteration: 529, Checking gradient for hidden b[0][14]:	mock grad = 0.067537039769749452, computed grad = 0.067537067182534022
Iteration: 529, Checking gradient for output layer W[105][1]:	mock grad = 0.043575635197440343, computed grad = 0.043575635165568810
Iteration: 530, Checking gradient for words E[377][22]:	mock grad = 0.009490384027699417, computed grad = 0.009490383960368278
Iteration: 530, Checking gradient for hidden W[222][23]:	mock grad = 0.000249162048326923, computed grad = 0.000249162051389383
Iteration: 530, Checking gradient for hidden b[0][0]:	mock grad = -0.165094354491329698, computed grad = -0.165094393313020171
Iteration: 530, Checking gradient for output layer W[73][0]:	mock grad = -0.020645527016527332, computed grad = -0.020645527013194193
Iteration: 531, Checking gradient for words E[362][48]:	mock grad = 0.028956583360217891, computed grad = 0.028956583466556697
Iteration: 531, Checking gradient for hidden W[161][4]:	mock grad = -0.001349695226249281, computed grad = -0.001349695225045394
Iteration: 531, Checking gradient for hidden b[0][22]:	mock grad = 0.061992157727897013, computed grad = 0.061992173176036093
Iteration: 531, Checking gradient for output layer W[63][0]:	mock grad = -0.094542515595874566, computed grad = -0.094542515520218876
Iteration: 532, Checking gradient for words E[143][33]:	mock grad = 0.011847087129479572, computed grad = 0.009923075869258459
Iteration: 532, Checking gradient for hidden W[130][38]:	mock grad = 0.008493649881702314, computed grad = 0.012992895289043923
Iteration: 532, Checking gradient for hidden b[0][39]:	mock grad = 0.079603992731214213, computed grad = 0.079604011664324903
Iteration: 532, Checking gradient for output layer W[87][0]:	mock grad = -0.060696797718057249, computed grad = -0.060696797618500456
Iteration: 533, Checking gradient for words E[347][27]:	mock grad = 0.006047536949133070, computed grad = 0.006047536928770232
Iteration: 533, Checking gradient for hidden W[125][4]:	mock grad = -0.003466538123858554, computed grad = -0.003466538166729604
Iteration: 533, Checking gradient for hidden b[0][24]:	mock grad = 0.080195616397410463, computed grad = 0.080195631040238219
Iteration: 533, Checking gradient for output layer W[97][1]:	mock grad = -0.134101834089406236, computed grad = -0.134101832884569339
Iteration: 534, Checking gradient for words E[529][42]:	mock grad = 0.022611196283273127, computed grad = 0.022611196375866682
Iteration: 534, Checking gradient for hidden W[132][27]:	mock grad = -0.020880960308500285, computed grad = -0.020880960798499701
Iteration: 534, Checking gradient for hidden b[0][21]:	mock grad = -0.068101406807158371, computed grad = -0.068101424100585714
Iteration: 534, Checking gradient for output layer W[77][0]:	mock grad = -0.086002319381439651, computed grad = -0.086002319293010540
Iteration: 535, Checking gradient for words E[34][47]:	mock grad = -0.036101684799921818, computed grad = -0.036101684780106717
Iteration: 535, Checking gradient for hidden W[73][17]:	mock grad = 0.001447044927910257, computed grad = 0.001447044948192083
Iteration: 535, Checking gradient for hidden b[0][29]:	mock grad = -0.100249456388090685, computed grad = -0.100249490378810854
Iteration: 535, Checking gradient for output layer W[31][1]:	mock grad = -0.011347217996171555, computed grad = -0.011347217995950613
Iteration: 536, Checking gradient for words E[48][25]:	mock grad = -0.082927644972474734, computed grad = -0.082820830951811183
Iteration: 536, Checking gradient for hidden W[85][33]:	mock grad = 0.000254810084976409, computed grad = 0.000254810084996276
Iteration: 536, Checking gradient for hidden b[0][21]:	mock grad = -0.070738094777755833, computed grad = -0.070738113543009698
Iteration: 536, Checking gradient for output layer W[11][0]:	mock grad = -0.031071585621544884, computed grad = -0.031071585618073841
Iteration: 537, Checking gradient for words E[42][24]:	mock grad = 0.000126859616006447, computed grad = 0.000126859615798014
Iteration: 537, Checking gradient for hidden W[201][30]:	mock grad = 0.002630936236230896, computed grad = 0.002630936250227529
Iteration: 537, Checking gradient for hidden b[0][21]:	mock grad = 0.057459482685873198, computed grad = 0.057459494288886206
Iteration: 537, Checking gradient for output layer W[96][0]:	mock grad = 0.193860857585576030, computed grad = 0.193860855567436458
Iteration: 538, Checking gradient for words E[488][33]:	mock grad = -0.022907470971111898, computed grad = -0.022907471038363887
Iteration: 538, Checking gradient for hidden W[234][13]:	mock grad = -0.019382752860203079, computed grad = -0.019382752965988490
Iteration: 538, Checking gradient for hidden b[0][12]:	mock grad = 0.089324693819697876, computed grad = 0.089324723119526717
Iteration: 538, Checking gradient for output layer W[51][0]:	mock grad = -0.162868595837278862, computed grad = -0.162868595187952547
current: 40, Cost = 0.349192, Correct(%) = 1, time = 2.55363
Iteration: 539, Checking gradient for words E[95][15]:	mock grad = 0.004423950498932960, computed grad = 0.004423950480840931
Iteration: 539, Checking gradient for hidden W[101][11]:	mock grad = 0.005555716881683326, computed grad = 0.005555716901540418
Iteration: 539, Checking gradient for hidden b[0][46]:	mock grad = -0.130737403772340777, computed grad = -0.130737433050903823
Iteration: 539, Checking gradient for output layer W[138][1]:	mock grad = 0.052200508988742289, computed grad = 0.052200508909749872
Iteration: 540, Checking gradient for words E[102][28]:	mock grad = 0.001424017368911734, computed grad = 0.003493292844212771
Iteration: 540, Checking gradient for hidden W[90][10]:	mock grad = -0.007167074089126091, computed grad = -0.007167074154966688
Iteration: 540, Checking gradient for hidden b[0][41]:	mock grad = 0.033129900884110164, computed grad = 0.033129907890608289
Iteration: 540, Checking gradient for output layer W[15][1]:	mock grad = -0.062090033036088510, computed grad = -0.062090032906693113
Iteration: 541, Checking gradient for words E[225][12]:	mock grad = -0.024398111858503491, computed grad = -0.024398111802463013
Iteration: 541, Checking gradient for hidden W[78][36]:	mock grad = -0.001591110526166784, computed grad = -0.001591110511505239
Iteration: 541, Checking gradient for hidden b[0][46]:	mock grad = 0.187667897783183601, computed grad = 0.187667942907398494
Iteration: 541, Checking gradient for output layer W[96][1]:	mock grad = 0.260617973743770825, computed grad = 0.260617972396458453
Iteration: 542, Checking gradient for words E[34][0]:	mock grad = -0.051794706532826140, computed grad = -0.051706598678888875
Iteration: 542, Checking gradient for hidden W[8][39]:	mock grad = -0.001480853349888900, computed grad = -0.001480853346031219
Iteration: 542, Checking gradient for hidden b[0][42]:	mock grad = -0.087076052316781283, computed grad = -0.087076081540038847
Iteration: 542, Checking gradient for output layer W[90][0]:	mock grad = 0.025031935380115389, computed grad = 0.025031935378013362
Iteration: 543, Checking gradient for words E[377][29]:	mock grad = 0.007331802642729368, computed grad = 0.007331802618872335
Iteration: 543, Checking gradient for hidden W[123][33]:	mock grad = -0.015412392844971201, computed grad = -0.015412392939343860
Iteration: 543, Checking gradient for hidden b[0][25]:	mock grad = 0.010573087221965594, computed grad = 0.010573086362332958
Iteration: 543, Checking gradient for output layer W[137][0]:	mock grad = 0.163943010804856737, computed grad = 0.163943010508990294
Iteration: 544, Checking gradient for words E[85][39]:	mock grad = 0.008273689714982657, computed grad = 0.008273689712513162
Iteration: 544, Checking gradient for hidden W[36][31]:	mock grad = 0.006522480253956520, computed grad = 0.006522480287548805
Iteration: 544, Checking gradient for hidden b[0][0]:	mock grad = 0.118012730778660480, computed grad = 0.118012741895273929
Iteration: 544, Checking gradient for output layer W[115][0]:	mock grad = -0.071846606412545588, computed grad = -0.071846606140654357
Iteration: 545, Checking gradient for words E[102][42]:	mock grad = 0.009238848861170545, computed grad = 0.009238849006564399
Iteration: 545, Checking gradient for hidden W[162][33]:	mock grad = -0.027755014105707021, computed grad = -0.027755014486014334
Iteration: 545, Checking gradient for hidden b[0][35]:	mock grad = 0.014150392191297767, computed grad = 0.014150390663811274
Iteration: 545, Checking gradient for output layer W[46][0]:	mock grad = -0.063545142052673365, computed grad = -0.063545142013214984
Iteration: 546, Checking gradient for words E[109][33]:	mock grad = -0.009001271350228546, computed grad = -0.009001271366031939
Iteration: 546, Checking gradient for hidden W[78][6]:	mock grad = -0.002366727113317646, computed grad = -0.002366727139866013
Iteration: 546, Checking gradient for hidden b[0][44]:	mock grad = -0.004004745837726809, computed grad = -0.004004757567201441
Iteration: 546, Checking gradient for output layer W[86][1]:	mock grad = 0.060452628643459239, computed grad = 0.060452628518848667
Iteration: 547, Checking gradient for words E[216][36]:	mock grad = -0.000854925429255582, computed grad = -0.000854925473527420
Iteration: 547, Checking gradient for hidden W[51][39]:	mock grad = -0.007574881443622727, computed grad = -0.007574881445320371
Iteration: 547, Checking gradient for hidden b[0][13]:	mock grad = 0.122357813585993735, computed grad = 0.122357841546101229
Iteration: 547, Checking gradient for output layer W[54][0]:	mock grad = 0.108654394437440738, computed grad = 0.108654393940860225
Iteration: 548, Checking gradient for words E[80][25]:	mock grad = -0.007744827060673920, computed grad = -0.007744827075440270
Iteration: 548, Checking gradient for hidden W[22][20]:	mock grad = -0.003083173917395321, computed grad = -0.003083173916398934
Iteration: 548, Checking gradient for hidden b[0][40]:	mock grad = 0.094261296021691532, computed grad = 0.094261319823374115
Iteration: 548, Checking gradient for output layer W[35][1]:	mock grad = 0.076959279131261216, computed grad = 0.076959278876512882
current: 50, Cost = 0.358054, Correct(%) = 1, time = 3.44368
Iteration: 549, Checking gradient for words E[354][12]:	mock grad = 0.015327315289415111, computed grad = 0.015327315379134950
Iteration: 549, Checking gradient for hidden W[136][33]:	mock grad = 0.008981254437184782, computed grad = 0.008981254624766132
Iteration: 549, Checking gradient for hidden b[0][10]:	mock grad = 0.086721668914824246, computed grad = 0.086721688212698600
Iteration: 549, Checking gradient for output layer W[140][1]:	mock grad = -0.123827432386452996, computed grad = -0.123827431414361228
Iteration: 550, Checking gradient for words E[42][24]:	mock grad = 0.004780786762081934, computed grad = 0.004780786729968893
Iteration: 550, Checking gradient for hidden W[5][36]:	mock grad = 0.001820407976471650, computed grad = 0.001820408011496388
Iteration: 550, Checking gradient for hidden b[0][4]:	mock grad = 0.031324054185100092, computed grad = 0.031324066311008807
Iteration: 550, Checking gradient for output layer W[4][1]:	mock grad = 0.004560649674079098, computed grad = 0.004560649673971153
Iteration: 551, Checking gradient for words E[102][43]:	mock grad = 0.034633888455093409, computed grad = 0.034633888548887382
Iteration: 551, Checking gradient for hidden W[222][7]:	mock grad = 0.002993123807720766, computed grad = 0.002993123835198979
Iteration: 551, Checking gradient for hidden b[0][40]:	mock grad = -0.121402095216799344, computed grad = -0.121402130669370584
Iteration: 551, Checking gradient for output layer W[33][1]:	mock grad = -0.043774843386773510, computed grad = -0.043774843367277522
Iteration: 552, Checking gradient for words E[48][2]:	mock grad = -0.072244099091484104, computed grad = -0.072244099091309966
Iteration: 552, Checking gradient for hidden W[8][44]:	mock grad = 0.020435782942113123, computed grad = 0.020435783056191668
Iteration: 552, Checking gradient for hidden b[0][19]:	mock grad = 0.041252307380723607, computed grad = 0.041252298818800735
Iteration: 552, Checking gradient for output layer W[101][1]:	mock grad = 0.156830720398654089, computed grad = 0.156830719811152292
Iteration: 553, Checking gradient for words E[278][9]:	mock grad = -0.014386825770623668, computed grad = -0.014386825787686531
Iteration: 553, Checking gradient for hidden W[87][0]:	mock grad = 0.025797443742592296, computed grad = 0.025797444223997140
Iteration: 553, Checking gradient for hidden b[0][7]:	mock grad = 0.073922923317759626, computed grad = 0.073922933668809976
Iteration: 553, Checking gradient for output layer W[14][0]:	mock grad = -0.011731151565469444, computed grad = -0.011731151564942623
Iteration: 554, Checking gradient for words E[254][47]:	mock grad = 0.000990018947183424, computed grad = 0.000990018958652583
Iteration: 554, Checking gradient for hidden W[7][35]:	mock grad = 0.002854096514071092, computed grad = 0.002854096503243971
Iteration: 554, Checking gradient for hidden b[0][15]:	mock grad = 0.089415947597815126, computed grad = 0.089415966426389132
Iteration: 554, Checking gradient for output layer W[133][1]:	mock grad = 0.063165082692856478, computed grad = 0.063165082559052385
Iteration: 555, Checking gradient for words E[475][3]:	mock grad = 0.028125367379727706, computed grad = 0.028125367387090136
Iteration: 555, Checking gradient for hidden W[211][27]:	mock grad = 0.011807536607649771, computed grad = 0.011807536690227218
Iteration: 555, Checking gradient for hidden b[0][17]:	mock grad = -0.018345566582506079, computed grad = -0.018345571222163355
Iteration: 555, Checking gradient for output layer W[1][0]:	mock grad = -0.012223338463707156, computed grad = -0.012223338462766400
Iteration: 556, Checking gradient for words E[49][40]:	mock grad = 0.027882940871193362, computed grad = 0.027882940983637021
Iteration: 556, Checking gradient for hidden W[226][27]:	mock grad = 0.005629072445223482, computed grad = 0.005629072483761262
Iteration: 556, Checking gradient for hidden b[0][32]:	mock grad = -0.076286684832060914, computed grad = -0.076286710626379248
Iteration: 556, Checking gradient for output layer W[89][1]:	mock grad = -0.114515182381536018, computed grad = -0.114515182257117668
Iteration: 557, Checking gradient for words E[639][38]:	mock grad = -0.000710358460553895, computed grad = -0.000710358505124719
Iteration: 557, Checking gradient for hidden W[192][11]:	mock grad = -0.003743894885999133, computed grad = -0.003743894906202249
Iteration: 557, Checking gradient for hidden b[0][1]:	mock grad = -0.090866414226453696, computed grad = -0.090866420620321339
Iteration: 557, Checking gradient for output layer W[123][1]:	mock grad = -0.188067806587421948, computed grad = -0.188067804076788275
Iteration: 558, Checking gradient for words E[48][19]:	mock grad = -0.003380626764998285, computed grad = -0.003380626762166937
Iteration: 558, Checking gradient for hidden W[177][22]:	mock grad = 0.003649000258371116, computed grad = 0.003649000273191200
Iteration: 558, Checking gradient for hidden b[0][23]:	mock grad = -0.163202001100842820, computed grad = -0.163202048334753685
Iteration: 558, Checking gradient for output layer W[69][0]:	mock grad = 0.101773899090273900, computed grad = 0.101773898866414053
current: 60, Cost = 0.44793, Correct(%) = 1, time = 4.13463
Iteration: 559, Checking gradient for words E[102][25]:	mock grad = 0.008269814438105483, computed grad = 0.007198464165797502
Iteration: 559, Checking gradient for hidden W[66][32]:	mock grad = 0.006677294350249108, computed grad = 0.006677294371480076
Iteration: 559, Checking gradient for hidden b[0][46]:	mock grad = 0.169054313348876439, computed grad = 0.169054360445222568
Iteration: 559, Checking gradient for output layer W[78][1]:	mock grad = 0.201959361546377725, computed grad = 0.201959359676335187
Iteration: 560, Checking gradient for words E[653][7]:	mock grad = 0.003478006116802712, computed grad = 0.003478006120693036
Iteration: 560, Checking gradient for hidden W[12][29]:	mock grad = -0.010062337943622657, computed grad = -0.010062337965214533
Iteration: 560, Checking gradient for hidden b[0][23]:	mock grad = 0.165072205637412095, computed grad = 0.165072244043706684
Iteration: 560, Checking gradient for output layer W[85][0]:	mock grad = -0.212960721041044465, computed grad = -0.212960719177414409
Iteration: 561, Checking gradient for words E[456][33]:	mock grad = 0.004901953562008732, computed grad = 0.004901953614779119
Iteration: 561, Checking gradient for hidden W[72][21]:	mock grad = -0.001005357637806625, computed grad = -0.001005357636282137
Iteration: 561, Checking gradient for hidden b[0][35]:	mock grad = -0.016918633039542108, computed grad = -0.016918632224892129
Iteration: 561, Checking gradient for output layer W[30][1]:	mock grad = 0.000318664234277843, computed grad = 0.000318664234239951
Iteration: 562, Checking gradient for words E[271][10]:	mock grad = 0.003396842046921034, computed grad = 0.003396842049274354
Iteration: 562, Checking gradient for hidden W[198][4]:	mock grad = 0.007643491561099003, computed grad = 0.007643491663147849
Iteration: 562, Checking gradient for hidden b[0][22]:	mock grad = 0.045900000068960223, computed grad = 0.045900006766688629
Iteration: 562, Checking gradient for output layer W[43][1]:	mock grad = -0.121586123991573869, computed grad = -0.121586123569068547
Iteration: 563, Checking gradient for words E[69][30]:	mock grad = -0.003034432077497762, computed grad = -0.003034432088768261
Iteration: 563, Checking gradient for hidden W[93][9]:	mock grad = 0.004710075367758693, computed grad = 0.004710075373761868
Iteration: 563, Checking gradient for hidden b[0][13]:	mock grad = 0.141356759949862187, computed grad = 0.141356791978861751
Iteration: 563, Checking gradient for output layer W[2][0]:	mock grad = -0.070440926062342868, computed grad = -0.070440925997941148
Iteration: 564, Checking gradient for words E[73][44]:	mock grad = -0.017370250254111763, computed grad = -0.017370250336581090
Iteration: 564, Checking gradient for hidden W[76][36]:	mock grad = -0.002235492706975650, computed grad = -0.002235492710809692
Iteration: 564, Checking gradient for hidden b[0][6]:	mock grad = -0.045585012155902049, computed grad = -0.045585026468950809
Iteration: 564, Checking gradient for output layer W[2][0]:	mock grad = -0.080011465430934736, computed grad = -0.080011465341394986
Iteration: 565, Checking gradient for words E[48][40]:	mock grad = 0.009809871466204267, computed grad = 0.009809871439331919
Iteration: 565, Checking gradient for hidden W[51][48]:	mock grad = 0.006883264126567035, computed grad = 0.006883264101556259
Iteration: 565, Checking gradient for hidden b[0][20]:	mock grad = 0.084897794403376858, computed grad = 0.084897809763698553
Iteration: 565, Checking gradient for output layer W[65][1]:	mock grad = 0.161543156649068820, computed grad = 0.161543153725685534
Iteration: 566, Checking gradient for words E[215][28]:	mock grad = -0.003183450736327087, computed grad = -0.003183450750582384
Iteration: 566, Checking gradient for hidden W[87][39]:	mock grad = -0.019676019587383520, computed grad = -0.019676019828236063
Iteration: 566, Checking gradient for hidden b[0][32]:	mock grad = 0.059808056690507927, computed grad = 0.059808076598931358
Iteration: 566, Checking gradient for output layer W[71][1]:	mock grad = 0.119982869547563542, computed grad = 0.119982868945635132
Iteration: 567, Checking gradient for words E[589][34]:	mock grad = -0.006362767775963674, computed grad = -0.006362767830394456
Iteration: 567, Checking gradient for hidden W[175][20]:	mock grad = 0.018677311866555080, computed grad = 0.018677312222106442
Iteration: 567, Checking gradient for hidden b[0][27]:	mock grad = -0.047412615265168823, computed grad = -0.047412638052527038
Iteration: 567, Checking gradient for output layer W[135][0]:	mock grad = -0.047575688811640759, computed grad = -0.047575688788854271
Iteration: 568, Checking gradient for words E[664][48]:	mock grad = 0.017190501935326896, computed grad = 0.017190502032900071
Iteration: 568, Checking gradient for hidden W[189][32]:	mock grad = -0.006406828240712592, computed grad = -0.006406828263359951
Iteration: 568, Checking gradient for hidden b[0][23]:	mock grad = -0.141530887190099985, computed grad = -0.141530926433240167
Iteration: 568, Checking gradient for output layer W[135][0]:	mock grad = -0.004243757156963124, computed grad = -0.004243757156928853
current: 70, Cost = 0.339438, Correct(%) = 1, time = 4.74523
Iteration: 569, Checking gradient for words E[664][3]:	mock grad = 0.003694237681262091, computed grad = 0.003694237732626939
Iteration: 569, Checking gradient for hidden W[195][20]:	mock grad = -0.011156490713121192, computed grad = -0.011156490893972248
Iteration: 569, Checking gradient for hidden b[0][10]:	mock grad = -0.080717066768959400, computed grad = -0.080717094032016254
Iteration: 569, Checking gradient for output layer W[100][0]:	mock grad = -0.105571941906024147, computed grad = -0.105571941190692783
Iteration: 570, Checking gradient for words E[684][36]:	mock grad = -0.007405109713448654, computed grad = -0.007405109753298391
Iteration: 570, Checking gradient for hidden W[114][18]:	mock grad = -0.000051672785134071, computed grad = -0.000051672763515469
Iteration: 570, Checking gradient for hidden b[0][16]:	mock grad = -0.022262120481314485, computed grad = -0.022262127463421256
Iteration: 570, Checking gradient for output layer W[81][1]:	mock grad = -0.024562565174773843, computed grad = -0.024562565172549251
Iteration: 571, Checking gradient for words E[69][40]:	mock grad = 0.003647259067512953, computed grad = 0.003647259039285308
Iteration: 571, Checking gradient for hidden W[21][37]:	mock grad = -0.015759676756099683, computed grad = -0.015759676903804885
Iteration: 571, Checking gradient for hidden b[0][20]:	mock grad = -0.126774091413484058, computed grad = -0.126774129303104999
Iteration: 571, Checking gradient for output layer W[12][0]:	mock grad = 0.072850679076613689, computed grad = 0.072850679011080194
Iteration: 572, Checking gradient for words E[34][24]:	mock grad = 0.018729813395512718, computed grad = 0.018729813434343084
Iteration: 572, Checking gradient for hidden W[168][2]:	mock grad = 0.022076056780645503, computed grad = 0.022076056821505780
Iteration: 572, Checking gradient for hidden b[0][7]:	mock grad = -0.098107146770859410, computed grad = -0.098107165374047983
Iteration: 572, Checking gradient for output layer W[126][1]:	mock grad = 0.152925548673243927, computed grad = 0.152925548021049740
Iteration: 573, Checking gradient for words E[333][8]:	mock grad = 0.044562471729459041, computed grad = 0.044562471764173980
Iteration: 573, Checking gradient for hidden W[175][44]:	mock grad = -0.001947962596504649, computed grad = -0.001947962598292451
Iteration: 573, Checking gradient for hidden b[0][33]:	mock grad = -0.137751208350689325, computed grad = -0.137751241531856333
Iteration: 573, Checking gradient for output layer W[88][0]:	mock grad = -0.035841047189122399, computed grad = -0.035841047184889806
Iteration: 574, Checking gradient for words E[609][20]:	mock grad = 0.033205295476568253, computed grad = 0.033205295411864635
Iteration: 574, Checking gradient for hidden W[212][49]:	mock grad = 0.010507707114026932, computed grad = 0.010507707149124032
Iteration: 574, Checking gradient for hidden b[0][28]:	mock grad = -0.014162655910976429, computed grad = -0.014162674358782861
Iteration: 574, Checking gradient for output layer W[60][0]:	mock grad = 0.068620711840827742, computed grad = 0.068620711800522191
Iteration: 575, Checking gradient for words E[122][10]:	mock grad = 0.031302052750759035, computed grad = 0.031302052769827386
Iteration: 575, Checking gradient for hidden W[45][21]:	mock grad = -0.000684026106007041, computed grad = -0.000684026093239642
Iteration: 575, Checking gradient for hidden b[0][35]:	mock grad = -0.013010223500392604, computed grad = -0.013010222091366134
Iteration: 575, Checking gradient for output layer W[99][1]:	mock grad = -0.175320358626884776, computed grad = -0.175320356748929623
Iteration: 576, Checking gradient for words E[42][38]:	mock grad = -0.057495020698433708, computed grad = -0.057495020770027758
Iteration: 576, Checking gradient for hidden W[170][31]:	mock grad = -0.007131308091518074, computed grad = -0.007131308120727881
Iteration: 576, Checking gradient for hidden b[0][46]:	mock grad = 0.160358665794807509, computed grad = 0.160358708616299389
Iteration: 576, Checking gradient for output layer W[121][0]:	mock grad = 0.114945241762781647, computed grad = 0.114945241416405955
Iteration: 577, Checking gradient for words E[698][36]:	mock grad = 0.038044777169732313, computed grad = 0.038044777128305257
Iteration: 577, Checking gradient for hidden W[227][47]:	mock grad = -0.001367210917563311, computed grad = -0.001367210917863650
Iteration: 577, Checking gradient for hidden b[0][49]:	mock grad = 0.041988650564095398, computed grad = 0.041988663355149836
Iteration: 577, Checking gradient for output layer W[53][1]:	mock grad = 0.044734535938756315, computed grad = 0.044734535883548449
Iteration: 578, Checking gradient for words E[555][14]:	mock grad = 0.034197294360716057, computed grad = 0.034197294525622937
Iteration: 578, Checking gradient for hidden W[198][17]:	mock grad = 0.000806579370571381, computed grad = 0.000806579384295007
Iteration: 578, Checking gradient for hidden b[0][5]:	mock grad = 0.020099952250940412, computed grad = 0.020099954928788261
Iteration: 578, Checking gradient for output layer W[90][0]:	mock grad = 0.043108572340661011, computed grad = 0.043108572316383126
current: 80, Cost = 0.36926, Correct(%) = 1, time = 5.2299
Iteration: 579, Checking gradient for words E[703][2]:	mock grad = 0.018917507230237218, computed grad = 0.018917507224615934
Iteration: 579, Checking gradient for hidden W[84][49]:	mock grad = -0.001636230519408421, computed grad = -0.001636230507085112
Iteration: 579, Checking gradient for hidden b[0][49]:	mock grad = 0.036589903241207589, computed grad = 0.036589909100326945
Iteration: 579, Checking gradient for output layer W[87][0]:	mock grad = -0.029623419184554578, computed grad = -0.029623419172540116
Iteration: 580, Checking gradient for words E[167][4]:	mock grad = 0.005656513896418192, computed grad = 0.005656513894363334
Iteration: 580, Checking gradient for hidden W[20][29]:	mock grad = -0.014820384916014451, computed grad = -0.014820385060045209
Iteration: 580, Checking gradient for hidden b[0][40]:	mock grad = -0.110875732495435697, computed grad = -0.110875760684904595
Iteration: 580, Checking gradient for output layer W[47][0]:	mock grad = 0.061450920916583662, computed grad = 0.061450920847309534
Iteration: 581, Checking gradient for words E[34][9]:	mock grad = -0.005319991952468239, computed grad = -0.005319991927682718
Iteration: 581, Checking gradient for hidden W[223][12]:	mock grad = -0.001099097572981877, computed grad = -0.001099097573281931
Iteration: 581, Checking gradient for hidden b[0][3]:	mock grad = -0.044579207754025241, computed grad = -0.044579221939326827
Iteration: 581, Checking gradient for output layer W[67][0]:	mock grad = -0.097170773409233080, computed grad = -0.097170773130601920
Iteration: 582, Checking gradient for words E[438][32]:	mock grad = 0.007530600505400997, computed grad = 0.007530600529104835
Iteration: 582, Checking gradient for hidden W[28][18]:	mock grad = 0.006394274466525740, computed grad = 0.006394274454997114
Iteration: 582, Checking gradient for hidden b[0][11]:	mock grad = -0.106559763263597285, computed grad = -0.106559794529615517
Iteration: 582, Checking gradient for output layer W[116][1]:	mock grad = -0.195513758413867755, computed grad = -0.195513756671049366
Iteration: 583, Checking gradient for words E[720][22]:	mock grad = -0.035466054978033901, computed grad = -0.035466055059526158
Iteration: 583, Checking gradient for hidden W[247][21]:	mock grad = -0.001792449346449398, computed grad = -0.001792449355974813
Iteration: 583, Checking gradient for hidden b[0][24]:	mock grad = -0.111998164057408589, computed grad = -0.111998174727971134
Iteration: 583, Checking gradient for output layer W[75][0]:	mock grad = -0.206541375935204163, computed grad = -0.206541374333687522
Iteration: 584, Checking gradient for words E[200][24]:	mock grad = 0.017720718467756358, computed grad = 0.017720718485777498
Iteration: 584, Checking gradient for hidden W[189][19]:	mock grad = 0.013808230959505563, computed grad = 0.013808231061734266
Iteration: 584, Checking gradient for hidden b[0][31]:	mock grad = -0.034079799098085672, computed grad = -0.034079800437840993
Iteration: 584, Checking gradient for output layer W[25][1]:	mock grad = 0.071039390378085931, computed grad = 0.071039390255799251
Iteration: 585, Checking gradient for words E[304][6]:	mock grad = -0.000162299805706034, computed grad = -0.000162299796912652
Iteration: 585, Checking gradient for hidden W[128][39]:	mock grad = -0.008844362894888658, computed grad = -0.008844362925834431
Iteration: 585, Checking gradient for hidden b[0][16]:	mock grad = -0.020388400290116682, computed grad = -0.020388408151436031
Iteration: 585, Checking gradient for output layer W[49][0]:	mock grad = 0.079495096249160291, computed grad = 0.079495096078806324
Iteration: 586, Checking gradient for words E[403][48]:	mock grad = 0.001218303463751402, computed grad = 0.001218303448632311
Iteration: 586, Checking gradient for hidden W[44][23]:	mock grad = 0.003340474833607443, computed grad = 0.003340474849917678
Iteration: 586, Checking gradient for hidden b[0][46]:	mock grad = 0.158925270031418098, computed grad = 0.158925312383594891
Iteration: 586, Checking gradient for output layer W[28][0]:	mock grad = -0.073142441457785257, computed grad = -0.073142441350130288
Iteration: 587, Checking gradient for words E[517][41]:	mock grad = 0.024796953172812053, computed grad = 0.022187500547338774
Iteration: 587, Checking gradient for hidden W[148][31]:	mock grad = 0.003977006014094231, computed grad = 0.003977006027514794
Iteration: 587, Checking gradient for hidden b[0][4]:	mock grad = 0.021754795026807106, computed grad = 0.021754801978241346
Iteration: 587, Checking gradient for output layer W[91][1]:	mock grad = 0.125127888538884013, computed grad = 0.125127887972344531
Iteration: 588, Checking gradient for words E[776][32]:	mock grad = -0.023541577559099691, computed grad = -0.023541577657237803
Iteration: 588, Checking gradient for hidden W[157][0]:	mock grad = 0.009563459039046984, computed grad = 0.009563459041992246
Iteration: 588, Checking gradient for hidden b[0][49]:	mock grad = -0.034602401921379755, computed grad = -0.034602398082205811
Iteration: 588, Checking gradient for output layer W[128][1]:	mock grad = 0.015834834503430928, computed grad = 0.015834834502345844
current: 90, Cost = 0.436412, Correct(%) = 1, time = 6.17537
Iteration: 589, Checking gradient for words E[652][8]:	mock grad = -0.018325351153453129, computed grad = -0.018325351237586374
Iteration: 589, Checking gradient for hidden W[172][24]:	mock grad = -0.022994198677156463, computed grad = -0.022994199066795406
Iteration: 589, Checking gradient for hidden b[0][44]:	mock grad = -0.001699169107111098, computed grad = -0.001699164024364227
Iteration: 589, Checking gradient for output layer W[60][1]:	mock grad = -0.067451987180611184, computed grad = -0.067451987103237063
Iteration: 590, Checking gradient for words E[119][17]:	mock grad = -0.004078378907257374, computed grad = -0.004078378845915492
Iteration: 590, Checking gradient for hidden W[220][43]:	mock grad = 0.003621301528339282, computed grad = 0.003621301586323158
Iteration: 590, Checking gradient for hidden b[0][19]:	mock grad = -0.045007189797607872, computed grad = -0.045007196499382278
Iteration: 590, Checking gradient for output layer W[137][0]:	mock grad = 0.185136376971511973, computed grad = 0.185136374432240208
Iteration: 591, Checking gradient for words E[84][39]:	mock grad = -0.018132005870502566, computed grad = -0.018132005885270232
Iteration: 591, Checking gradient for hidden W[249][12]:	mock grad = -0.013794705067321500, computed grad = -0.013794705279218954
Iteration: 591, Checking gradient for hidden b[0][9]:	mock grad = -0.057518031738562625, computed grad = -0.057518032138214110
Iteration: 591, Checking gradient for output layer W[129][0]:	mock grad = 0.190908055387323250, computed grad = 0.190908053497430230
Iteration: 592, Checking gradient for words E[102][35]:	mock grad = 0.020381746978576087, computed grad = 0.020381746962523446
Iteration: 592, Checking gradient for hidden W[235][31]:	mock grad = 0.013283536186792233, computed grad = 0.013283536188550519
Iteration: 592, Checking gradient for hidden b[0][33]:	mock grad = -0.086177680484439412, computed grad = -0.086177691539901638
Iteration: 592, Checking gradient for output layer W[118][1]:	mock grad = 0.023089693384964471, computed grad = 0.023089693378779617
Iteration: 593, Checking gradient for words E[287][26]:	mock grad = 0.007308642297143564, computed grad = 0.007308642242161323
Iteration: 593, Checking gradient for hidden W[70][22]:	mock grad = -0.002970188089118642, computed grad = -0.002970188132649332
Iteration: 593, Checking gradient for hidden b[0][11]:	mock grad = 0.101285479463864014, computed grad = 0.101285507367824976
Iteration: 593, Checking gradient for output layer W[135][0]:	mock grad = -0.059391568257877170, computed grad = -0.059391568198333383
Iteration: 594, Checking gradient for words E[762][49]:	mock grad = -0.015100306739623814, computed grad = -0.015100306747840332
Iteration: 594, Checking gradient for hidden W[27][33]:	mock grad = -0.001128187292254168, computed grad = -0.001128187350108407
Iteration: 594, Checking gradient for hidden b[0][20]:	mock grad = -0.100235771921225458, computed grad = -0.100235801999162252
Iteration: 594, Checking gradient for output layer W[142][0]:	mock grad = -0.116609021479585628, computed grad = -0.116609020684662265
Iteration: 595, Checking gradient for words E[410][25]:	mock grad = -0.031276560883530813, computed grad = -0.031276561008109396
Iteration: 595, Checking gradient for hidden W[86][5]:	mock grad = -0.018764228685463902, computed grad = -0.018764228890488371
Iteration: 595, Checking gradient for hidden b[0][47]:	mock grad = 0.092978447649799678, computed grad = 0.092978470775243890
Iteration: 595, Checking gradient for output layer W[73][1]:	mock grad = 0.012257896471773444, computed grad = 0.012257896471354637
Iteration: 596, Checking gradient for words E[119][23]:	mock grad = 0.059916185262548494, computed grad = 0.059916185289373362
Iteration: 596, Checking gradient for hidden W[32][43]:	mock grad = 0.000134832426168519, computed grad = 0.000134832427228544
Iteration: 596, Checking gradient for hidden b[0][23]:	mock grad = 0.153486731453877923, computed grad = 0.153486764205877757
Iteration: 596, Checking gradient for output layer W[71][0]:	mock grad = -0.084884836163423572, computed grad = -0.084884835980521864
Iteration: 597, Checking gradient for words E[771][10]:	mock grad = -0.008976718585074206, computed grad = -0.008976718585697391
Iteration: 597, Checking gradient for hidden W[128][5]:	mock grad = -0.012344234659289466, computed grad = -0.012344234707485668
Iteration: 597, Checking gradient for hidden b[0][16]:	mock grad = -0.027693182744648048, computed grad = -0.027693193529621549
Iteration: 597, Checking gradient for output layer W[82][1]:	mock grad = -0.302649237284791095, computed grad = -0.302649235860889654
Iteration: 598, Checking gradient for words E[776][30]:	mock grad = -0.001580403740236713, computed grad = -0.001580403720277570
Iteration: 598, Checking gradient for hidden W[248][16]:	mock grad = -0.004756688818441379, computed grad = -0.004756688866487204
Iteration: 598, Checking gradient for hidden b[0][40]:	mock grad = -0.101615598373960836, computed grad = -0.101615620472952181
Iteration: 598, Checking gradient for output layer W[143][1]:	mock grad = 0.182678569136701663, computed grad = 0.182678566805912074
current: 100, Cost = 0.440009, Correct(%) = 1, time = 6.95907
Iteration: 599, Checking gradient for words E[362][46]:	mock grad = -0.012649541537762543, computed grad = -0.012649541596882672
Iteration: 599, Checking gradient for hidden W[75][13]:	mock grad = -0.000516134102013233, computed grad = -0.000516134097360794
Iteration: 599, Checking gradient for hidden b[0][4]:	mock grad = 0.024264615272107104, computed grad = 0.024264624941931063
Iteration: 599, Checking gradient for output layer W[41][1]:	mock grad = -0.063003744487644120, computed grad = -0.063003744426662234
current: 6, Correct(%) = 1, time = 7.01041
Dev start.
Dev finished. Total time taken is: 0.635887
dev:
Accuracy:	P=100/100=1
Test start.
Test finished. Total time taken is: 0.543438
test:
Accuracy:	P=59/100=0.59
##### Iteration 6
random: 0, 99
Iteration: 600, Checking gradient for words E[14][4]:	mock grad = -0.017215954564137048, computed grad = -0.017215954570910703
Iteration: 600, Checking gradient for hidden W[160][40]:	mock grad = 0.003010254184532268, computed grad = 0.003010254188017473
Iteration: 600, Checking gradient for hidden b[0][16]:	mock grad = 0.014554486686579882, computed grad = 0.014554490438180903
Iteration: 600, Checking gradient for output layer W[53][0]:	mock grad = -0.050271027485421227, computed grad = -0.050271027376979013
Iteration: 601, Checking gradient for words E[12][23]:	mock grad = 0.016943429971438828, computed grad = 0.016943430005361186
Iteration: 601, Checking gradient for hidden W[24][11]:	mock grad = -0.006896251898064198, computed grad = -0.006896251931559928
Iteration: 601, Checking gradient for hidden b[0][4]:	mock grad = 0.016029803733358383, computed grad = 0.016029805969469534
Iteration: 601, Checking gradient for output layer W[144][1]:	mock grad = -0.073514155403159309, computed grad = -0.073514155231795802
Iteration: 602, Checking gradient for words E[6][27]:	mock grad = 0.011611683165962949, computed grad = 0.011611683150159714
Iteration: 602, Checking gradient for hidden W[115][39]:	mock grad = 0.006193032459078651, computed grad = 0.006193032547501148
Iteration: 602, Checking gradient for hidden b[0][35]:	mock grad = -0.007963111056330519, computed grad = -0.007963107509543552
Iteration: 602, Checking gradient for output layer W[112][0]:	mock grad = -0.015532794517048032, computed grad = -0.015532794515034224
Iteration: 603, Checking gradient for words E[60][0]:	mock grad = 0.088735601789458052, computed grad = 0.088735602024720639
Iteration: 603, Checking gradient for hidden W[55][23]:	mock grad = -0.005164294773513101, computed grad = -0.005164294793340695
Iteration: 603, Checking gradient for hidden b[0][32]:	mock grad = -0.066395155905035752, computed grad = -0.066395177824184831
Iteration: 603, Checking gradient for output layer W[44][1]:	mock grad = -0.046616161681778401, computed grad = -0.046616161663189229
Iteration: 604, Checking gradient for words E[171][0]:	mock grad = 0.016052800252075450, computed grad = 0.016052800336462891
Iteration: 604, Checking gradient for hidden W[227][5]:	mock grad = 0.015043627954236527, computed grad = 0.015043628048322584
Iteration: 604, Checking gradient for hidden b[0][2]:	mock grad = -0.092189336793113696, computed grad = -0.092189380196293622
Iteration: 604, Checking gradient for output layer W[77][0]:	mock grad = 0.119199018603549645, computed grad = 0.119199018133085713
Iteration: 605, Checking gradient for words E[223][26]:	mock grad = -0.015473144260980121, computed grad = -0.015473144286494536
Iteration: 605, Checking gradient for hidden W[72][7]:	mock grad = -0.009380317313123498, computed grad = -0.009380317404487718
Iteration: 605, Checking gradient for hidden b[0][36]:	mock grad = -0.004740766521293960, computed grad = -0.004740779717491304
Iteration: 605, Checking gradient for output layer W[11][1]:	mock grad = -0.038812893157824657, computed grad = -0.038812893134666591
Iteration: 606, Checking gradient for words E[48][22]:	mock grad = 0.018683235703031986, computed grad = 0.018683235724883313
Iteration: 606, Checking gradient for hidden W[85][35]:	mock grad = -0.000499446427243422, computed grad = -0.000499446423115723
Iteration: 606, Checking gradient for hidden b[0][7]:	mock grad = -0.065853315047881278, computed grad = -0.065853323769566680
Iteration: 606, Checking gradient for output layer W[6][1]:	mock grad = -0.034272787098332813, computed grad = -0.034272787065575322
Iteration: 607, Checking gradient for words E[48][37]:	mock grad = -0.013739751240471332, computed grad = -0.013739751065712966
Iteration: 607, Checking gradient for hidden W[102][42]:	mock grad = -0.028981538737482548, computed grad = -0.028981539764631123
Iteration: 607, Checking gradient for hidden b[0][25]:	mock grad = 0.006899361369938628, computed grad = 0.006899356372796053
Iteration: 607, Checking gradient for output layer W[64][1]:	mock grad = 0.108388825748650941, computed grad = 0.108388825641567557
Iteration: 608, Checking gradient for words E[776][20]:	mock grad = 0.007445650449772190, computed grad = 0.007445650665638324
Iteration: 608, Checking gradient for hidden W[215][22]:	mock grad = 0.000808944302849168, computed grad = 0.000808944316766680
Iteration: 608, Checking gradient for hidden b[0][18]:	mock grad = -0.038272109100345730, computed grad = -0.038272087669375558
Iteration: 608, Checking gradient for output layer W[91][0]:	mock grad = 0.017347786793214803, computed grad = 0.017347786790950531
current: 10, Cost = 0.407844, Correct(%) = 1, time = 0.769478
Iteration: 609, Checking gradient for words E[103][18]:	mock grad = -0.016265000018278464, computed grad = -0.016265000064610777
Iteration: 609, Checking gradient for hidden W[181][25]:	mock grad = -0.004258654810712637, computed grad = -0.004258654889773411
Iteration: 609, Checking gradient for hidden b[0][9]:	mock grad = -0.055495834589885940, computed grad = -0.055495834939197346
Iteration: 609, Checking gradient for output layer W[72][1]:	mock grad = 0.036726180769025563, computed grad = 0.036726180752822711
Iteration: 610, Checking gradient for words E[312][3]:	mock grad = 0.006076239240759840, computed grad = 0.006076239230522695
Iteration: 610, Checking gradient for hidden W[132][49]:	mock grad = -0.004386093000879887, computed grad = -0.004386093000508781
Iteration: 610, Checking gradient for hidden b[0][30]:	mock grad = 0.000859363562505377, computed grad = 0.000859361757757052
Iteration: 610, Checking gradient for output layer W[105][0]:	mock grad = -0.078502038148020592, computed grad = -0.078502037829445717
Iteration: 611, Checking gradient for words E[134][35]:	mock grad = 0.021732780122074580, computed grad = 0.021732780088344374
Iteration: 611, Checking gradient for hidden W[248][4]:	mock grad = 0.008060248329527786, computed grad = 0.008060248362982301
Iteration: 611, Checking gradient for hidden b[0][4]:	mock grad = 0.025258266627636861, computed grad = 0.025258276565766354
Iteration: 611, Checking gradient for output layer W[32][1]:	mock grad = 0.131705328298786783, computed grad = 0.131705327869141575
Iteration: 612, Checking gradient for words E[194][20]:	mock grad = 0.014804597607026571, computed grad = 0.014804597640029567
Iteration: 612, Checking gradient for hidden W[208][32]:	mock grad = -0.001276771901870255, computed grad = -0.001276771905845378
Iteration: 612, Checking gradient for hidden b[0][8]:	mock grad = -0.041701441510472170, computed grad = -0.041701465537980306
Iteration: 612, Checking gradient for output layer W[145][0]:	mock grad = -0.102639394085113356, computed grad = -0.102639393523915906
Iteration: 613, Checking gradient for words E[28][29]:	mock grad = -0.034799574044064308, computed grad = -0.034799574009000869
Iteration: 613, Checking gradient for hidden W[185][1]:	mock grad = -0.005627915162426111, computed grad = -0.005627915147458369
Iteration: 613, Checking gradient for hidden b[0][36]:	mock grad = -0.006571399789934151, computed grad = -0.006571394893833809
Iteration: 613, Checking gradient for output layer W[76][1]:	mock grad = 0.055546405442086222, computed grad = 0.055546405366902751
Iteration: 614, Checking gradient for words E[118][32]:	mock grad = -0.053610548029214833, computed grad = -0.053610548258837556
Iteration: 614, Checking gradient for hidden W[29][43]:	mock grad = -0.002929585404043422, computed grad = -0.002929585444324482
Iteration: 614, Checking gradient for hidden b[0][4]:	mock grad = -0.031732408489693054, computed grad = -0.031732423102121182
Iteration: 614, Checking gradient for output layer W[92][0]:	mock grad = 0.017459257944107964, computed grad = 0.017459257943937621
Iteration: 615, Checking gradient for words E[102][28]:	mock grad = 0.010316182703223875, computed grad = 0.010316182680844577
Iteration: 615, Checking gradient for hidden W[240][0]:	mock grad = -0.016951520596386604, computed grad = -0.016951520703394351
Iteration: 615, Checking gradient for hidden b[0][33]:	mock grad = -0.113945460883224658, computed grad = -0.113945492554686986
Iteration: 615, Checking gradient for output layer W[86][0]:	mock grad = 0.039014334986847699, computed grad = 0.039014334967673869
Iteration: 616, Checking gradient for words E[102][17]:	mock grad = -0.013079940021643033, computed grad = -0.013079940060650703
Iteration: 616, Checking gradient for hidden W[156][1]:	mock grad = 0.019209913883683205, computed grad = 0.019209914036578148
Iteration: 616, Checking gradient for hidden b[0][16]:	mock grad = -0.020296760386229185, computed grad = -0.020296766976450560
Iteration: 616, Checking gradient for output layer W[72][1]:	mock grad = -0.067766696935922521, computed grad = -0.067766696854263384
Iteration: 617, Checking gradient for words E[42][34]:	mock grad = 0.014403431428833269, computed grad = 0.014403431451582370
Iteration: 617, Checking gradient for hidden W[204][33]:	mock grad = 0.001285102422698969, computed grad = 0.001285102423136010
Iteration: 617, Checking gradient for hidden b[0][19]:	mock grad = 0.032499333724039792, computed grad = 0.032499330335455857
Iteration: 617, Checking gradient for output layer W[125][1]:	mock grad = 0.074083293832571950, computed grad = 0.074083293503889730
Iteration: 618, Checking gradient for words E[91][49]:	mock grad = -0.012664573194043349, computed grad = -0.012664573171771316
Iteration: 618, Checking gradient for hidden W[180][41]:	mock grad = -0.000307228273543902, computed grad = -0.000307228265584873
Iteration: 618, Checking gradient for hidden b[0][17]:	mock grad = -0.017419340698487940, computed grad = -0.017419345307574594
Iteration: 618, Checking gradient for output layer W[148][1]:	mock grad = 0.160923987956584913, computed grad = 0.160923985421414562
current: 20, Cost = 0.485428, Correct(%) = 1, time = 1.43093
Iteration: 619, Checking gradient for words E[435][7]:	mock grad = -0.018295309329563469, computed grad = -0.018295309401966983
Iteration: 619, Checking gradient for hidden W[185][47]:	mock grad = 0.017419865176143690, computed grad = 0.017419865290971975
Iteration: 619, Checking gradient for hidden b[0][4]:	mock grad = 0.027001669318349464, computed grad = 0.027001680788851491
Iteration: 619, Checking gradient for output layer W[90][1]:	mock grad = 0.020245232694643400, computed grad = 0.020245232693247763
Iteration: 620, Checking gradient for words E[68][21]:	mock grad = -0.038946545074303174, computed grad = -0.038946545217191819
Iteration: 620, Checking gradient for hidden W[112][33]:	mock grad = 0.001312732426733687, computed grad = 0.001312732431036296
Iteration: 620, Checking gradient for hidden b[0][31]:	mock grad = 0.041825910818749090, computed grad = 0.041825925602046815
Iteration: 620, Checking gradient for output layer W[22][1]:	mock grad = 0.045280149049031770, computed grad = 0.045280149012674936
Iteration: 621, Checking gradient for words E[776][9]:	mock grad = 0.009052307982149177, computed grad = 0.009052308087836990
Iteration: 621, Checking gradient for hidden W[226][47]:	mock grad = -0.008677048206673366, computed grad = -0.008677048319196322
Iteration: 621, Checking gradient for hidden b[0][37]:	mock grad = -0.070039052912251876, computed grad = -0.070039075350423224
Iteration: 621, Checking gradient for output layer W[54][1]:	mock grad = -0.139245141710875320, computed grad = -0.139245141361251185
Iteration: 622, Checking gradient for words E[294][26]:	mock grad = 0.003792808374980527, computed grad = 0.003792808357353642
Iteration: 622, Checking gradient for hidden W[154][40]:	mock grad = -0.002005559465112494, computed grad = -0.002005559422907192
Iteration: 622, Checking gradient for hidden b[0][42]:	mock grad = 0.064457840947551581, computed grad = 0.064457860945299081
Iteration: 622, Checking gradient for output layer W[88][1]:	mock grad = -0.101785578077995265, computed grad = -0.101785577515010772
Iteration: 623, Checking gradient for words E[455][12]:	mock grad = 0.013499594287674777, computed grad = 0.013499594366588086
Iteration: 623, Checking gradient for hidden W[32][25]:	mock grad = -0.008561901187531529, computed grad = -0.008561901241721985
Iteration: 623, Checking gradient for hidden b[0][35]:	mock grad = -0.013772617672075382, computed grad = -0.013772617256220393
Iteration: 623, Checking gradient for output layer W[106][0]:	mock grad = -0.045167833940373825, computed grad = -0.045167833903814437
Iteration: 624, Checking gradient for words E[285][41]:	mock grad = -0.004459377154708566, computed grad = -0.004459377231643228
Iteration: 624, Checking gradient for hidden W[234][36]:	mock grad = -0.009633069088355040, computed grad = -0.009633069177946188
Iteration: 624, Checking gradient for hidden b[0][22]:	mock grad = 0.040547518125461091, computed grad = 0.040547522253962243
Iteration: 624, Checking gradient for output layer W[121][1]:	mock grad = -0.177575683249275862, computed grad = -0.177575681613729996
Iteration: 625, Checking gradient for words E[465][25]:	mock grad = -0.028132756092230649, computed grad = -0.028132756148099802
Iteration: 625, Checking gradient for hidden W[103][46]:	mock grad = -0.012528290130375819, computed grad = -0.012528290241220699
Iteration: 625, Checking gradient for hidden b[0][14]:	mock grad = -0.057131206572358373, computed grad = -0.057131229385848220
Iteration: 625, Checking gradient for output layer W[14][0]:	mock grad = 0.002391708716326102, computed grad = 0.002391708716280273
Iteration: 626, Checking gradient for words E[475][13]:	mock grad = -0.003317741573027888, computed grad = -0.003317741578877626
Iteration: 626, Checking gradient for hidden W[109][5]:	mock grad = 0.003190821895343587, computed grad = 0.003190821866488066
Iteration: 626, Checking gradient for hidden b[0][2]:	mock grad = -0.062174736407266673, computed grad = -0.062174755965000515
Iteration: 626, Checking gradient for output layer W[28][0]:	mock grad = 0.066044816264820838, computed grad = 0.066044816068768844
Iteration: 627, Checking gradient for words E[487][9]:	mock grad = -0.048600766393513961, computed grad = -0.048600766360154181
Iteration: 627, Checking gradient for hidden W[189][29]:	mock grad = -0.028471808348584648, computed grad = -0.028471808930334748
Iteration: 627, Checking gradient for hidden b[0][34]:	mock grad = -0.043356422673279660, computed grad = -0.043356436626462894
Iteration: 627, Checking gradient for output layer W[53][1]:	mock grad = 0.093319385007173405, computed grad = 0.093319384922207121
Iteration: 628, Checking gradient for words E[180][21]:	mock grad = -0.018363263158460708, computed grad = -0.018363263232728012
Iteration: 628, Checking gradient for hidden W[178][44]:	mock grad = -0.032531821389542026, computed grad = -0.032531821916217035
Iteration: 628, Checking gradient for hidden b[0][1]:	mock grad = -0.090007520235052541, computed grad = -0.090007532096630136
Iteration: 628, Checking gradient for output layer W[91][1]:	mock grad = 0.124807680707578195, computed grad = 0.124807679530772889
current: 30, Cost = 0.375571, Correct(%) = 1, time = 2.12701
Iteration: 629, Checking gradient for words E[21][41]:	mock grad = -0.032227659836625566, computed grad = -0.032227659873554242
Iteration: 629, Checking gradient for hidden W[1][20]:	mock grad = -0.002943185592185760, computed grad = -0.002943185585810055
Iteration: 629, Checking gradient for hidden b[0][42]:	mock grad = 0.067654608112810433, computed grad = 0.067654629969796376
Iteration: 629, Checking gradient for output layer W[67][1]:	mock grad = -0.062853387525041438, computed grad = -0.062853387416666170
Iteration: 630, Checking gradient for words E[377][23]:	mock grad = 0.035574487636857999, computed grad = 0.035574487691109533
Iteration: 630, Checking gradient for hidden W[17][37]:	mock grad = -0.007475569870935006, computed grad = -0.007475569897017156
Iteration: 630, Checking gradient for hidden b[0][43]:	mock grad = -0.011909025679102125, computed grad = -0.011909029247861459
Iteration: 630, Checking gradient for output layer W[80][1]:	mock grad = -0.006219964373943565, computed grad = -0.006219964373801860
Iteration: 631, Checking gradient for words E[362][3]:	mock grad = -0.049392465592934531, computed grad = -0.049392465814753088
Iteration: 631, Checking gradient for hidden W[67][31]:	mock grad = 0.007136553240694177, computed grad = 0.007136553252592421
Iteration: 631, Checking gradient for hidden b[0][5]:	mock grad = -0.024992240932775189, computed grad = -0.024992244651207002
Iteration: 631, Checking gradient for output layer W[83][0]:	mock grad = -0.176236004198249141, computed grad = -0.176236003579230943
Iteration: 632, Checking gradient for words E[102][11]:	mock grad = -0.026596085503233224, computed grad = -0.026596085535900267
Iteration: 632, Checking gradient for hidden W[74][39]:	mock grad = 0.006229958399078805, computed grad = 0.006229958386888349
Iteration: 632, Checking gradient for hidden b[0][46]:	mock grad = 0.136079056640897589, computed grad = 0.136079085945936706
Iteration: 632, Checking gradient for output layer W[142][1]:	mock grad = -0.196782229278102960, computed grad = -0.196782225481118622
Iteration: 633, Checking gradient for words E[524][47]:	mock grad = 0.002861387532970827, computed grad = 0.002861387515917507
Iteration: 633, Checking gradient for hidden W[139][25]:	mock grad = -0.007929747330437387, computed grad = -0.007929747535833963
Iteration: 633, Checking gradient for hidden b[0][10]:	mock grad = -0.080946396026126965, computed grad = -0.080946422024118697
Iteration: 633, Checking gradient for output layer W[91][0]:	mock grad = -0.005944690776721329, computed grad = -0.005944690776590769
Iteration: 634, Checking gradient for words E[330][4]:	mock grad = -0.017537896700725586, computed grad = -0.017537896687558230
Iteration: 634, Checking gradient for hidden W[235][21]:	mock grad = 0.003586335097144344, computed grad = 0.003586335126066459
Iteration: 634, Checking gradient for hidden b[0][46]:	mock grad = 0.173570893524876979, computed grad = 0.173570945341908811
Iteration: 634, Checking gradient for output layer W[63][0]:	mock grad = -0.064870798063054735, computed grad = -0.064870798016772868
Iteration: 635, Checking gradient for words E[383][18]:	mock grad = -0.001390330580663957, computed grad = -0.001390330534879809
Iteration: 635, Checking gradient for hidden W[248][1]:	mock grad = -0.002599665089780823, computed grad = -0.002599665188219387
Iteration: 635, Checking gradient for hidden b[0][47]:	mock grad = -0.099070072483414817, computed grad = -0.099070099339486142
Iteration: 635, Checking gradient for output layer W[102][0]:	mock grad = 0.176437571677651173, computed grad = 0.176437570540204286
Iteration: 636, Checking gradient for words E[102][49]:	mock grad = -0.029579680359492766, computed grad = -0.029579680407055643
Iteration: 636, Checking gradient for hidden W[223][14]:	mock grad = -0.012914984167333721, computed grad = -0.012914984396725310
Iteration: 636, Checking gradient for hidden b[0][0]:	mock grad = -0.204115101726354276, computed grad = -0.204115155499289713
Iteration: 636, Checking gradient for output layer W[77][1]:	mock grad = 0.012860801003711231, computed grad = 0.012860801003424786
Iteration: 637, Checking gradient for words E[536][11]:	mock grad = 0.029864847482102830, computed grad = 0.029366571888987298
Iteration: 637, Checking gradient for hidden W[146][19]:	mock grad = 0.002108207280304697, computed grad = 0.002108207282563941
Iteration: 637, Checking gradient for hidden b[0][49]:	mock grad = -0.030098566311753672, computed grad = -0.030098560980538824
Iteration: 637, Checking gradient for output layer W[28][1]:	mock grad = -0.069458631851260222, computed grad = -0.069458631740246884
Iteration: 638, Checking gradient for words E[542][13]:	mock grad = 0.014684591723074236, computed grad = 0.014684591732457317
Iteration: 638, Checking gradient for hidden W[244][35]:	mock grad = 0.002321690854317771, computed grad = 0.002321690854810385
Iteration: 638, Checking gradient for hidden b[0][30]:	mock grad = 0.002887973901438690, computed grad = 0.002887980459071164
Iteration: 638, Checking gradient for output layer W[23][0]:	mock grad = 0.085911333747246132, computed grad = 0.085911333630329351
current: 40, Cost = 0.329849, Correct(%) = 1, time = 2.56183
Iteration: 639, Checking gradient for words E[546][9]:	mock grad = -0.005723837698895506, computed grad = -0.005723837683287530
Iteration: 639, Checking gradient for hidden W[112][40]:	mock grad = 0.000561440969243376, computed grad = 0.000561440984852592
Iteration: 639, Checking gradient for hidden b[0][29]:	mock grad = 0.076455849773005014, computed grad = 0.076455877269000896
Iteration: 639, Checking gradient for output layer W[133][1]:	mock grad = 0.058446000109430019, computed grad = 0.058445999976661597
Iteration: 640, Checking gradient for words E[377][4]:	mock grad = 0.017035922087210720, computed grad = 0.017035922205331316
Iteration: 640, Checking gradient for hidden W[93][2]:	mock grad = -0.002414873435691023, computed grad = -0.002414873440952016
Iteration: 640, Checking gradient for hidden b[0][9]:	mock grad = 0.048391423161125990, computed grad = 0.048391430853161548
Iteration: 640, Checking gradient for output layer W[9][1]:	mock grad = -0.044789794340360789, computed grad = -0.044789794282688020
Iteration: 641, Checking gradient for words E[24][34]:	mock grad = -0.001337573493742639, computed grad = -0.001337573600673519
Iteration: 641, Checking gradient for hidden W[130][23]:	mock grad = 0.004020533496462697, computed grad = 0.004020533438123400
Iteration: 641, Checking gradient for hidden b[0][2]:	mock grad = 0.070232611352083474, computed grad = 0.070232643977773637
Iteration: 641, Checking gradient for output layer W[116][0]:	mock grad = 0.100880736127628712, computed grad = 0.100880736031644908
Iteration: 642, Checking gradient for words E[101][11]:	mock grad = 0.031818774086866952, computed grad = 0.031818774048034307
Iteration: 642, Checking gradient for hidden W[212][45]:	mock grad = -0.001183839751450755, computed grad = -0.001183839759663383
Iteration: 642, Checking gradient for hidden b[0][43]:	mock grad = -0.014541097345471821, computed grad = -0.014541101475600834
Iteration: 642, Checking gradient for output layer W[40][1]:	mock grad = -0.053203596614093129, computed grad = -0.053203596590499926
Iteration: 643, Checking gradient for words E[564][23]:	mock grad = 0.054677758822119227, computed grad = 0.054677758958308731
Iteration: 643, Checking gradient for hidden W[37][46]:	mock grad = -0.023855539027062278, computed grad = -0.023855539412117224
Iteration: 643, Checking gradient for hidden b[0][37]:	mock grad = 0.069879509200398982, computed grad = 0.069879522434430380
Iteration: 643, Checking gradient for output layer W[67][1]:	mock grad = 0.121608768841485748, computed grad = 0.121608768683795290
Iteration: 644, Checking gradient for words E[568][32]:	mock grad = 0.002535363398670221, computed grad = 0.002535363397274799
Iteration: 644, Checking gradient for hidden W[18][24]:	mock grad = -0.006644977788183848, computed grad = -0.006644977803512607
Iteration: 644, Checking gradient for hidden b[0][45]:	mock grad = 0.006270573061961660, computed grad = 0.006270572964911117
Iteration: 644, Checking gradient for output layer W[33][0]:	mock grad = 0.033345066876633300, computed grad = 0.033345066843460086
Iteration: 645, Checking gradient for words E[194][27]:	mock grad = 0.061807692572690698, computed grad = 0.061807692679645240
Iteration: 645, Checking gradient for hidden W[143][15]:	mock grad = 0.002125988437656767, computed grad = 0.002125988439695934
Iteration: 645, Checking gradient for hidden b[0][4]:	mock grad = 0.025462996789654690, computed grad = 0.025463006665883282
Iteration: 645, Checking gradient for output layer W[8][0]:	mock grad = 0.083585125964708018, computed grad = 0.083585125853662123
Iteration: 646, Checking gradient for words E[407][25]:	mock grad = 0.009104209966637189, computed grad = 0.009104209988435294
Iteration: 646, Checking gradient for hidden W[162][10]:	mock grad = -0.003598308096625091, computed grad = -0.003598308105346941
Iteration: 646, Checking gradient for hidden b[0][23]:	mock grad = 0.124866540718804320, computed grad = 0.124866568399789726
Iteration: 646, Checking gradient for output layer W[89][1]:	mock grad = 0.138457331292834995, computed grad = 0.138457329482031460
Iteration: 647, Checking gradient for words E[19][42]:	mock grad = 0.020388925321468410, computed grad = 0.020388925374808731
Iteration: 647, Checking gradient for hidden W[166][41]:	mock grad = -0.002489837819308338, computed grad = -0.002489837813799352
Iteration: 647, Checking gradient for hidden b[0][7]:	mock grad = -0.064063864212121624, computed grad = -0.064063866325515237
Iteration: 647, Checking gradient for output layer W[147][0]:	mock grad = -0.037759918750157606, computed grad = -0.037759918725202138
Iteration: 648, Checking gradient for words E[252][43]:	mock grad = -0.005616462479124396, computed grad = -0.005616462462038559
Iteration: 648, Checking gradient for hidden W[150][24]:	mock grad = -0.003627661321475850, computed grad = -0.003627661336715693
Iteration: 648, Checking gradient for hidden b[0][11]:	mock grad = -0.081509528315232282, computed grad = -0.081509550660150656
Iteration: 648, Checking gradient for output layer W[86][1]:	mock grad = 0.062043597388566907, computed grad = 0.062043597235316023
current: 50, Cost = 0.336092, Correct(%) = 1, time = 3.45247
Iteration: 649, Checking gradient for words E[197][17]:	mock grad = 0.030859485032591616, computed grad = 0.030859485092120314
Iteration: 649, Checking gradient for hidden W[69][10]:	mock grad = -0.001525715849576281, computed grad = -0.001525715830386735
Iteration: 649, Checking gradient for hidden b[0][31]:	mock grad = -0.031245059835871558, computed grad = -0.031245062969038145
Iteration: 649, Checking gradient for output layer W[27][1]:	mock grad = -0.032741137970554446, computed grad = -0.032741137948564904
Iteration: 650, Checking gradient for words E[48][6]:	mock grad = -0.015135465015581051, computed grad = -0.015135465150121374
Iteration: 650, Checking gradient for hidden W[114][45]:	mock grad = 0.003686261266566682, computed grad = 0.003686261296759078
Iteration: 650, Checking gradient for hidden b[0][40]:	mock grad = 0.131004583596716628, computed grad = 0.131004616106351279
Iteration: 650, Checking gradient for output layer W[60][0]:	mock grad = -0.117563697173195258, computed grad = -0.117563697016158264
Iteration: 651, Checking gradient for words E[131][31]:	mock grad = 0.012802418138196936, computed grad = 0.012802418164951007
Iteration: 651, Checking gradient for hidden W[221][37]:	mock grad = 0.010233328702674571, computed grad = 0.010233328718356985
Iteration: 651, Checking gradient for hidden b[0][24]:	mock grad = 0.098106542429310384, computed grad = 0.098106564593205908
Iteration: 651, Checking gradient for output layer W[46][0]:	mock grad = 0.097629068015997467, computed grad = 0.097629067756826199
Iteration: 652, Checking gradient for words E[107][10]:	mock grad = -0.000059068396229822, computed grad = -0.000059068472617534
Iteration: 652, Checking gradient for hidden W[249][8]:	mock grad = 0.027663349563084028, computed grad = 0.027663349607103250
Iteration: 652, Checking gradient for hidden b[0][44]:	mock grad = 0.011699342349236197, computed grad = 0.011699360744564477
Iteration: 652, Checking gradient for output layer W[127][1]:	mock grad = 0.177337158626778013, computed grad = 0.177337157566103687
Iteration: 653, Checking gradient for words E[97][48]:	mock grad = -0.012962797466403719, computed grad = -0.012962797523865621
Iteration: 653, Checking gradient for hidden W[184][30]:	mock grad = 0.003228377218278400, computed grad = 0.003228377219233611
Iteration: 653, Checking gradient for hidden b[0][4]:	mock grad = 0.019444839737425834, computed grad = 0.019444845219683381
Iteration: 653, Checking gradient for output layer W[143][1]:	mock grad = -0.206791625781271859, computed grad = -0.206791622252171736
Iteration: 654, Checking gradient for words E[632][4]:	mock grad = -0.002339436858178612, computed grad = -0.002339436842638284
Iteration: 654, Checking gradient for hidden W[35][31]:	mock grad = -0.018812129967099578, computed grad = -0.018812130343573395
Iteration: 654, Checking gradient for hidden b[0][41]:	mock grad = 0.033784408934589560, computed grad = 0.033784418114587614
Iteration: 654, Checking gradient for output layer W[57][1]:	mock grad = -0.165956334043287912, computed grad = -0.165956331037153043
Iteration: 655, Checking gradient for words E[776][22]:	mock grad = 0.012371547508271963, computed grad = 0.012371547557670809
Iteration: 655, Checking gradient for hidden W[145][21]:	mock grad = -0.003178407482568035, computed grad = -0.003178407617120267
Iteration: 655, Checking gradient for hidden b[0][20]:	mock grad = -0.098306843326650961, computed grad = -0.098306872754277227
Iteration: 655, Checking gradient for output layer W[72][0]:	mock grad = 0.078745899107363071, computed grad = 0.078745898825292163
Iteration: 656, Checking gradient for words E[435][15]:	mock grad = 0.077839318983308203, computed grad = 0.079315068382550605
Iteration: 656, Checking gradient for hidden W[123][21]:	mock grad = 0.007269358051675834, computed grad = 0.007269358115682423
Iteration: 656, Checking gradient for hidden b[0][23]:	mock grad = -0.182773176977457652, computed grad = -0.182773234414312008
Iteration: 656, Checking gradient for output layer W[106][1]:	mock grad = 0.071578720060860856, computed grad = 0.071578720024515846
Iteration: 657, Checking gradient for words E[338][5]:	mock grad = 0.000469864798735564, computed grad = 0.000469864809681819
Iteration: 657, Checking gradient for hidden W[79][12]:	mock grad = 0.012640153796955600, computed grad = 0.012640153930497881
Iteration: 657, Checking gradient for hidden b[0][2]:	mock grad = 0.077308560087641087, computed grad = 0.077308606486160586
Iteration: 657, Checking gradient for output layer W[26][1]:	mock grad = -0.048220478678612233, computed grad = -0.048220478630174625
Iteration: 658, Checking gradient for words E[109][24]:	mock grad = 0.009673188701930791, computed grad = 0.009673188833749095
Iteration: 658, Checking gradient for hidden W[107][18]:	mock grad = -0.011268252334106776, computed grad = -0.011268252284455724
Iteration: 658, Checking gradient for hidden b[0][41]:	mock grad = 0.041999463043507212, computed grad = 0.041999474344782468
Iteration: 658, Checking gradient for output layer W[114][1]:	mock grad = 0.137710153170600424, computed grad = 0.137710152500980981
current: 60, Cost = 0.425334, Correct(%) = 1, time = 4.14981
Iteration: 659, Checking gradient for words E[25][43]:	mock grad = 0.005257487336512989, computed grad = 0.005257487346977981
Iteration: 659, Checking gradient for hidden W[226][34]:	mock grad = -0.005891405609054212, computed grad = -0.005891405609115284
Iteration: 659, Checking gradient for hidden b[0][18]:	mock grad = 0.059438787723131714, computed grad = 0.059438783425640504
Iteration: 659, Checking gradient for output layer W[93][1]:	mock grad = -0.082699179583867188, computed grad = -0.082699179426156288
Iteration: 660, Checking gradient for words E[102][44]:	mock grad = 0.009717295777311730, computed grad = 0.009717295827940797
Iteration: 660, Checking gradient for hidden W[63][34]:	mock grad = -0.012033267242250734, computed grad = -0.012033267327064291
Iteration: 660, Checking gradient for hidden b[0][10]:	mock grad = 0.103593374998017795, computed grad = 0.103593397727147402
Iteration: 660, Checking gradient for output layer W[18][1]:	mock grad = 0.117847014036276398, computed grad = 0.117847013684223317
Iteration: 661, Checking gradient for words E[514][25]:	mock grad = -0.019822769018573716, computed grad = -0.019822768999547359
Iteration: 661, Checking gradient for hidden W[169][5]:	mock grad = 0.000808774590854711, computed grad = 0.000808774661902331
Iteration: 661, Checking gradient for hidden b[0][33]:	mock grad = 0.117764913947576710, computed grad = 0.117764938989751933
Iteration: 661, Checking gradient for output layer W[77][0]:	mock grad = 0.042156638811324809, computed grad = 0.042156638799960532
Iteration: 662, Checking gradient for words E[658][39]:	mock grad = 0.002222357877973424, computed grad = 0.002222357866249581
Iteration: 662, Checking gradient for hidden W[7][43]:	mock grad = 0.001781198123051997, computed grad = 0.001781198128911300
Iteration: 662, Checking gradient for hidden b[0][0]:	mock grad = -0.177702534476131824, computed grad = -0.177702577090634167
Iteration: 662, Checking gradient for output layer W[63][0]:	mock grad = -0.041739875429824824, computed grad = -0.041739875409126374
Iteration: 663, Checking gradient for words E[208][2]:	mock grad = -0.007884894635795670, computed grad = -0.007884894627564301
Iteration: 663, Checking gradient for hidden W[113][25]:	mock grad = -0.007330999922089410, computed grad = -0.007331000048573621
Iteration: 663, Checking gradient for hidden b[0][14]:	mock grad = 0.072915242094656607, computed grad = 0.072915270764954923
Iteration: 663, Checking gradient for output layer W[91][1]:	mock grad = -0.007116847863974929, computed grad = -0.007116847863916280
Iteration: 664, Checking gradient for words E[316][28]:	mock grad = -0.020396163631974940, computed grad = -0.020396163624836407
Iteration: 664, Checking gradient for hidden W[95][4]:	mock grad = 0.003436340397594551, computed grad = 0.003436340418828689
Iteration: 664, Checking gradient for hidden b[0][14]:	mock grad = 0.065819440424219611, computed grad = 0.065819460657334378
Iteration: 664, Checking gradient for output layer W[78][0]:	mock grad = 0.166483553563218090, computed grad = 0.166483552593570672
Iteration: 665, Checking gradient for words E[362][2]:	mock grad = 0.004988650573523046, computed grad = 0.004988650616047784
Iteration: 665, Checking gradient for hidden W[224][40]:	mock grad = 0.000268251448554135, computed grad = 0.000268251470430334
Iteration: 665, Checking gradient for hidden b[0][34]:	mock grad = -0.023395216286331566, computed grad = -0.023395218512448503
Iteration: 665, Checking gradient for output layer W[39][0]:	mock grad = -0.046566028280237148, computed grad = -0.046566028196412271
Iteration: 666, Checking gradient for words E[296][9]:	mock grad = -0.002499476585954286, computed grad = -0.002499476570097062
Iteration: 666, Checking gradient for hidden W[240][22]:	mock grad = -0.003607718845055619, computed grad = -0.003607718856155463
Iteration: 666, Checking gradient for hidden b[0][38]:	mock grad = 0.057935578912288177, computed grad = 0.057935585159901791
Iteration: 666, Checking gradient for output layer W[2][0]:	mock grad = 0.066266515069218768, computed grad = 0.066266514949305827
Iteration: 667, Checking gradient for words E[148][24]:	mock grad = -0.005688444087242894, computed grad = -0.005688444084624817
Iteration: 667, Checking gradient for hidden W[217][8]:	mock grad = 0.005897227160434060, computed grad = 0.005897227187595999
Iteration: 667, Checking gradient for hidden b[0][22]:	mock grad = -0.049586082008473698, computed grad = -0.049586092852241019
Iteration: 667, Checking gradient for output layer W[6][1]:	mock grad = -0.051694388854606199, computed grad = -0.051694388818104675
Iteration: 668, Checking gradient for words E[474][8]:	mock grad = -0.002452419101989234, computed grad = -0.001207076244666736
Iteration: 668, Checking gradient for hidden W[164][14]:	mock grad = 0.000358375641634767, computed grad = 0.000358375604457408
Iteration: 668, Checking gradient for hidden b[0][2]:	mock grad = -0.064677246239652764, computed grad = -0.064677270445592530
Iteration: 668, Checking gradient for output layer W[108][0]:	mock grad = -0.154768983804232096, computed grad = -0.154768981946324047
current: 70, Cost = 0.322572, Correct(%) = 1, time = 4.7652
Iteration: 669, Checking gradient for words E[311][14]:	mock grad = -0.004553823360842202, computed grad = -0.004553823381902724
Iteration: 669, Checking gradient for hidden W[221][47]:	mock grad = -0.000025200873499109, computed grad = -0.000025200872614620
Iteration: 669, Checking gradient for hidden b[0][36]:	mock grad = 0.006159415498835363, computed grad = 0.006159411001736425
Iteration: 669, Checking gradient for output layer W[103][0]:	mock grad = -0.109097475361613627, computed grad = -0.109097474436706796
Iteration: 670, Checking gradient for words E[34][38]:	mock grad = -0.017822020586338860, computed grad = -0.020535639856995372
Iteration: 670, Checking gradient for hidden W[99][43]:	mock grad = 0.001603293560803465, computed grad = 0.001603293574375151
Iteration: 670, Checking gradient for hidden b[0][36]:	mock grad = 0.005035791091179131, computed grad = 0.005035782075309848
Iteration: 670, Checking gradient for output layer W[78][0]:	mock grad = 0.149592650930757687, computed grad = 0.149592650310261838
Iteration: 671, Checking gradient for words E[555][22]:	mock grad = -0.047271523005737448, computed grad = -0.047271523274213093
Iteration: 671, Checking gradient for hidden W[129][19]:	mock grad = -0.012619956577503144, computed grad = -0.012619956596449551
Iteration: 671, Checking gradient for hidden b[0][4]:	mock grad = -0.025272048617325238, computed grad = -0.025272060175668756
Iteration: 671, Checking gradient for output layer W[136][1]:	mock grad = 0.161143041498418294, computed grad = 0.161143040615591926
Iteration: 672, Checking gradient for words E[394][31]:	mock grad = -0.005802805716353943, computed grad = -0.005802805716978316
Iteration: 672, Checking gradient for hidden W[61][35]:	mock grad = 0.000463479209955597, computed grad = 0.000463479182820634
Iteration: 672, Checking gradient for hidden b[0][9]:	mock grad = 0.068151245825986972, computed grad = 0.068151261046309142
Iteration: 672, Checking gradient for output layer W[120][0]:	mock grad = -0.043259079281876556, computed grad = -0.043259079264343546
Iteration: 673, Checking gradient for words E[333][4]:	mock grad = 0.052703970704448544, computed grad = 0.052703970750949389
Iteration: 673, Checking gradient for hidden W[141][38]:	mock grad = 0.009864618415789117, computed grad = 0.009864618612372868
Iteration: 673, Checking gradient for hidden b[0][14]:	mock grad = -0.069017537648985439, computed grad = -0.069017562735397039
Iteration: 673, Checking gradient for output layer W[134][1]:	mock grad = -0.116514786145383020, computed grad = -0.116514785965320333
Iteration: 674, Checking gradient for words E[102][1]:	mock grad = -0.030624581105559434, computed grad = -0.030624581167756303
Iteration: 674, Checking gradient for hidden W[9][44]:	mock grad = 0.020466150958547979, computed grad = 0.020466150942626927
Iteration: 674, Checking gradient for hidden b[0][25]:	mock grad = -0.006310008170806736, computed grad = -0.006310003237429643
Iteration: 674, Checking gradient for output layer W[17][1]:	mock grad = 0.019651308909601761, computed grad = 0.019651308908433227
Iteration: 675, Checking gradient for words E[694][2]:	mock grad = 0.003057779634496605, computed grad = 0.003057779594735229
Iteration: 675, Checking gradient for hidden W[164][28]:	mock grad = -0.013849517108532883, computed grad = -0.013849517305600436
Iteration: 675, Checking gradient for hidden b[0][45]:	mock grad = 0.010108668363484341, computed grad = 0.010108671100973061
Iteration: 675, Checking gradient for output layer W[42][0]:	mock grad = -0.041693353716315551, computed grad = -0.041693353686184494
Iteration: 676, Checking gradient for words E[320][0]:	mock grad = -0.026654150977661883, computed grad = -0.026654151065515243
Iteration: 676, Checking gradient for hidden W[154][1]:	mock grad = -0.013727700254373421, computed grad = -0.013727700509217974
Iteration: 676, Checking gradient for hidden b[0][8]:	mock grad = 0.034775384453455960, computed grad = 0.034775406668709458
Iteration: 676, Checking gradient for output layer W[96][1]:	mock grad = 0.208669378596232358, computed grad = 0.208669376081077329
Iteration: 677, Checking gradient for words E[94][22]:	mock grad = 0.008783016037877678, computed grad = 0.009864093527930643
Iteration: 677, Checking gradient for hidden W[45][12]:	mock grad = -0.000782150675893156, computed grad = -0.000782150691809332
Iteration: 677, Checking gradient for hidden b[0][24]:	mock grad = -0.086310857495219828, computed grad = -0.086310869737165521
Iteration: 677, Checking gradient for output layer W[11][1]:	mock grad = 0.023802681089979405, computed grad = 0.023802681080356235
Iteration: 678, Checking gradient for words E[166][0]:	mock grad = -0.020385852839865937, computed grad = -0.020385852878476496
Iteration: 678, Checking gradient for hidden W[143][4]:	mock grad = -0.008585435281482923, computed grad = -0.008585435333782497
Iteration: 678, Checking gradient for hidden b[0][42]:	mock grad = 0.072187532122708342, computed grad = 0.072187555732498437
Iteration: 678, Checking gradient for output layer W[12][0]:	mock grad = 0.052570293113535227, computed grad = 0.052570293061794733
current: 80, Cost = 0.353816, Correct(%) = 1, time = 5.25305
Iteration: 679, Checking gradient for words E[706][0]:	mock grad = -0.007510172746083565, computed grad = -0.007510172747632656
Iteration: 679, Checking gradient for hidden W[60][25]:	mock grad = -0.004711573059562602, computed grad = -0.004711573072943881
Iteration: 679, Checking gradient for hidden b[0][15]:	mock grad = -0.090482184388201992, computed grad = -0.090482205781150918
Iteration: 679, Checking gradient for output layer W[80][1]:	mock grad = 0.069152839949571110, computed grad = 0.069152839773490252
Iteration: 680, Checking gradient for words E[707][31]:	mock grad = 0.044034169659007416, computed grad = 0.044034169693401556
Iteration: 680, Checking gradient for hidden W[244][41]:	mock grad = -0.009427085046864736, computed grad = -0.009427085113305832
Iteration: 680, Checking gradient for hidden b[0][18]:	mock grad = -0.070011982476608781, computed grad = -0.070011973649956219
Iteration: 680, Checking gradient for output layer W[99][1]:	mock grad = -0.201919694003044325, computed grad = -0.201919691122109252
Iteration: 681, Checking gradient for words E[712][16]:	mock grad = 0.008962650311278297, computed grad = 0.008962650341435008
Iteration: 681, Checking gradient for hidden W[144][21]:	mock grad = 0.001865448667870417, computed grad = 0.001865448673152677
Iteration: 681, Checking gradient for hidden b[0][17]:	mock grad = 0.018972563087693572, computed grad = 0.018972566835463410
Iteration: 681, Checking gradient for output layer W[65][0]:	mock grad = -0.204782512939871708, computed grad = -0.204782509690532522
Iteration: 682, Checking gradient for words E[671][28]:	mock grad = -0.016981194898291196, computed grad = -0.016981194940929759
Iteration: 682, Checking gradient for hidden W[170][18]:	mock grad = 0.000071138916452052, computed grad = 0.000071139055805234
Iteration: 682, Checking gradient for hidden b[0][49]:	mock grad = 0.040461477151476632, computed grad = 0.040461483329667466
Iteration: 682, Checking gradient for output layer W[148][1]:	mock grad = -0.191916349246812334, computed grad = -0.191916347117509878
Iteration: 683, Checking gradient for words E[148][24]:	mock grad = -0.015547165171825306, computed grad = -0.015547165220804928
Iteration: 683, Checking gradient for hidden W[124][12]:	mock grad = 0.000739346578210176, computed grad = 0.000739346586552677
Iteration: 683, Checking gradient for hidden b[0][34]:	mock grad = -0.028478866739622966, computed grad = -0.028478866716064203
Iteration: 683, Checking gradient for output layer W[135][1]:	mock grad = -0.056063685567181443, computed grad = -0.056063685529203301
Iteration: 684, Checking gradient for words E[293][8]:	mock grad = -0.023252166699405663, computed grad = -0.023252166744726889
Iteration: 684, Checking gradient for hidden W[194][44]:	mock grad = -0.023414812089311843, computed grad = -0.023414812201941207
Iteration: 684, Checking gradient for hidden b[0][22]:	mock grad = 0.039226443230622188, computed grad = 0.039226446895730079
Iteration: 684, Checking gradient for output layer W[122][0]:	mock grad = 0.160940554956967574, computed grad = 0.160940553255550067
Iteration: 685, Checking gradient for words E[582][0]:	mock grad = -0.047598726514469503, computed grad = -0.047598726775504030
Iteration: 685, Checking gradient for hidden W[28][1]:	mock grad = 0.002438583336539235, computed grad = 0.002438583357448150
Iteration: 685, Checking gradient for hidden b[0][42]:	mock grad = 0.057878927882976106, computed grad = 0.057878940531797887
Iteration: 685, Checking gradient for output layer W[122][1]:	mock grad = 0.128905844143240023, computed grad = 0.128905843225608885
Iteration: 686, Checking gradient for words E[10][7]:	mock grad = 0.019923377887587357, computed grad = 0.019923377969395879
Iteration: 686, Checking gradient for hidden W[198][20]:	mock grad = -0.001038864318209098, computed grad = -0.001038864271276886
Iteration: 686, Checking gradient for hidden b[0][30]:	mock grad = -0.003591325304447279, computed grad = -0.003591326377752060
Iteration: 686, Checking gradient for output layer W[8][1]:	mock grad = -0.061237790791618396, computed grad = -0.061237790714872391
Iteration: 687, Checking gradient for words E[424][46]:	mock grad = 0.012706880169494061, computed grad = 0.012706880208549822
Iteration: 687, Checking gradient for hidden W[208][47]:	mock grad = 0.003350339503910282, computed grad = 0.003350339511133985
Iteration: 687, Checking gradient for hidden b[0][9]:	mock grad = -0.051604611603417450, computed grad = -0.051604609548802069
Iteration: 687, Checking gradient for output layer W[84][1]:	mock grad = 0.158053519625972472, computed grad = 0.158053518264801462
Iteration: 688, Checking gradient for words E[48][0]:	mock grad = 0.007346033914862327, computed grad = 0.007346033826819882
Iteration: 688, Checking gradient for hidden W[118][24]:	mock grad = 0.005169911774433045, computed grad = 0.005169911773775796
Iteration: 688, Checking gradient for hidden b[0][28]:	mock grad = -0.011617777381123240, computed grad = -0.011617793347925160
Iteration: 688, Checking gradient for output layer W[53][0]:	mock grad = 0.097307290808645863, computed grad = 0.097307290505321789
current: 90, Cost = 0.41072, Correct(%) = 1, time = 6.19672
Iteration: 689, Checking gradient for words E[743][16]:	mock grad = -0.001577803087654539, computed grad = -0.001577803087334354
Iteration: 689, Checking gradient for hidden W[8][23]:	mock grad = -0.007182254744525496, computed grad = -0.007182254770233267
Iteration: 689, Checking gradient for hidden b[0][22]:	mock grad = -0.040509690358403461, computed grad = -0.040509693291647657
Iteration: 689, Checking gradient for output layer W[26][1]:	mock grad = 0.059881361213182105, computed grad = 0.059881361144881455
Iteration: 690, Checking gradient for words E[330][29]:	mock grad = -0.011802851345443788, computed grad = -0.011802851371147272
Iteration: 690, Checking gradient for hidden W[29][26]:	mock grad = 0.000555012906022778, computed grad = 0.000555012913403310
Iteration: 690, Checking gradient for hidden b[0][39]:	mock grad = 0.073618650031970212, computed grad = 0.073618664925279270
Iteration: 690, Checking gradient for output layer W[69][1]:	mock grad = 0.087933115635763226, computed grad = 0.087933115309721005
Iteration: 691, Checking gradient for words E[326][43]:	mock grad = -0.017379644169063235, computed grad = -0.017379644184897382
Iteration: 691, Checking gradient for hidden W[171][9]:	mock grad = 0.007676207854662165, computed grad = 0.007676207854707292
Iteration: 691, Checking gradient for hidden b[0][36]:	mock grad = -0.000523756203518744, computed grad = -0.000523745221791339
Iteration: 691, Checking gradient for output layer W[93][0]:	mock grad = -0.021697330983616592, computed grad = -0.021697330980515447
Iteration: 692, Checking gradient for words E[754][26]:	mock grad = -0.005393668442321697, computed grad = -0.005393668419111977
Iteration: 692, Checking gradient for hidden W[146][38]:	mock grad = 0.004644400053988029, computed grad = 0.004644400069967199
Iteration: 692, Checking gradient for hidden b[0][5]:	mock grad = -0.003482013473776746, computed grad = -0.003482004463130895
Iteration: 692, Checking gradient for output layer W[130][0]:	mock grad = 0.122118587175645832, computed grad = 0.122118586065635290
Iteration: 693, Checking gradient for words E[48][6]:	mock grad = -0.016140538327130693, computed grad = -0.016140538362516433
Iteration: 693, Checking gradient for hidden W[235][11]:	mock grad = -0.009107143005282481, computed grad = -0.009107143087985643
Iteration: 693, Checking gradient for hidden b[0][42]:	mock grad = 0.065929746001003320, computed grad = 0.065929764211541592
Iteration: 693, Checking gradient for output layer W[148][0]:	mock grad = -0.173683509991001683, computed grad = -0.173683508190080022
Iteration: 694, Checking gradient for words E[225][42]:	mock grad = 0.002634818114938264, computed grad = 0.002634818082410814
Iteration: 694, Checking gradient for hidden W[239][4]:	mock grad = -0.005216186609674489, computed grad = -0.005216186621633830
Iteration: 694, Checking gradient for hidden b[0][48]:	mock grad = -0.076393607303992805, computed grad = -0.076393622582380513
Iteration: 694, Checking gradient for output layer W[9][0]:	mock grad = 0.053960488604676282, computed grad = 0.053960488507105761
Iteration: 695, Checking gradient for words E[48][42]:	mock grad = 0.008755985664049559, computed grad = 0.008755985681740464
Iteration: 695, Checking gradient for hidden W[94][23]:	mock grad = -0.007877722076943261, computed grad = -0.007877722151820817
Iteration: 695, Checking gradient for hidden b[0][33]:	mock grad = 0.108646187508470993, computed grad = 0.108646208952082476
Iteration: 695, Checking gradient for output layer W[32][0]:	mock grad = 0.115285797067266049, computed grad = 0.115285796696444467
Iteration: 696, Checking gradient for words E[102][12]:	mock grad = -0.001703524341145624, computed grad = -0.001703524346865039
Iteration: 696, Checking gradient for hidden W[56][43]:	mock grad = -0.001239284358967030, computed grad = -0.001239284367495853
Iteration: 696, Checking gradient for hidden b[0][25]:	mock grad = 0.004847297527615924, computed grad = 0.004847293831782361
Iteration: 696, Checking gradient for output layer W[2][0]:	mock grad = 0.039807753334591967, computed grad = 0.039807753311140767
Iteration: 697, Checking gradient for words E[146][38]:	mock grad = 0.040887306003434265, computed grad = 0.040887306156426467
Iteration: 697, Checking gradient for hidden W[164][31]:	mock grad = -0.028158528215027623, computed grad = -0.028158529041020040
Iteration: 697, Checking gradient for hidden b[0][0]:	mock grad = 0.222596402309871433, computed grad = 0.222596468944902315
Iteration: 697, Checking gradient for output layer W[34][1]:	mock grad = -0.029542714066022047, computed grad = -0.029542714064173064
Iteration: 698, Checking gradient for words E[509][24]:	mock grad = 0.039014945940579748, computed grad = 0.039014946117651578
Iteration: 698, Checking gradient for hidden W[248][20]:	mock grad = 0.008950116293926502, computed grad = 0.008950116365390066
Iteration: 698, Checking gradient for hidden b[0][15]:	mock grad = 0.081708003466712809, computed grad = 0.081708010431946670
Iteration: 698, Checking gradient for output layer W[74][1]:	mock grad = -0.140737736533946833, computed grad = -0.140737735221525634
current: 100, Cost = 0.416051, Correct(%) = 1, time = 6.98027
Iteration: 699, Checking gradient for words E[409][5]:	mock grad = 0.029379565921222595, computed grad = 0.029379566037513707
Iteration: 699, Checking gradient for hidden W[144][45]:	mock grad = -0.005403298238226828, computed grad = -0.005403298281778080
Iteration: 699, Checking gradient for hidden b[0][23]:	mock grad = 0.152558418938603113, computed grad = 0.152558451587542138
Iteration: 699, Checking gradient for output layer W[74][0]:	mock grad = -0.163882061819636826, computed grad = -0.163882060485927322
current: 7, Correct(%) = 1, time = 7.03172
Dev start.
Dev finished. Total time taken is: 0.634505
dev:
Accuracy:	P=100/100=1
Test start.
Test finished. Total time taken is: 0.547405
test:
Accuracy:	P=59/100=0.59
##### Iteration 7
random: 0, 99
Iteration: 700, Checking gradient for words E[26][22]:	mock grad = 0.000921059626290965, computed grad = 0.000921059657812960
Iteration: 700, Checking gradient for hidden W[148][35]:	mock grad = -0.000100236767386352, computed grad = -0.000100236786935582
Iteration: 700, Checking gradient for hidden b[0][31]:	mock grad = -0.025535538058896190, computed grad = -0.025535538586946984
Iteration: 700, Checking gradient for output layer W[66][1]:	mock grad = 0.027370773912349655, computed grad = 0.027370773892558320
Iteration: 701, Checking gradient for words E[73][18]:	mock grad = 0.011121944692660257, computed grad = 0.011121944722751855
Iteration: 701, Checking gradient for hidden W[124][15]:	mock grad = -0.009482847783653936, computed grad = -0.009482847808543941
Iteration: 701, Checking gradient for hidden b[0][40]:	mock grad = 0.103964043824317232, computed grad = 0.103964073481182279
Iteration: 701, Checking gradient for output layer W[47][0]:	mock grad = -0.051659397798248241, computed grad = -0.051659397731774769
Iteration: 702, Checking gradient for words E[114][49]:	mock grad = 0.003465690537413080, computed grad = 0.003465690536543650
Iteration: 702, Checking gradient for hidden W[226][24]:	mock grad = -0.017592693783458069, computed grad = -0.017592694045561925
Iteration: 702, Checking gradient for hidden b[0][8]:	mock grad = -0.049742434332200602, computed grad = -0.049742460692253868
Iteration: 702, Checking gradient for output layer W[95][1]:	mock grad = -0.027988290075442457, computed grad = -0.027988290061891814
Iteration: 703, Checking gradient for words E[138][2]:	mock grad = -0.008562274983359153, computed grad = -0.008562275057344077
Iteration: 703, Checking gradient for hidden W[164][32]:	mock grad = 0.009358462348812990, computed grad = 0.009358462401576669
Iteration: 703, Checking gradient for hidden b[0][11]:	mock grad = 0.099709571292960764, computed grad = 0.099709591785658774
Iteration: 703, Checking gradient for output layer W[41][0]:	mock grad = -0.064264121894197501, computed grad = -0.064264121833852134
Iteration: 704, Checking gradient for words E[171][37]:	mock grad = 0.010678107049799657, computed grad = 0.010678107054128671
Iteration: 704, Checking gradient for hidden W[26][47]:	mock grad = 0.018990046647182623, computed grad = 0.018990046826835742
Iteration: 704, Checking gradient for hidden b[0][29]:	mock grad = 0.090858553317363366, computed grad = 0.090858585453969978
Iteration: 704, Checking gradient for output layer W[89][0]:	mock grad = 0.111568610652756917, computed grad = 0.111568610181156949
Iteration: 705, Checking gradient for words E[118][28]:	mock grad = -0.018426152115141869, computed grad = -0.018426152199972179
Iteration: 705, Checking gradient for hidden W[72][34]:	mock grad = -0.008272587746632754, computed grad = -0.008272587826471656
Iteration: 705, Checking gradient for hidden b[0][25]:	mock grad = -0.002914570525036630, computed grad = -0.002914565723606433
Iteration: 705, Checking gradient for output layer W[60][1]:	mock grad = -0.089934337313163359, computed grad = -0.089934336992178066
Iteration: 706, Checking gradient for words E[255][46]:	mock grad = 0.008125508057788000, computed grad = 0.008125508088487686
Iteration: 706, Checking gradient for hidden W[81][34]:	mock grad = -0.010737137420446796, computed grad = -0.010737137662764363
Iteration: 706, Checking gradient for hidden b[0][20]:	mock grad = -0.086834419237791227, computed grad = -0.086834445282625378
Iteration: 706, Checking gradient for output layer W[101][1]:	mock grad = 0.091164622243927740, computed grad = 0.091164621549017624
Iteration: 707, Checking gradient for words E[25][9]:	mock grad = 0.091791971850163634, computed grad = 0.091791971975786230
Iteration: 707, Checking gradient for hidden W[227][3]:	mock grad = -0.008031557386933841, computed grad = -0.008031557473737674
Iteration: 707, Checking gradient for hidden b[0][29]:	mock grad = -0.111384402111280867, computed grad = -0.111384439046239875
Iteration: 707, Checking gradient for output layer W[108][0]:	mock grad = 0.205063088365597945, computed grad = 0.205063087509665648
Iteration: 708, Checking gradient for words E[776][8]:	mock grad = 0.046581016462016178, computed grad = 0.046581016414452357
Iteration: 708, Checking gradient for hidden W[53][11]:	mock grad = 0.003371259907902768, computed grad = 0.003371259915353794
Iteration: 708, Checking gradient for hidden b[0][45]:	mock grad = 0.008742582574911006, computed grad = 0.008742584109074646
Iteration: 708, Checking gradient for output layer W[2][0]:	mock grad = -0.063773769294867000, computed grad = -0.063773769164797767
current: 10, Cost = 0.393302, Correct(%) = 1, time = 0.772728
Iteration: 709, Checking gradient for words E[148][10]:	mock grad = -0.048932714712929259, computed grad = -0.048932714846104571
Iteration: 709, Checking gradient for hidden W[60][39]:	mock grad = 0.004617804800627345, computed grad = 0.004617804815090039
Iteration: 709, Checking gradient for hidden b[0][45]:	mock grad = -0.010213912500250943, computed grad = -0.010213914954437119
Iteration: 709, Checking gradient for output layer W[81][1]:	mock grad = 0.045641839943738605, computed grad = 0.045641839908317751
Iteration: 710, Checking gradient for words E[312][49]:	mock grad = 0.019999706237172621, computed grad = 0.019999706250541760
Iteration: 710, Checking gradient for hidden W[155][4]:	mock grad = -0.000410623585578307, computed grad = -0.000410623590539232
Iteration: 710, Checking gradient for hidden b[0][7]:	mock grad = -0.063184149529715850, computed grad = -0.063184155294573704
Iteration: 710, Checking gradient for output layer W[14][1]:	mock grad = 0.001858849132313489, computed grad = 0.001858849132262141
Iteration: 711, Checking gradient for words E[328][47]:	mock grad = 0.009424162149856974, computed grad = 0.009424162212732897
Iteration: 711, Checking gradient for hidden W[227][15]:	mock grad = -0.026411602152970337, computed grad = -0.026411602682088271
Iteration: 711, Checking gradient for hidden b[0][26]:	mock grad = -0.033861008254199021, computed grad = -0.033861014437976816
Iteration: 711, Checking gradient for output layer W[31][1]:	mock grad = -0.054895073977268005, computed grad = -0.054895073942364460
Iteration: 712, Checking gradient for words E[254][1]:	mock grad = 0.015212313049656201, computed grad = 0.014480934789391407
Iteration: 712, Checking gradient for hidden W[116][41]:	mock grad = 0.006643453149934864, computed grad = 0.006643453235719719
Iteration: 712, Checking gradient for hidden b[0][5]:	mock grad = 0.008459004224026234, computed grad = 0.008458997049444486
Iteration: 712, Checking gradient for output layer W[32][1]:	mock grad = -0.085645000658746140, computed grad = -0.085645000283564018
Iteration: 713, Checking gradient for words E[158][3]:	mock grad = -0.041771657877859303, computed grad = -0.041771657960462547
Iteration: 713, Checking gradient for hidden W[92][28]:	mock grad = -0.007765504317969185, computed grad = -0.007765504356396070
Iteration: 713, Checking gradient for hidden b[0][4]:	mock grad = 0.016057148800818766, computed grad = 0.016057151829246968
Iteration: 713, Checking gradient for output layer W[20][1]:	mock grad = 0.004390954516320100, computed grad = 0.004390954516231406
Iteration: 714, Checking gradient for words E[375][22]:	mock grad = 0.014911442676845965, computed grad = 0.014911442736756909
Iteration: 714, Checking gradient for hidden W[161][49]:	mock grad = -0.012810321984246809, computed grad = -0.012810322147372086
Iteration: 714, Checking gradient for hidden b[0][1]:	mock grad = 0.136608202703891735, computed grad = 0.136608244567738762
Iteration: 714, Checking gradient for output layer W[86][1]:	mock grad = -0.059260973216568313, computed grad = -0.059260973205631458
Iteration: 715, Checking gradient for words E[49][44]:	mock grad = -0.019404924221561037, computed grad = -0.019404924257113876
Iteration: 715, Checking gradient for hidden W[145][33]:	mock grad = -0.001666117688620128, computed grad = -0.001666117680037626
Iteration: 715, Checking gradient for hidden b[0][4]:	mock grad = 0.021101833138970072, computed grad = 0.021101841135180068
Iteration: 715, Checking gradient for output layer W[139][0]:	mock grad = 0.098229883174649757, computed grad = 0.098229882820878758
Iteration: 716, Checking gradient for words E[69][29]:	mock grad = -0.015308329046781477, computed grad = -0.015308329036939669
Iteration: 716, Checking gradient for hidden W[206][39]:	mock grad = 0.015210149507199899, computed grad = 0.015210149621407315
Iteration: 716, Checking gradient for hidden b[0][10]:	mock grad = -0.094342298270710945, computed grad = -0.094342327834828502
Iteration: 716, Checking gradient for output layer W[16][0]:	mock grad = -0.026435561784915462, computed grad = -0.026435561779191402
Iteration: 717, Checking gradient for words E[420][28]:	mock grad = 0.006817082774029926, computed grad = 0.006817082808475386
Iteration: 717, Checking gradient for hidden W[212][17]:	mock grad = -0.001379997814465739, computed grad = -0.001379997816246195
Iteration: 717, Checking gradient for hidden b[0][33]:	mock grad = 0.075165515673381034, computed grad = 0.075165528440841040
Iteration: 717, Checking gradient for output layer W[76][0]:	mock grad = 0.085106993563566213, computed grad = 0.085106992984300370
Iteration: 718, Checking gradient for words E[365][36]:	mock grad = -0.000815166631418229, computed grad = -0.000583365499399796
Iteration: 718, Checking gradient for hidden W[97][33]:	mock grad = -0.003068623688318173, computed grad = -0.003068623752477869
Iteration: 718, Checking gradient for hidden b[0][31]:	mock grad = 0.036557456284114220, computed grad = 0.036557469672344550
Iteration: 718, Checking gradient for output layer W[35][1]:	mock grad = -0.088209666110894425, computed grad = -0.088209665628309855
current: 20, Cost = 0.475684, Correct(%) = 1, time = 1.43064
Iteration: 719, Checking gradient for words E[440][6]:	mock grad = 0.017425644323110223, computed grad = 0.017425644330186622
Iteration: 719, Checking gradient for hidden W[31][10]:	mock grad = 0.006445646439173780, computed grad = 0.006445646501584672
Iteration: 719, Checking gradient for hidden b[0][41]:	mock grad = -0.045746017745124101, computed grad = -0.045746030811254002
Iteration: 719, Checking gradient for output layer W[51][1]:	mock grad = 0.094088016526938656, computed grad = 0.094088016380684897
Iteration: 720, Checking gradient for words E[443][17]:	mock grad = -0.007466914788478718, computed grad = -0.007466914794685551
Iteration: 720, Checking gradient for hidden W[211][6]:	mock grad = -0.001509976150032566, computed grad = -0.001509976167284187
Iteration: 720, Checking gradient for hidden b[0][23]:	mock grad = -0.133916831435682004, computed grad = -0.133916868259347205
Iteration: 720, Checking gradient for output layer W[121][0]:	mock grad = -0.111591649029435525, computed grad = -0.111591648396654822
Iteration: 721, Checking gradient for words E[102][3]:	mock grad = 0.016853713240227997, computed grad = 0.016250425926917195
Iteration: 721, Checking gradient for hidden W[71][49]:	mock grad = -0.014597126839488439, computed grad = -0.014597126908364462
Iteration: 721, Checking gradient for hidden b[0][2]:	mock grad = -0.067141110202340037, computed grad = -0.067141137156500053
Iteration: 721, Checking gradient for output layer W[22][1]:	mock grad = 0.093147694129824821, computed grad = 0.093147694017425758
Iteration: 722, Checking gradient for words E[109][11]:	mock grad = -0.006283974589071928, computed grad = -0.006283974595148282
Iteration: 722, Checking gradient for hidden W[124][17]:	mock grad = -0.000290753305709490, computed grad = -0.000290753314755331
Iteration: 722, Checking gradient for hidden b[0][42]:	mock grad = 0.062377255025586331, computed grad = 0.062377274371501437
Iteration: 722, Checking gradient for output layer W[134][1]:	mock grad = 0.096603737926387323, computed grad = 0.096603737382205151
Iteration: 723, Checking gradient for words E[457][7]:	mock grad = 0.010624368399991813, computed grad = 0.010624368418906528
Iteration: 723, Checking gradient for hidden W[61][25]:	mock grad = 0.007806137766269705, computed grad = 0.007806137808422080
Iteration: 723, Checking gradient for hidden b[0][28]:	mock grad = -0.021741989338269763, computed grad = -0.021742014812351943
Iteration: 723, Checking gradient for output layer W[122][1]:	mock grad = 0.105506033588637083, computed grad = 0.105506033055607518
Iteration: 724, Checking gradient for words E[69][6]:	mock grad = 0.024568994414192469, computed grad = 0.024568994499466250
Iteration: 724, Checking gradient for hidden W[185][2]:	mock grad = -0.016533015222419678, computed grad = -0.016533015434456427
Iteration: 724, Checking gradient for hidden b[0][48]:	mock grad = 0.075920710652460643, computed grad = 0.075920704185147542
Iteration: 724, Checking gradient for output layer W[135][1]:	mock grad = -0.014786188524767985, computed grad = -0.014786188523696965
Iteration: 725, Checking gradient for words E[306][32]:	mock grad = 0.001397370839928413, computed grad = 0.001397370783621661
Iteration: 725, Checking gradient for hidden W[9][10]:	mock grad = 0.018327499400949687, computed grad = 0.018327499576447463
Iteration: 725, Checking gradient for hidden b[0][38]:	mock grad = 0.067734781569822600, computed grad = 0.067734798521392720
Iteration: 725, Checking gradient for output layer W[131][0]:	mock grad = 0.085621692108212022, computed grad = 0.085621691869099389
Iteration: 726, Checking gradient for words E[237][38]:	mock grad = 0.008220383620860527, computed grad = 0.008220383646285323
Iteration: 726, Checking gradient for hidden W[6][19]:	mock grad = 0.002700207177408487, computed grad = 0.002700207182757321
Iteration: 726, Checking gradient for hidden b[0][24]:	mock grad = 0.075248673236466557, computed grad = 0.075248691527659339
Iteration: 726, Checking gradient for output layer W[46][0]:	mock grad = 0.079523373943679587, computed grad = 0.079523373539117037
Iteration: 727, Checking gradient for words E[485][46]:	mock grad = 0.013512988286223937, computed grad = 0.013512988371783032
Iteration: 727, Checking gradient for hidden W[151][40]:	mock grad = 0.004230718362807639, computed grad = 0.004230718397282542
Iteration: 727, Checking gradient for hidden b[0][47]:	mock grad = -0.102771878638274750, computed grad = -0.102771902436372325
Iteration: 727, Checking gradient for output layer W[127][1]:	mock grad = -0.058281684004291012, computed grad = -0.058281683979855420
Iteration: 728, Checking gradient for words E[281][35]:	mock grad = 0.004230608852601181, computed grad = 0.004230608855949185
Iteration: 728, Checking gradient for hidden W[166][43]:	mock grad = 0.001529083752366267, computed grad = 0.001529083764004602
Iteration: 728, Checking gradient for hidden b[0][35]:	mock grad = 0.009929865341040678, computed grad = 0.009929864247519706
Iteration: 728, Checking gradient for output layer W[133][0]:	mock grad = 0.042137291097116281, computed grad = 0.042137291043341178
current: 30, Cost = 0.362503, Correct(%) = 1, time = 2.12613
Iteration: 729, Checking gradient for words E[49][45]:	mock grad = 0.021128367654305036, computed grad = 0.021128367714146442
Iteration: 729, Checking gradient for hidden W[114][3]:	mock grad = -0.000579069368927820, computed grad = -0.000579069380161658
Iteration: 729, Checking gradient for hidden b[0][40]:	mock grad = -0.097838757933343956, computed grad = -0.097838781026183222
Iteration: 729, Checking gradient for output layer W[91][0]:	mock grad = 0.032740053888868914, computed grad = 0.032740053871631015
Iteration: 730, Checking gradient for words E[377][35]:	mock grad = -0.007648900178597318, computed grad = -0.009158059636382419
Iteration: 730, Checking gradient for hidden W[190][17]:	mock grad = 0.003083967069605897, computed grad = 0.003083967135853040
Iteration: 730, Checking gradient for hidden b[0][14]:	mock grad = -0.049972512041418504, computed grad = -0.049972529032634661
Iteration: 730, Checking gradient for output layer W[133][1]:	mock grad = -0.057456142426220991, computed grad = -0.057456142329229770
Iteration: 731, Checking gradient for words E[118][26]:	mock grad = -0.039462632194064984, computed grad = -0.039462632231628068
Iteration: 731, Checking gradient for hidden W[177][29]:	mock grad = 0.015151509051736234, computed grad = 0.015151509116176527
Iteration: 731, Checking gradient for hidden b[0][35]:	mock grad = 0.015285206676707297, computed grad = 0.015285205595813065
Iteration: 731, Checking gradient for output layer W[17][1]:	mock grad = -0.027903995128530923, computed grad = -0.027903995125578607
Iteration: 732, Checking gradient for words E[517][9]:	mock grad = 0.012136200339296632, computed grad = 0.012136200320969851
Iteration: 732, Checking gradient for hidden W[8][24]:	mock grad = -0.013013808492806644, computed grad = -0.013013808515374772
Iteration: 732, Checking gradient for hidden b[0][7]:	mock grad = 0.070851307478220216, computed grad = 0.070851319958341555
Iteration: 732, Checking gradient for output layer W[125][1]:	mock grad = -0.025244419132791629, computed grad = -0.025244419123738603
Iteration: 733, Checking gradient for words E[354][26]:	mock grad = -0.017729279684602917, computed grad = -0.017729279690064673
Iteration: 733, Checking gradient for hidden W[82][22]:	mock grad = -0.009736371861107651, computed grad = -0.009736371991927796
Iteration: 733, Checking gradient for hidden b[0][2]:	mock grad = -0.051206526231334371, computed grad = -0.051206540891294443
Iteration: 733, Checking gradient for output layer W[69][0]:	mock grad = 0.031170979502337381, computed grad = 0.031170979481774209
Iteration: 734, Checking gradient for words E[776][14]:	mock grad = -0.041367817874615742, computed grad = -0.041367818021045451
Iteration: 734, Checking gradient for hidden W[97][26]:	mock grad = 0.001880472209864203, computed grad = 0.001880472212684453
Iteration: 734, Checking gradient for hidden b[0][41]:	mock grad = -0.045267978924679042, computed grad = -0.045267991496976086
Iteration: 734, Checking gradient for output layer W[114][1]:	mock grad = -0.100027077085035332, computed grad = -0.100027076885996741
Iteration: 735, Checking gradient for words E[383][22]:	mock grad = -0.057788563735200471, computed grad = -0.057788563778720194
Iteration: 735, Checking gradient for hidden W[247][29]:	mock grad = -0.011219037403392695, computed grad = -0.011219037433554548
Iteration: 735, Checking gradient for hidden b[0][27]:	mock grad = 0.040455502949038324, computed grad = 0.040455524740255565
Iteration: 735, Checking gradient for output layer W[135][1]:	mock grad = 0.054955591593047748, computed grad = 0.054955591553320624
Iteration: 736, Checking gradient for words E[531][31]:	mock grad = -0.041711969390706427, computed grad = -0.041711969467370935
Iteration: 736, Checking gradient for hidden W[188][8]:	mock grad = -0.004343801948886172, computed grad = -0.004343801981167690
Iteration: 736, Checking gradient for hidden b[0][6]:	mock grad = 0.046810267107955106, computed grad = 0.046810282221095879
Iteration: 736, Checking gradient for output layer W[5][0]:	mock grad = -0.053011688099180443, computed grad = -0.053011688077078872
Iteration: 737, Checking gradient for words E[69][35]:	mock grad = -0.018549481002966273, computed grad = -0.018549481026505287
Iteration: 737, Checking gradient for hidden W[91][41]:	mock grad = 0.006478843778912058, computed grad = 0.006478843800712821
Iteration: 737, Checking gradient for hidden b[0][8]:	mock grad = -0.045695657076877394, computed grad = -0.045695681049800450
Iteration: 737, Checking gradient for output layer W[25][0]:	mock grad = 0.082191627564509595, computed grad = 0.082191627347182383
Iteration: 738, Checking gradient for words E[102][7]:	mock grad = -0.016385167507432241, computed grad = -0.016385167635728292
Iteration: 738, Checking gradient for hidden W[130][28]:	mock grad = -0.002152575805658419, computed grad = -0.002152575808523971
Iteration: 738, Checking gradient for hidden b[0][3]:	mock grad = -0.047350072739704707, computed grad = -0.047350086674934268
Iteration: 738, Checking gradient for output layer W[51][0]:	mock grad = -0.158868819974705699, computed grad = -0.158868819123157784
current: 40, Cost = 0.313604, Correct(%) = 1, time = 2.561
Iteration: 739, Checking gradient for words E[65][12]:	mock grad = 0.051258849053242850, computed grad = 0.051869256895590264
Iteration: 739, Checking gradient for hidden W[115][7]:	mock grad = 0.006467546718541550, computed grad = 0.006467546667259514
Iteration: 739, Checking gradient for hidden b[0][24]:	mock grad = 0.075676265496982253, computed grad = 0.075676282353874663
Iteration: 739, Checking gradient for output layer W[119][0]:	mock grad = -0.129273270433688525, computed grad = -0.129273268757470777
Iteration: 740, Checking gradient for words E[172][11]:	mock grad = 0.017387948397618036, computed grad = 0.017387948449224828
Iteration: 740, Checking gradient for hidden W[19][48]:	mock grad = 0.012460578761863683, computed grad = 0.012460578832194733
Iteration: 740, Checking gradient for hidden b[0][12]:	mock grad = -0.063205245421638079, computed grad = -0.063205266466948337
Iteration: 740, Checking gradient for output layer W[117][1]:	mock grad = 0.119854471989894185, computed grad = 0.119854470696372617
Iteration: 741, Checking gradient for words E[152][13]:	mock grad = 0.012099128720133301, computed grad = 0.012099128729470825
Iteration: 741, Checking gradient for hidden W[205][44]:	mock grad = 0.016808872618900850, computed grad = 0.016808872723079984
Iteration: 741, Checking gradient for hidden b[0][6]:	mock grad = 0.050582674306243192, computed grad = 0.050582691777342266
Iteration: 741, Checking gradient for output layer W[138][0]:	mock grad = 0.100638571232880558, computed grad = 0.100638571118600334
Iteration: 742, Checking gradient for words E[6][0]:	mock grad = 0.014205699341712785, computed grad = 0.014205699338396086
Iteration: 742, Checking gradient for hidden W[28][4]:	mock grad = 0.012463575433607765, computed grad = 0.012463575559382937
Iteration: 742, Checking gradient for hidden b[0][13]:	mock grad = -0.145618036771516524, computed grad = -0.145618082003633686
Iteration: 742, Checking gradient for output layer W[52][1]:	mock grad = -0.003766159570511851, computed grad = -0.003766159570523314
Iteration: 743, Checking gradient for words E[562][23]:	mock grad = -0.008152422575513629, computed grad = -0.008152422630030173
Iteration: 743, Checking gradient for hidden W[181][13]:	mock grad = 0.000618934484220013, computed grad = 0.000618934504003992
Iteration: 743, Checking gradient for hidden b[0][22]:	mock grad = 0.057602157384151198, computed grad = 0.057602169184978645
Iteration: 743, Checking gradient for output layer W[0][0]:	mock grad = -0.101620587042716704, computed grad = -0.101620586930217374
Iteration: 744, Checking gradient for words E[217][33]:	mock grad = -0.005274674900246357, computed grad = -0.005274674931311215
Iteration: 744, Checking gradient for hidden W[114][12]:	mock grad = -0.003217902307478537, computed grad = -0.003217902310073097
Iteration: 744, Checking gradient for hidden b[0][16]:	mock grad = -0.014763142441948096, computed grad = -0.014763147964256327
Iteration: 744, Checking gradient for output layer W[0][0]:	mock grad = 0.050225531151859570, computed grad = 0.050225531015958010
Iteration: 745, Checking gradient for words E[102][43]:	mock grad = 0.027495078805506168, computed grad = 0.027495078761802509
Iteration: 745, Checking gradient for hidden W[135][41]:	mock grad = 0.008526184628376843, computed grad = 0.008526184753766694
Iteration: 745, Checking gradient for hidden b[0][15]:	mock grad = -0.112116821004509815, computed grad = -0.112116847680006959
Iteration: 745, Checking gradient for output layer W[118][1]:	mock grad = 0.054067931758994270, computed grad = 0.054067931723133095
Iteration: 746, Checking gradient for words E[405][49]:	mock grad = 0.007826041566183495, computed grad = 0.007826041579697917
Iteration: 746, Checking gradient for hidden W[93][41]:	mock grad = 0.004678654746625988, computed grad = 0.004678654772058214
Iteration: 746, Checking gradient for hidden b[0][29]:	mock grad = -0.064580082107607018, computed grad = -0.064580092883462920
Iteration: 746, Checking gradient for output layer W[93][0]:	mock grad = -0.031879611937762897, computed grad = -0.031879611912113984
Iteration: 747, Checking gradient for words E[102][36]:	mock grad = -0.005019019382990209, computed grad = -0.004791212301107137
Iteration: 747, Checking gradient for hidden W[112][38]:	mock grad = 0.011134331564660593, computed grad = 0.011134331606881365
Iteration: 747, Checking gradient for hidden b[0][31]:	mock grad = 0.038808921740074442, computed grad = 0.038808935762463997
Iteration: 747, Checking gradient for output layer W[41][1]:	mock grad = 0.055669823284865139, computed grad = 0.055669823191459904
Iteration: 748, Checking gradient for words E[279][30]:	mock grad = 0.003969138603204803, computed grad = 0.003969138600651879
Iteration: 748, Checking gradient for hidden W[234][4]:	mock grad = 0.002236232393398874, computed grad = 0.002236232399920712
Iteration: 748, Checking gradient for hidden b[0][13]:	mock grad = -0.097546736317705163, computed grad = -0.097546752316936408
Iteration: 748, Checking gradient for output layer W[130][1]:	mock grad = -0.151222912936038734, computed grad = -0.151222910449640452
current: 50, Cost = 0.318683, Correct(%) = 1, time = 3.44816
Iteration: 749, Checking gradient for words E[353][29]:	mock grad = 0.000285482400858017, computed grad = 0.000285482406685396
Iteration: 749, Checking gradient for hidden W[135][23]:	mock grad = 0.014165176138225544, computed grad = 0.014165176329353454
Iteration: 749, Checking gradient for hidden b[0][28]:	mock grad = 0.023597956538223830, computed grad = 0.023597977950914409
Iteration: 749, Checking gradient for output layer W[36][1]:	mock grad = -0.052788812719806177, computed grad = -0.052788812611046003
Iteration: 750, Checking gradient for words E[360][49]:	mock grad = 0.009827611548607340, computed grad = 0.009827611509593628
Iteration: 750, Checking gradient for hidden W[96][48]:	mock grad = -0.001709845402098065, computed grad = -0.001709845397943812
Iteration: 750, Checking gradient for hidden b[0][22]:	mock grad = 0.055166812375495589, computed grad = 0.055166822536534078
Iteration: 750, Checking gradient for output layer W[65][1]:	mock grad = 0.262635744395278703, computed grad = 0.262635742199419209
Iteration: 751, Checking gradient for words E[48][30]:	mock grad = -0.007652671368485464, computed grad = -0.006916078130002949
Iteration: 751, Checking gradient for hidden W[92][19]:	mock grad = -0.009098223822862073, computed grad = -0.009098223816227626
Iteration: 751, Checking gradient for hidden b[0][16]:	mock grad = -0.019526243679984789, computed grad = -0.019526250234968102
Iteration: 751, Checking gradient for output layer W[98][1]:	mock grad = 0.030130924129539993, computed grad = 0.030130924120487873
Iteration: 752, Checking gradient for words E[437][29]:	mock grad = -0.032553567901771885, computed grad = -0.032553567937381345
Iteration: 752, Checking gradient for hidden W[150][43]:	mock grad = 0.000229835494469643, computed grad = 0.000229835493743964
Iteration: 752, Checking gradient for hidden b[0][28]:	mock grad = -0.029568964498571937, computed grad = -0.029568996522645847
Iteration: 752, Checking gradient for output layer W[108][0]:	mock grad = -0.187157119397179939, computed grad = -0.187157117842249671
Iteration: 753, Checking gradient for words E[315][17]:	mock grad = 0.010547928087828451, computed grad = 0.010547928099288116
Iteration: 753, Checking gradient for hidden W[63][18]:	mock grad = -0.011319550456939664, computed grad = -0.011319550500374783
Iteration: 753, Checking gradient for hidden b[0][23]:	mock grad = 0.130078184220033810, computed grad = 0.130078200551749290
Iteration: 753, Checking gradient for output layer W[93][1]:	mock grad = 0.041524631578082616, computed grad = 0.041524631544621299
Iteration: 754, Checking gradient for words E[79][19]:	mock grad = 0.006043629851598675, computed grad = 0.006043629881091426
Iteration: 754, Checking gradient for hidden W[236][26]:	mock grad = 0.007074689976988013, computed grad = 0.007074690059465702
Iteration: 754, Checking gradient for hidden b[0][14]:	mock grad = 0.056964605696263604, computed grad = 0.056964626521689106
Iteration: 754, Checking gradient for output layer W[100][1]:	mock grad = 0.104526779093588873, computed grad = 0.104526778201642218
Iteration: 755, Checking gradient for words E[475][47]:	mock grad = -0.013859177243558118, computed grad = -0.013859177255031534
Iteration: 755, Checking gradient for hidden W[139][40]:	mock grad = 0.004211633534062509, computed grad = 0.004211633544815277
Iteration: 755, Checking gradient for hidden b[0][18]:	mock grad = -0.047589808369302711, computed grad = -0.047589794377399512
Iteration: 755, Checking gradient for output layer W[105][1]:	mock grad = 0.058082854645868931, computed grad = 0.058082854514532989
Iteration: 756, Checking gradient for words E[48][2]:	mock grad = -0.027481448713340129, computed grad = -0.027481448738892918
Iteration: 756, Checking gradient for hidden W[202][7]:	mock grad = 0.001284238172527541, computed grad = 0.001284238107205106
Iteration: 756, Checking gradient for hidden b[0][49]:	mock grad = -0.041437555859669128, computed grad = -0.041437551155995969
Iteration: 756, Checking gradient for output layer W[104][1]:	mock grad = 0.085184210391719972, computed grad = 0.085184210321370094
Iteration: 757, Checking gradient for words E[639][15]:	mock grad = 0.010814504714129480, computed grad = 0.010814504752152114
Iteration: 757, Checking gradient for hidden W[177][30]:	mock grad = -0.004463165532736513, computed grad = -0.004463165537212728
Iteration: 757, Checking gradient for hidden b[0][45]:	mock grad = -0.007099775538393915, computed grad = -0.007099775234739269
Iteration: 757, Checking gradient for output layer W[117][1]:	mock grad = -0.164751986745453882, computed grad = -0.164751984529574097
Iteration: 758, Checking gradient for words E[642][35]:	mock grad = -0.016333435356591375, computed grad = -0.016333435338967882
Iteration: 758, Checking gradient for hidden W[39][44]:	mock grad = -0.014789970553957810, computed grad = -0.014789970551813816
Iteration: 758, Checking gradient for hidden b[0][49]:	mock grad = -0.035648997982334896, computed grad = -0.035648994607333964
Iteration: 758, Checking gradient for output layer W[19][0]:	mock grad = -0.022658560821703366, computed grad = -0.022658560818224410
current: 60, Cost = 0.405104, Correct(%) = 1, time = 4.13887
Iteration: 759, Checking gradient for words E[398][39]:	mock grad = -0.009465001025388275, computed grad = -0.008183127671035909
Iteration: 759, Checking gradient for hidden W[45][18]:	mock grad = -0.000384077900233981, computed grad = -0.000384078072887587
Iteration: 759, Checking gradient for hidden b[0][47]:	mock grad = -0.079583966316826293, computed grad = -0.079583978995364707
Iteration: 759, Checking gradient for output layer W[81][0]:	mock grad = -0.141861737691290068, computed grad = -0.141861736736548394
Iteration: 760, Checking gradient for words E[776][13]:	mock grad = 0.007620710800959607, computed grad = 0.007620710771575988
Iteration: 760, Checking gradient for hidden W[58][35]:	mock grad = 0.000162630001715458, computed grad = 0.000162630000615970
Iteration: 760, Checking gradient for hidden b[0][16]:	mock grad = 0.018622244157939782, computed grad = 0.018622247796956797
Iteration: 760, Checking gradient for output layer W[89][1]:	mock grad = 0.167514084856129619, computed grad = 0.167514083711957668
Iteration: 761, Checking gradient for words E[386][42]:	mock grad = 0.020798072731448292, computed grad = 0.020798072807193677
Iteration: 761, Checking gradient for hidden W[64][18]:	mock grad = 0.003446111022847864, computed grad = 0.003446111122175229
Iteration: 761, Checking gradient for hidden b[0][28]:	mock grad = -0.014782159774018311, computed grad = -0.014782179117444802
Iteration: 761, Checking gradient for output layer W[87][0]:	mock grad = 0.070408270250177374, computed grad = 0.070408270183582256
Iteration: 762, Checking gradient for words E[58][6]:	mock grad = -0.014350953366826236, computed grad = -0.014350953321570700
Iteration: 762, Checking gradient for hidden W[160][21]:	mock grad = -0.001804299973595658, computed grad = -0.001804299981121359
Iteration: 762, Checking gradient for hidden b[0][32]:	mock grad = 0.059295109935464385, computed grad = 0.059295129506629171
Iteration: 762, Checking gradient for output layer W[77][1]:	mock grad = 0.023174052003832601, computed grad = 0.023174051999691636
Iteration: 763, Checking gradient for words E[208][22]:	mock grad = 0.004316224883171849, computed grad = 0.004316224924787808
Iteration: 763, Checking gradient for hidden W[65][23]:	mock grad = -0.012098923097836334, computed grad = -0.012098923260297699
Iteration: 763, Checking gradient for hidden b[0][15]:	mock grad = 0.105527491327916945, computed grad = 0.105527519101986683
Iteration: 763, Checking gradient for output layer W[38][1]:	mock grad = 0.003638394590721461, computed grad = 0.003638394590746508
Iteration: 764, Checking gradient for words E[90][6]:	mock grad = -0.019307124115852670, computed grad = -0.019307124136082338
Iteration: 764, Checking gradient for hidden W[13][47]:	mock grad = 0.001125252770517449, computed grad = 0.001125252917374239
Iteration: 764, Checking gradient for hidden b[0][19]:	mock grad = 0.052495742638158926, computed grad = 0.052495746405466795
Iteration: 764, Checking gradient for output layer W[27][1]:	mock grad = 0.064781635004451665, computed grad = 0.064781634938119753
Iteration: 765, Checking gradient for words E[48][28]:	mock grad = 0.002097824991637021, computed grad = 0.002097824993730959
Iteration: 765, Checking gradient for hidden W[190][22]:	mock grad = 0.005558483130330139, computed grad = 0.005558483238791640
Iteration: 765, Checking gradient for hidden b[0][13]:	mock grad = -0.093784976742644632, computed grad = -0.093784997033437997
Iteration: 765, Checking gradient for output layer W[81][1]:	mock grad = 0.084400204128309841, computed grad = 0.084400203554642950
Iteration: 766, Checking gradient for words E[90][42]:	mock grad = 0.007585020949246735, computed grad = 0.007585021025121735
Iteration: 766, Checking gradient for hidden W[108][17]:	mock grad = -0.001860908460243893, computed grad = -0.001860908462044500
Iteration: 766, Checking gradient for hidden b[0][1]:	mock grad = -0.081482894002227191, computed grad = -0.081482898996442996
Iteration: 766, Checking gradient for output layer W[126][1]:	mock grad = -0.125240031421225417, computed grad = -0.125240030464404073
Iteration: 767, Checking gradient for words E[589][31]:	mock grad = -0.008346432505967893, computed grad = -0.008346432502657670
Iteration: 767, Checking gradient for hidden W[203][13]:	mock grad = 0.009460298080909979, computed grad = 0.009460298070002857
Iteration: 767, Checking gradient for hidden b[0][14]:	mock grad = 0.062966762889193895, computed grad = 0.062966782377421593
Iteration: 767, Checking gradient for output layer W[7][1]:	mock grad = -0.039307191952830944, computed grad = -0.039307191933681707
Iteration: 768, Checking gradient for words E[430][23]:	mock grad = -0.025742258742494917, computed grad = -0.025742258762910118
Iteration: 768, Checking gradient for hidden W[47][35]:	mock grad = -0.002202528904787782, computed grad = -0.002202528896856227
Iteration: 768, Checking gradient for hidden b[0][36]:	mock grad = 0.003457906209730632, computed grad = 0.003457898518721428
Iteration: 768, Checking gradient for output layer W[4][0]:	mock grad = 0.027223531949754909, computed grad = 0.027223531937687211
current: 70, Cost = 0.307951, Correct(%) = 1, time = 4.75188
Iteration: 769, Checking gradient for words E[677][35]:	mock grad = 0.019356460863095837, computed grad = 0.019356460882898184
Iteration: 769, Checking gradient for hidden W[214][18]:	mock grad = 0.001072194987156161, computed grad = 0.001072194986065538
Iteration: 769, Checking gradient for hidden b[0][28]:	mock grad = -0.008821833431149617, computed grad = -0.008821845866870938
Iteration: 769, Checking gradient for output layer W[60][1]:	mock grad = -0.075029498050882548, computed grad = -0.075029497704863807
Iteration: 770, Checking gradient for words E[684][9]:	mock grad = -0.048932308617299913, computed grad = -0.048932308737194619
Iteration: 770, Checking gradient for hidden W[42][6]:	mock grad = -0.003183231608833159, computed grad = -0.003183231650484631
Iteration: 770, Checking gradient for hidden b[0][48]:	mock grad = -0.107429315720192209, computed grad = -0.107429343584895934
Iteration: 770, Checking gradient for output layer W[52][1]:	mock grad = -0.096120786995723684, computed grad = -0.096120786803905192
Iteration: 771, Checking gradient for words E[69][49]:	mock grad = 0.024661789851387894, computed grad = 0.024661789901709072
Iteration: 771, Checking gradient for hidden W[77][12]:	mock grad = 0.007130906048813390, computed grad = 0.007130906112113208
Iteration: 771, Checking gradient for hidden b[0][40]:	mock grad = -0.111177588415251050, computed grad = -0.111177612944362986
Iteration: 771, Checking gradient for output layer W[95][0]:	mock grad = 0.062780506218662602, computed grad = 0.062780506156590560
Iteration: 772, Checking gradient for words E[54][0]:	mock grad = 0.027147887911943558, computed grad = 0.027147888166895028
Iteration: 772, Checking gradient for hidden W[170][11]:	mock grad = -0.002768243558798344, computed grad = -0.002768243592432269
Iteration: 772, Checking gradient for hidden b[0][0]:	mock grad = 0.171273735454835485, computed grad = 0.171273771399015728
Iteration: 772, Checking gradient for output layer W[41][1]:	mock grad = 0.079811980124466775, computed grad = 0.079811979996485025
Iteration: 773, Checking gradient for words E[166][33]:	mock grad = 0.001838528644881876, computed grad = 0.001838528538824800
Iteration: 773, Checking gradient for hidden W[213][3]:	mock grad = 0.000294082256824257, computed grad = 0.000294082269859033
Iteration: 773, Checking gradient for hidden b[0][2]:	mock grad = 0.069826551031382245, computed grad = 0.069826586495374723
Iteration: 773, Checking gradient for output layer W[145][0]:	mock grad = 0.075772858306455415, computed grad = 0.075772858246063515
Iteration: 774, Checking gradient for words E[693][35]:	mock grad = 0.061668309901175000, computed grad = 0.061668310046732171
Iteration: 774, Checking gradient for hidden W[197][12]:	mock grad = 0.011972844927737869, computed grad = 0.011972845110455084
Iteration: 774, Checking gradient for hidden b[0][11]:	mock grad = 0.113394006314965212, computed grad = 0.113394040895753487
Iteration: 774, Checking gradient for output layer W[109][0]:	mock grad = -0.159437252606003943, computed grad = -0.159437251866722168
Iteration: 775, Checking gradient for words E[11][45]:	mock grad = -0.011776415578729216, computed grad = -0.011776415633445791
Iteration: 775, Checking gradient for hidden W[150][14]:	mock grad = 0.017351841786583666, computed grad = 0.017351841827148693
Iteration: 775, Checking gradient for hidden b[0][7]:	mock grad = -0.071774769665744476, computed grad = -0.071774776991784861
Iteration: 775, Checking gradient for output layer W[90][1]:	mock grad = -0.105880423743515140, computed grad = -0.105880423153184494
Iteration: 776, Checking gradient for words E[76][1]:	mock grad = 0.014160711503397039, computed grad = 0.014160711574368259
Iteration: 776, Checking gradient for hidden W[224][33]:	mock grad = 0.010174125173095794, computed grad = 0.010174125216558996
Iteration: 776, Checking gradient for hidden b[0][44]:	mock grad = 0.001473340443869375, computed grad = 0.001473333860556352
Iteration: 776, Checking gradient for output layer W[78][1]:	mock grad = 0.129362031351859530, computed grad = 0.129362030637248049
Iteration: 777, Checking gradient for words E[698][29]:	mock grad = -0.039591519395282271, computed grad = -0.039591519373747469
Iteration: 777, Checking gradient for hidden W[166][13]:	mock grad = 0.000170327914689183, computed grad = 0.000170327914802672
Iteration: 777, Checking gradient for hidden b[0][9]:	mock grad = -0.046454208869478597, computed grad = -0.046454211292845167
Iteration: 777, Checking gradient for output layer W[58][0]:	mock grad = 0.020619438077823515, computed grad = 0.020619438070680274
Iteration: 778, Checking gradient for words E[124][13]:	mock grad = -0.018324881247533398, computed grad = -0.017712056306814383
Iteration: 778, Checking gradient for hidden W[124][44]:	mock grad = -0.014327870263786124, computed grad = -0.014327870308833680
Iteration: 778, Checking gradient for hidden b[0][0]:	mock grad = 0.160617876194757425, computed grad = 0.160617918078922728
Iteration: 778, Checking gradient for output layer W[37][0]:	mock grad = -0.038423508308155219, computed grad = -0.038423508284964235
current: 80, Cost = 0.338515, Correct(%) = 1, time = 5.23788
Iteration: 779, Checking gradient for words E[104][10]:	mock grad = 0.004206685075086591, computed grad = 0.004206685107956049
Iteration: 779, Checking gradient for hidden W[89][35]:	mock grad = -0.003854238110334585, computed grad = -0.003854238127322531
Iteration: 779, Checking gradient for hidden b[0][29]:	mock grad = -0.071965126975132021, computed grad = -0.071965141486520445
Iteration: 779, Checking gradient for output layer W[93][0]:	mock grad = -0.001301513053858505, computed grad = -0.001301513053802668
Iteration: 780, Checking gradient for words E[116][35]:	mock grad = -0.057596084314770479, computed grad = -0.057596084235217296
Iteration: 780, Checking gradient for hidden W[249][12]:	mock grad = 0.001976929326713561, computed grad = 0.001976929346077582
Iteration: 780, Checking gradient for hidden b[0][44]:	mock grad = -0.007570529040601404, computed grad = -0.007570528898128836
Iteration: 780, Checking gradient for output layer W[32][0]:	mock grad = 0.087603128540447717, computed grad = 0.087603128271393355
Iteration: 781, Checking gradient for words E[491][32]:	mock grad = 0.012911992272962580, computed grad = 0.012911992301648315
Iteration: 781, Checking gradient for hidden W[218][38]:	mock grad = -0.003168386158908243, computed grad = -0.003168386167086616
Iteration: 781, Checking gradient for hidden b[0][48]:	mock grad = 0.079591777374682371, computed grad = 0.079591775485432836
Iteration: 781, Checking gradient for output layer W[47][1]:	mock grad = 0.053974721623661903, computed grad = 0.053974721553034365
Iteration: 782, Checking gradient for words E[656][46]:	mock grad = 0.000892351911641054, computed grad = 0.000892351899443439
Iteration: 782, Checking gradient for hidden W[103][18]:	mock grad = 0.006931953708044780, computed grad = 0.006931953692213251
Iteration: 782, Checking gradient for hidden b[0][13]:	mock grad = -0.123906761149011890, computed grad = -0.123906788802202572
Iteration: 782, Checking gradient for output layer W[56][1]:	mock grad = 0.177539814778721050, computed grad = 0.177539812703859490
Iteration: 783, Checking gradient for words E[58][46]:	mock grad = -0.001550702425107131, computed grad = -0.001550702442739337
Iteration: 783, Checking gradient for hidden W[232][20]:	mock grad = 0.006741081312239272, computed grad = 0.006741081328124347
Iteration: 783, Checking gradient for hidden b[0][0]:	mock grad = -0.181679522028355134, computed grad = -0.181679576498097434
Iteration: 783, Checking gradient for output layer W[85][1]:	mock grad = 0.205580051228893002, computed grad = 0.205580048984998681
Iteration: 784, Checking gradient for words E[532][44]:	mock grad = 0.007627670895793415, computed grad = 0.007627670894845412
Iteration: 784, Checking gradient for hidden W[127][3]:	mock grad = 0.003279532642203042, computed grad = 0.003279532650794422
Iteration: 784, Checking gradient for hidden b[0][33]:	mock grad = -0.090491003903458989, computed grad = -0.090491018257981781
Iteration: 784, Checking gradient for output layer W[72][0]:	mock grad = -0.100777332095902672, computed grad = -0.100777331586694621
Iteration: 785, Checking gradient for words E[719][33]:	mock grad = 0.008684571631473181, computed grad = 0.008684571530205493
Iteration: 785, Checking gradient for hidden W[144][27]:	mock grad = 0.011297338496973897, computed grad = 0.011297338574610492
Iteration: 785, Checking gradient for hidden b[0][47]:	mock grad = 0.070931041627947389, computed grad = 0.070931055990757982
Iteration: 785, Checking gradient for output layer W[127][0]:	mock grad = -0.160974284652087851, computed grad = -0.160974282520122469
Iteration: 786, Checking gradient for words E[160][16]:	mock grad = -0.031844744280029591, computed grad = -0.031844744378330514
Iteration: 786, Checking gradient for hidden W[64][26]:	mock grad = -0.011121968536059734, computed grad = -0.011121968627881640
Iteration: 786, Checking gradient for hidden b[0][36]:	mock grad = -0.010134864131094989, computed grad = -0.010134862160292503
Iteration: 786, Checking gradient for output layer W[148][1]:	mock grad = -0.144916154009189224, computed grad = -0.144916152811570748
Iteration: 787, Checking gradient for words E[517][28]:	mock grad = -0.010799094330271597, computed grad = -0.010799094372186437
Iteration: 787, Checking gradient for hidden W[90][13]:	mock grad = -0.001837689127814901, computed grad = -0.001837689151065843
Iteration: 787, Checking gradient for hidden b[0][39]:	mock grad = 0.085309316917081546, computed grad = 0.085309341418415591
Iteration: 787, Checking gradient for output layer W[93][0]:	mock grad = 0.060243887465644130, computed grad = 0.060243887378606281
Iteration: 788, Checking gradient for words E[552][28]:	mock grad = -0.005746726906258326, computed grad = -0.005746726962429957
Iteration: 788, Checking gradient for hidden W[176][17]:	mock grad = -0.000007219518483037, computed grad = -0.000007219497772334
Iteration: 788, Checking gradient for hidden b[0][3]:	mock grad = 0.028630981634941710, computed grad = 0.028630977412575885
Iteration: 788, Checking gradient for output layer W[20][0]:	mock grad = 0.033962067836540433, computed grad = 0.033962067821136671
current: 90, Cost = 0.393635, Correct(%) = 1, time = 6.18154
Iteration: 789, Checking gradient for words E[652][15]:	mock grad = 0.004922218787534183, computed grad = 0.004922218812097224
Iteration: 789, Checking gradient for hidden W[67][33]:	mock grad = -0.010249056325462025, computed grad = -0.010249056465563098
Iteration: 789, Checking gradient for hidden b[0][8]:	mock grad = -0.045237610975135123, computed grad = -0.045237636967592372
Iteration: 789, Checking gradient for output layer W[136][1]:	mock grad = 0.152763537762973245, computed grad = 0.152763536441098385
Iteration: 790, Checking gradient for words E[705][21]:	mock grad = 0.005093825469537627, computed grad = 0.005093825475625420
Iteration: 790, Checking gradient for hidden W[61][19]:	mock grad = -0.017496607630368421, computed grad = -0.017496607934801314
Iteration: 790, Checking gradient for hidden b[0][9]:	mock grad = -0.036804111290866803, computed grad = -0.036804103577851813
Iteration: 790, Checking gradient for output layer W[46][1]:	mock grad = 0.062414212318090412, computed grad = 0.062414212181864027
Iteration: 791, Checking gradient for words E[69][30]:	mock grad = 0.013937831782190324, computed grad = 0.013937831821772044
Iteration: 791, Checking gradient for hidden W[215][16]:	mock grad = 0.005455530356884841, computed grad = 0.005455530430017294
Iteration: 791, Checking gradient for hidden b[0][2]:	mock grad = 0.059243490810267918, computed grad = 0.059243521114330434
Iteration: 791, Checking gradient for output layer W[93][0]:	mock grad = -0.022720557604882785, computed grad = -0.022720557600828580
Iteration: 792, Checking gradient for words E[752][37]:	mock grad = 0.014233368214960906, computed grad = 0.014231166170148196
Iteration: 792, Checking gradient for hidden W[82][49]:	mock grad = 0.000805051233593312, computed grad = 0.000805051235160131
Iteration: 792, Checking gradient for hidden b[0][11]:	mock grad = -0.081974917818528059, computed grad = -0.081974943281321280
Iteration: 792, Checking gradient for output layer W[18][1]:	mock grad = 0.084630213895264816, computed grad = 0.084630213451714412
Iteration: 793, Checking gradient for words E[645][32]:	mock grad = -0.010068004844293599, computed grad = -0.010068004866762860
Iteration: 793, Checking gradient for hidden W[85][32]:	mock grad = 0.006999850552080744, computed grad = 0.006999850580304940
Iteration: 793, Checking gradient for hidden b[0][42]:	mock grad = 0.063500163897417528, computed grad = 0.063500181392833061
Iteration: 793, Checking gradient for output layer W[114][0]:	mock grad = -0.126805684705244559, computed grad = -0.126805683888185694
Iteration: 794, Checking gradient for words E[148][15]:	mock grad = 0.007091457712521398, computed grad = 0.007091457715541528
Iteration: 794, Checking gradient for hidden W[171][4]:	mock grad = -0.003779611934873195, computed grad = -0.003779611971051921
Iteration: 794, Checking gradient for hidden b[0][48]:	mock grad = -0.073480671734571512, computed grad = -0.073480686781497384
Iteration: 794, Checking gradient for output layer W[34][0]:	mock grad = 0.006650289382487129, computed grad = 0.006650289382287069
Iteration: 795, Checking gradient for words E[148][0]:	mock grad = -0.008366326325892626, computed grad = -0.014273993073560949
Iteration: 795, Checking gradient for hidden W[151][3]:	mock grad = -0.003368891770827442, computed grad = -0.003368891783609587
Iteration: 795, Checking gradient for hidden b[0][43]:	mock grad = 0.012254006775153092, computed grad = 0.012254009719818930
Iteration: 795, Checking gradient for output layer W[88][1]:	mock grad = -0.099711110187389318, computed grad = -0.099711109906915785
Iteration: 796, Checking gradient for words E[675][17]:	mock grad = 0.003015801543188035, computed grad = 0.003015801605458750
Iteration: 796, Checking gradient for hidden W[141][6]:	mock grad = 0.001224681156763996, computed grad = 0.001224681149350961
Iteration: 796, Checking gradient for hidden b[0][11]:	mock grad = -0.093035010838682375, computed grad = -0.093035038229565090
Iteration: 796, Checking gradient for output layer W[120][1]:	mock grad = -0.124888508217013694, computed grad = -0.124888507342553662
Iteration: 797, Checking gradient for words E[146][35]:	mock grad = -0.026151751722158778, computed grad = -0.026151751858491692
Iteration: 797, Checking gradient for hidden W[155][30]:	mock grad = 0.017008083613545555, computed grad = 0.017008083740258900
Iteration: 797, Checking gradient for hidden b[0][34]:	mock grad = 0.037428985810494808, computed grad = 0.037428993843554415
Iteration: 797, Checking gradient for output layer W[94][0]:	mock grad = 0.184554007458814784, computed grad = 0.184554006876652660
Iteration: 798, Checking gradient for words E[42][26]:	mock grad = -0.015311808512497027, computed grad = -0.015311808569398646
Iteration: 798, Checking gradient for hidden W[199][16]:	mock grad = -0.003624152876080533, computed grad = -0.003624152890337761
Iteration: 798, Checking gradient for hidden b[0][13]:	mock grad = 0.114445068147311924, computed grad = 0.114445094709792916
Iteration: 798, Checking gradient for output layer W[83][1]:	mock grad = -0.121505956521772429, computed grad = -0.121505955520477466
current: 100, Cost = 0.396748, Correct(%) = 1, time = 6.96584
Iteration: 799, Checking gradient for words E[49][8]:	mock grad = -0.011406321572121625, computed grad = -0.011406321629037704
Iteration: 799, Checking gradient for hidden W[198][32]:	mock grad = 0.002552807834427240, computed grad = 0.002552807835636121
Iteration: 799, Checking gradient for hidden b[0][34]:	mock grad = -0.030640073052756733, computed grad = -0.030640077930697979
Iteration: 799, Checking gradient for output layer W[70][1]:	mock grad = 0.151081024621912974, computed grad = 0.151081023378585450
current: 8, Correct(%) = 1, time = 7.0171
Dev start.
Dev finished. Total time taken is: 0.639924
dev:
Accuracy:	P=100/100=1
Test start.
Test finished. Total time taken is: 0.545353
test:
Accuracy:	P=60/100=0.6
##### Iteration 8
random: 0, 99
Iteration: 800, Checking gradient for words E[54][19]:	mock grad = -0.001525505748750655, computed grad = -0.001525505757835082
Iteration: 800, Checking gradient for hidden W[234][3]:	mock grad = -0.000502237014338158, computed grad = -0.000502236985639621
Iteration: 800, Checking gradient for hidden b[0][18]:	mock grad = 0.053889347660845965, computed grad = 0.053889357303199742
Iteration: 800, Checking gradient for output layer W[78][0]:	mock grad = -0.148412621501281539, computed grad = -0.148412617923548651
Iteration: 801, Checking gradient for words E[78][13]:	mock grad = -0.000546942472395839, computed grad = -0.000546942423731532
Iteration: 801, Checking gradient for hidden W[30][49]:	mock grad = 0.001352769911755480, computed grad = 0.001352769912171266
Iteration: 801, Checking gradient for hidden b[0][13]:	mock grad = -0.102676331277140598, computed grad = -0.102676345410949485
Iteration: 801, Checking gradient for output layer W[110][0]:	mock grad = 0.124346154348448090, computed grad = 0.124346153284059929
Iteration: 802, Checking gradient for words E[124][27]:	mock grad = -0.030194615535050762, computed grad = -0.030194615613725884
Iteration: 802, Checking gradient for hidden W[249][26]:	mock grad = -0.009512264380867608, computed grad = -0.009512264427600291
Iteration: 802, Checking gradient for hidden b[0][32]:	mock grad = -0.049630313920995262, computed grad = -0.049630329295973039
Iteration: 802, Checking gradient for output layer W[113][1]:	mock grad = 0.112503613275138914, computed grad = 0.112503612285855994
Iteration: 803, Checking gradient for words E[131][14]:	mock grad = -0.001480117512803192, computed grad = -0.001480117564227124
Iteration: 803, Checking gradient for hidden W[41][12]:	mock grad = -0.005707002920990467, computed grad = -0.005707002952363534
Iteration: 803, Checking gradient for hidden b[0][28]:	mock grad = -0.004745264062522292, computed grad = -0.004745272267736888
Iteration: 803, Checking gradient for output layer W[144][1]:	mock grad = 0.114495273775189199, computed grad = 0.114495273369502035
Iteration: 804, Checking gradient for words E[174][41]:	mock grad = -0.016691411242025778, computed grad = -0.016691411289350255
Iteration: 804, Checking gradient for hidden W[246][43]:	mock grad = -0.000042387657883669, computed grad = -0.000042387657346647
Iteration: 804, Checking gradient for hidden b[0][0]:	mock grad = 0.156967738595831241, computed grad = 0.156967770104972176
Iteration: 804, Checking gradient for output layer W[105][0]:	mock grad = -0.097474066418440364, computed grad = -0.097474066054596395
Iteration: 805, Checking gradient for words E[197][10]:	mock grad = -0.016426451587170376, computed grad = -0.016842365047874355
Iteration: 805, Checking gradient for hidden W[28][12]:	mock grad = -0.000855712254943519, computed grad = -0.000855712251344909
Iteration: 805, Checking gradient for hidden b[0][22]:	mock grad = -0.034961985366815940, computed grad = -0.034961986306105579
Iteration: 805, Checking gradient for output layer W[116][1]:	mock grad = 0.144221371818964439, computed grad = 0.144221370363863399
Iteration: 806, Checking gradient for words E[124][39]:	mock grad = -0.005079947238528781, computed grad = -0.003693107262983518
Iteration: 806, Checking gradient for hidden W[238][32]:	mock grad = 0.008115882350612225, computed grad = 0.008115882411756077
Iteration: 806, Checking gradient for hidden b[0][20]:	mock grad = -0.083918498605034531, computed grad = -0.083918523739137454
Iteration: 806, Checking gradient for output layer W[59][0]:	mock grad = 0.108526650664547697, computed grad = 0.108526649350603424
Iteration: 807, Checking gradient for words E[69][17]:	mock grad = 0.000993387611392116, computed grad = 0.000993387629173736
Iteration: 807, Checking gradient for hidden W[47][37]:	mock grad = 0.009760597381447411, computed grad = 0.009760597533914209
Iteration: 807, Checking gradient for hidden b[0][25]:	mock grad = 0.007338082061658646, computed grad = 0.007338078000470857
Iteration: 807, Checking gradient for output layer W[5][0]:	mock grad = -0.084215161864797938, computed grad = -0.084215161797989824
Iteration: 808, Checking gradient for words E[225][21]:	mock grad = -0.008591258949836034, computed grad = -0.010446125561563362
Iteration: 808, Checking gradient for hidden W[233][12]:	mock grad = -0.000393435920148510, computed grad = -0.000240707933630855
Iteration: 808, Checking gradient for hidden b[0][0]:	mock grad = 0.151756092767518780, computed grad = 0.151756136847556095
Iteration: 808, Checking gradient for output layer W[50][0]:	mock grad = 0.140399827671028321, computed grad = 0.140399826092693636
current: 10, Cost = 0.380538, Correct(%) = 1, time = 0.769234
Iteration: 809, Checking gradient for words E[6][10]:	mock grad = 0.020360206939212810, computed grad = 0.021141999156186784
Iteration: 809, Checking gradient for hidden W[130][35]:	mock grad = 0.007119314455572567, computed grad = 0.007119314530537396
Iteration: 809, Checking gradient for hidden b[0][43]:	mock grad = -0.011617336716035487, computed grad = -0.011617340109054786
Iteration: 809, Checking gradient for output layer W[3][0]:	mock grad = -0.013739334991352825, computed grad = -0.013739334990228084
Iteration: 810, Checking gradient for words E[54][14]:	mock grad = -0.011845957572559351, computed grad = -0.011740477480817121
Iteration: 810, Checking gradient for hidden W[236][29]:	mock grad = 0.016220914743819703, computed grad = 0.016220915092564663
Iteration: 810, Checking gradient for hidden b[0][37]:	mock grad = -0.043950672800330270, computed grad = -0.043950686783006551
Iteration: 810, Checking gradient for output layer W[2][1]:	mock grad = 0.073933142730686630, computed grad = 0.073933142385223513
Iteration: 811, Checking gradient for words E[329][5]:	mock grad = -0.016768283443213683, computed grad = -0.016768283465026013
Iteration: 811, Checking gradient for hidden W[120][21]:	mock grad = -0.005924766785431190, computed grad = -0.005924766825797168
Iteration: 811, Checking gradient for hidden b[0][3]:	mock grad = -0.043973991650458899, computed grad = -0.043974002685519827
Iteration: 811, Checking gradient for output layer W[37][0]:	mock grad = 0.043781567790363018, computed grad = 0.043781567770810555
Iteration: 812, Checking gradient for words E[776][43]:	mock grad = 0.001259071327225358, computed grad = 0.001259071323121119
Iteration: 812, Checking gradient for hidden W[156][44]:	mock grad = 0.011128116924552511, computed grad = 0.011128116933089750
Iteration: 812, Checking gradient for hidden b[0][18]:	mock grad = -0.043491747608315201, computed grad = -0.043491733864829983
Iteration: 812, Checking gradient for output layer W[128][0]:	mock grad = -0.039540382561320531, computed grad = -0.039540382519202472
Iteration: 813, Checking gradient for words E[373][24]:	mock grad = -0.038177143129536351, computed grad = -0.038177143293735193
Iteration: 813, Checking gradient for hidden W[64][24]:	mock grad = -0.007992553421265480, computed grad = -0.007992553342434476
Iteration: 813, Checking gradient for hidden b[0][13]:	mock grad = -0.098264450794416369, computed grad = -0.098264463181314321
Iteration: 813, Checking gradient for output layer W[121][0]:	mock grad = 0.042335518541686268, computed grad = 0.042335518494536754
Iteration: 814, Checking gradient for words E[376][4]:	mock grad = 0.020873220725248220, computed grad = 0.020873220901458704
Iteration: 814, Checking gradient for hidden W[142][6]:	mock grad = -0.001735409990932801, computed grad = -0.001735410014307744
Iteration: 814, Checking gradient for hidden b[0][46]:	mock grad = -0.202783796648120518, computed grad = -0.202783841281180238
Iteration: 814, Checking gradient for output layer W[117][1]:	mock grad = 0.151952608680672174, computed grad = 0.151952608450710819
Iteration: 815, Checking gradient for words E[382][15]:	mock grad = 0.044966241119492878, computed grad = 0.044966241216781999
Iteration: 815, Checking gradient for hidden W[183][18]:	mock grad = 0.001191347409473797, computed grad = 0.001191347425824014
Iteration: 815, Checking gradient for hidden b[0][0]:	mock grad = -0.160299428928784016, computed grad = -0.160299463879846199
Iteration: 815, Checking gradient for output layer W[74][1]:	mock grad = 0.191581837068827143, computed grad = 0.191581834096576142
Iteration: 816, Checking gradient for words E[315][8]:	mock grad = 0.036076773031662457, computed grad = 0.036076773196970856
Iteration: 816, Checking gradient for hidden W[33][32]:	mock grad = 0.015440115482989736, computed grad = 0.015440115737358172
Iteration: 816, Checking gradient for hidden b[0][11]:	mock grad = 0.095875834405734217, computed grad = 0.095875859911988645
Iteration: 816, Checking gradient for output layer W[35][1]:	mock grad = -0.090160200062527895, computed grad = -0.090160199805726690
Iteration: 817, Checking gradient for words E[413][44]:	mock grad = -0.005545578906807069, computed grad = -0.005737447274459627
Iteration: 817, Checking gradient for hidden W[35][9]:	mock grad = -0.000665788590453742, computed grad = -0.000665788609407510
Iteration: 817, Checking gradient for hidden b[0][40]:	mock grad = -0.078462096635478762, computed grad = -0.078462114002192787
Iteration: 817, Checking gradient for output layer W[137][1]:	mock grad = 0.109221341569376573, computed grad = 0.109221340188829397
Iteration: 818, Checking gradient for words E[118][44]:	mock grad = 0.012172826917233870, computed grad = 0.013106591202471861
Iteration: 818, Checking gradient for hidden W[226][20]:	mock grad = -0.000452952318041877, computed grad = -0.000452952326041045
Iteration: 818, Checking gradient for hidden b[0][20]:	mock grad = -0.089819079719333228, computed grad = -0.089819105824826356
Iteration: 818, Checking gradient for output layer W[122][1]:	mock grad = 0.099256992716595382, computed grad = 0.099256991943573708
current: 20, Cost = 0.467686, Correct(%) = 1, time = 1.42857
Iteration: 819, Checking gradient for words E[54][6]:	mock grad = -0.008719536822715535, computed grad = -0.010989513713749642
Iteration: 819, Checking gradient for hidden W[220][48]:	mock grad = -0.000110364936939389, computed grad = -0.000110364928260664
Iteration: 819, Checking gradient for hidden b[0][38]:	mock grad = 0.074582505597942106, computed grad = 0.074582519958518306
Iteration: 819, Checking gradient for output layer W[118][0]:	mock grad = -0.025117054440193520, computed grad = -0.025117054437194357
Iteration: 820, Checking gradient for words E[326][36]:	mock grad = -0.031169021868115987, computed grad = -0.031169021842794031
Iteration: 820, Checking gradient for hidden W[45][37]:	mock grad = -0.006450272330704721, computed grad = -0.006450272387040631
Iteration: 820, Checking gradient for hidden b[0][20]:	mock grad = -0.101181711662623464, computed grad = -0.101181743032575086
Iteration: 820, Checking gradient for output layer W[57][1]:	mock grad = -0.144583265757058932, computed grad = -0.144583264189560134
Iteration: 821, Checking gradient for words E[776][28]:	mock grad = -0.047126735254920948, computed grad = -0.047126735082276307
Iteration: 821, Checking gradient for hidden W[97][22]:	mock grad = -0.001650618832144124, computed grad = -0.001650618832731661
Iteration: 821, Checking gradient for hidden b[0][7]:	mock grad = -0.105689555062044027, computed grad = -0.105689576772549254
Iteration: 821, Checking gradient for output layer W[117][1]:	mock grad = 0.054412127405117916, computed grad = 0.054412127381207771
Iteration: 822, Checking gradient for words E[447][6]:	mock grad = -0.002159829788073164, computed grad = -0.002159829799131300
Iteration: 822, Checking gradient for hidden W[162][14]:	mock grad = 0.010501111717825529, computed grad = 0.010501111714249497
Iteration: 822, Checking gradient for hidden b[0][11]:	mock grad = 0.078289985317531174, computed grad = 0.078290002203414658
Iteration: 822, Checking gradient for output layer W[78][0]:	mock grad = 0.127292903745135355, computed grad = 0.127292902304559141
Iteration: 823, Checking gradient for words E[42][41]:	mock grad = -0.001646534059346338, computed grad = -0.001646534065085080
Iteration: 823, Checking gradient for hidden W[169][23]:	mock grad = 0.000153433593635954, computed grad = 0.000153433593914632
Iteration: 823, Checking gradient for hidden b[0][39]:	mock grad = -0.080826360886659954, computed grad = -0.080826392091859198
Iteration: 823, Checking gradient for output layer W[36][1]:	mock grad = 0.019193135379624859, computed grad = 0.019193135376038152
Iteration: 824, Checking gradient for words E[285][38]:	mock grad = -0.017505069997925116, computed grad = -0.017505070040160428
Iteration: 824, Checking gradient for hidden W[85][33]:	mock grad = -0.008952677455403846, computed grad = -0.008952677557271338
Iteration: 824, Checking gradient for hidden b[0][18]:	mock grad = 0.071254030269196633, computed grad = 0.071254039431925625
Iteration: 824, Checking gradient for output layer W[118][0]:	mock grad = 0.002984403687678050, computed grad = 0.002984403687744872
Iteration: 825, Checking gradient for words E[51][14]:	mock grad = -0.044028235280463290, computed grad = -0.038385464453004989
Iteration: 825, Checking gradient for hidden W[61][3]:	mock grad = 0.018292209851028529, computed grad = 0.018292210223397207
Iteration: 825, Checking gradient for hidden b[0][46]:	mock grad = 0.141205992865323093, computed grad = 0.141206026884280716
Iteration: 825, Checking gradient for output layer W[89][0]:	mock grad = -0.138893650829141624, computed grad = -0.138893649706169686
Iteration: 826, Checking gradient for words E[470][48]:	mock grad = -0.004511360474079851, computed grad = -0.004511360480256410
Iteration: 826, Checking gradient for hidden W[141][22]:	mock grad = -0.001943904370582672, computed grad = -0.001943904372942751
Iteration: 826, Checking gradient for hidden b[0][25]:	mock grad = -0.000251456949984519, computed grad = -0.000251452067809814
Iteration: 826, Checking gradient for output layer W[102][0]:	mock grad = -0.178570941229139191, computed grad = -0.178570935891811527
Iteration: 827, Checking gradient for words E[68][34]:	mock grad = -0.053962902422932668, computed grad = -0.053962902629107745
Iteration: 827, Checking gradient for hidden W[183][41]:	mock grad = -0.009668343961072301, computed grad = -0.009668344075440934
Iteration: 827, Checking gradient for hidden b[0][36]:	mock grad = -0.018025951034505994, computed grad = -0.018025955084560678
Iteration: 827, Checking gradient for output layer W[48][1]:	mock grad = -0.149233845681384647, computed grad = -0.149233845204410992
Iteration: 828, Checking gradient for words E[281][15]:	mock grad = 0.001773028331070181, computed grad = 0.001773028333437824
Iteration: 828, Checking gradient for hidden W[62][17]:	mock grad = 0.000689708546719459, computed grad = 0.000689708541315109
Iteration: 828, Checking gradient for hidden b[0][23]:	mock grad = 0.117479529627345647, computed grad = 0.117479553251450500
Iteration: 828, Checking gradient for output layer W[64][0]:	mock grad = -0.099303107189474549, computed grad = -0.099303106386317425
current: 30, Cost = 0.351807, Correct(%) = 1, time = 2.12722
Iteration: 829, Checking gradient for words E[143][14]:	mock grad = 0.009162634872050823, computed grad = 0.009162634950187581
Iteration: 829, Checking gradient for hidden W[213][23]:	mock grad = 0.005666406553417325, computed grad = 0.005666406553322988
Iteration: 829, Checking gradient for hidden b[0][16]:	mock grad = -0.016617676930591463, computed grad = -0.016617681805140085
Iteration: 829, Checking gradient for output layer W[82][1]:	mock grad = -0.227227802460483463, computed grad = -0.227227796098959461
Iteration: 830, Checking gradient for words E[512][39]:	mock grad = 0.001396308854484385, computed grad = 0.001396308723005408
Iteration: 830, Checking gradient for hidden W[52][11]:	mock grad = 0.001075436818348541, computed grad = 0.001075436796601021
Iteration: 830, Checking gradient for hidden b[0][5]:	mock grad = -0.018412784974886076, computed grad = -0.018412787793561473
Iteration: 830, Checking gradient for output layer W[59][0]:	mock grad = -0.090654502662990133, computed grad = -0.090654502232725045
Iteration: 831, Checking gradient for words E[102][27]:	mock grad = 0.021550298031275394, computed grad = 0.021550298168965691
Iteration: 831, Checking gradient for hidden W[118][22]:	mock grad = 0.007147505082488737, computed grad = 0.007147505094395425
Iteration: 831, Checking gradient for hidden b[0][14]:	mock grad = -0.069898761447834135, computed grad = -0.069898791907849783
Iteration: 831, Checking gradient for output layer W[38][1]:	mock grad = -0.024336937860691688, computed grad = -0.024336937858345509
Iteration: 832, Checking gradient for words E[34][48]:	mock grad = -0.008861302241175428, computed grad = -0.008861302297680377
Iteration: 832, Checking gradient for hidden W[244][45]:	mock grad = -0.000355624023701262, computed grad = -0.000355624023678028
Iteration: 832, Checking gradient for hidden b[0][45]:	mock grad = -0.009359506892320235, computed grad = -0.009359509440492336
Iteration: 832, Checking gradient for output layer W[45][0]:	mock grad = 0.027261734955125183, computed grad = 0.027261734942860185
Iteration: 833, Checking gradient for words E[318][7]:	mock grad = 0.021716714618563726, computed grad = 0.021716714648154931
Iteration: 833, Checking gradient for hidden W[144][36]:	mock grad = 0.003249460733323328, computed grad = 0.003249460735998724
Iteration: 833, Checking gradient for hidden b[0][20]:	mock grad = -0.088331423923615482, computed grad = -0.088331449957378100
Iteration: 833, Checking gradient for output layer W[79][0]:	mock grad = 0.028778456392236640, computed grad = 0.028778456373958684
Iteration: 834, Checking gradient for words E[776][24]:	mock grad = -0.059716646836294185, computed grad = -0.059716646917210084
Iteration: 834, Checking gradient for hidden W[138][13]:	mock grad = 0.005910095725980558, computed grad = 0.005910095844392359
Iteration: 834, Checking gradient for hidden b[0][17]:	mock grad = 0.020505677301163594, computed grad = 0.020505680556545215
Iteration: 834, Checking gradient for output layer W[109][1]:	mock grad = -0.002384997035004277, computed grad = -0.002384997035045989
Iteration: 835, Checking gradient for words E[383][41]:	mock grad = 0.026368239611246347, computed grad = 0.026368239583630729
Iteration: 835, Checking gradient for hidden W[176][35]:	mock grad = 0.005743012409631376, computed grad = 0.005743012443951778
Iteration: 835, Checking gradient for hidden b[0][17]:	mock grad = 0.020528581312129202, computed grad = 0.020528585244191627
Iteration: 835, Checking gradient for output layer W[3][0]:	mock grad = -0.000032059231463988, computed grad = -0.000032059231515291
Iteration: 836, Checking gradient for words E[531][14]:	mock grad = -0.025038117187181630, computed grad = -0.025038117281967750
Iteration: 836, Checking gradient for hidden W[197][42]:	mock grad = -0.002656706769993411, computed grad = -0.002656706795422919
Iteration: 836, Checking gradient for hidden b[0][2]:	mock grad = 0.067194268713738348, computed grad = 0.067194300035364293
Iteration: 836, Checking gradient for output layer W[133][1]:	mock grad = 0.018661230964361897, computed grad = 0.018661230963219994
Iteration: 837, Checking gradient for words E[69][11]:	mock grad = 0.010769420379669326, computed grad = 0.010769420368827444
Iteration: 837, Checking gradient for hidden W[166][47]:	mock grad = 0.002045242841280936, computed grad = 0.004071978868302099
Iteration: 837, Checking gradient for hidden b[0][34]:	mock grad = 0.030859028441049263, computed grad = 0.030859038224378538
Iteration: 837, Checking gradient for output layer W[137][1]:	mock grad = 0.125490503273817389, computed grad = 0.125490502392405140
Iteration: 838, Checking gradient for words E[278][49]:	mock grad = -0.040309441827673931, computed grad = -0.040309441825329639
Iteration: 838, Checking gradient for hidden W[175][26]:	mock grad = -0.012809100266991669, computed grad = -0.012809100433450871
Iteration: 838, Checking gradient for hidden b[0][1]:	mock grad = -0.113952320460353196, computed grad = -0.113952338289276492
Iteration: 838, Checking gradient for output layer W[97][1]:	mock grad = 0.179323303123063438, computed grad = 0.179323301705651278
current: 40, Cost = 0.300572, Correct(%) = 1, time = 2.56583
Iteration: 839, Checking gradient for words E[65][44]:	mock grad = -0.007450809406311931, computed grad = -0.007450809414180547
Iteration: 839, Checking gradient for hidden W[212][39]:	mock grad = 0.005305270535299611, computed grad = 0.005305270557549078
Iteration: 839, Checking gradient for hidden b[0][28]:	mock grad = -0.005188885127932474, computed grad = -0.005188893194309799
Iteration: 839, Checking gradient for output layer W[53][0]:	mock grad = 0.080179311750833149, computed grad = 0.080179311297097947
Iteration: 840, Checking gradient for words E[377][9]:	mock grad = -0.006197012296899063, computed grad = -0.006197012319410612
Iteration: 840, Checking gradient for hidden W[246][47]:	mock grad = -0.016360229981859931, computed grad = -0.016360230181550139
Iteration: 840, Checking gradient for hidden b[0][37]:	mock grad = -0.044302318447042577, computed grad = -0.044302333762099771
Iteration: 840, Checking gradient for output layer W[98][1]:	mock grad = -0.025460977814667496, computed grad = -0.025460977800499135
Iteration: 841, Checking gradient for words E[152][33]:	mock grad = -0.045497616092626814, computed grad = -0.045497616232814189
Iteration: 841, Checking gradient for hidden W[150][9]:	mock grad = -0.000074088395640715, computed grad = -0.000074088397620133
Iteration: 841, Checking gradient for hidden b[0][20]:	mock grad = 0.130860214411221421, computed grad = 0.130860249177455368
Iteration: 841, Checking gradient for output layer W[4][1]:	mock grad = 0.025973002246382215, computed grad = 0.025973002243977555
Iteration: 842, Checking gradient for words E[34][41]:	mock grad = 0.012915369512506159, computed grad = 0.012915369480744062
Iteration: 842, Checking gradient for hidden W[156][15]:	mock grad = 0.004128332691449854, computed grad = 0.004128332692332132
Iteration: 842, Checking gradient for hidden b[0][35]:	mock grad = 0.016980909307129366, computed grad = 0.016980911019243261
Iteration: 842, Checking gradient for output layer W[85][1]:	mock grad = 0.165574695522724413, computed grad = 0.165574694577423132
Iteration: 843, Checking gradient for words E[148][4]:	mock grad = 0.034394407221582934, computed grad = 0.037214276334264204
Iteration: 843, Checking gradient for hidden W[212][21]:	mock grad = -0.006174602879216451, computed grad = -0.006174602912716316
Iteration: 843, Checking gradient for hidden b[0][20]:	mock grad = 0.128314195352807747, computed grad = 0.128314226270329207
Iteration: 843, Checking gradient for output layer W[51][0]:	mock grad = -0.177799234759135061, computed grad = -0.177799234036123605
Iteration: 844, Checking gradient for words E[237][22]:	mock grad = 0.008634256935285967, computed grad = 0.008634256948489360
Iteration: 844, Checking gradient for hidden W[114][34]:	mock grad = -0.000373983353396667, computed grad = 0.000123263624616151
Iteration: 844, Checking gradient for hidden b[0][8]:	mock grad = -0.032161946300540700, computed grad = -0.032161959215286022
Iteration: 844, Checking gradient for output layer W[54][1]:	mock grad = -0.086513289214912303, computed grad = -0.086513288419610562
Iteration: 845, Checking gradient for words E[194][7]:	mock grad = -0.011690783921269876, computed grad = -0.011690783954842092
Iteration: 845, Checking gradient for hidden W[223][42]:	mock grad = -0.004905495086915401, computed grad = -0.004905495094727077
Iteration: 845, Checking gradient for hidden b[0][42]:	mock grad = -0.075986470503608405, computed grad = -0.075986493063227439
Iteration: 845, Checking gradient for output layer W[34][0]:	mock grad = 0.018312043423535096, computed grad = 0.018312043421984298
Iteration: 846, Checking gradient for words E[27][4]:	mock grad = -0.012177446496841604, computed grad = -0.012177446539411264
Iteration: 846, Checking gradient for hidden W[80][3]:	mock grad = -0.001812640203907590, computed grad = -0.001812640205948193
Iteration: 846, Checking gradient for hidden b[0][20]:	mock grad = 0.079179899622139338, computed grad = 0.079179914157206516
Iteration: 846, Checking gradient for output layer W[127][0]:	mock grad = 0.108977752652894555, computed grad = 0.108977751484003932
Iteration: 847, Checking gradient for words E[166][32]:	mock grad = -0.011881596447022025, computed grad = -0.009827999544741894
Iteration: 847, Checking gradient for hidden W[81][48]:	mock grad = 0.003839846145770043, computed grad = 0.003839846166655024
Iteration: 847, Checking gradient for hidden b[0][28]:	mock grad = -0.011913517048156574, computed grad = -0.011913532765746769
Iteration: 847, Checking gradient for output layer W[38][0]:	mock grad = -0.006827492043365302, computed grad = -0.006827492043180759
Iteration: 848, Checking gradient for words E[405][34]:	mock grad = -0.010869617292574851, computed grad = -0.010869617319196450
Iteration: 848, Checking gradient for hidden W[36][42]:	mock grad = -0.006413009167705042, computed grad = -0.006413009181589595
Iteration: 848, Checking gradient for hidden b[0][0]:	mock grad = -0.136844926838963765, computed grad = -0.136844967438601528
Iteration: 848, Checking gradient for output layer W[32][1]:	mock grad = 0.074670808442006686, computed grad = 0.074670808107403397
current: 50, Cost = 0.303532, Correct(%) = 1, time = 3.45526
Iteration: 849, Checking gradient for words E[612][7]:	mock grad = -0.006686841032971236, computed grad = -0.006686841033908601
Iteration: 849, Checking gradient for hidden W[33][27]:	mock grad = 0.008112172286217634, computed grad = 0.008112172314395309
Iteration: 849, Checking gradient for hidden b[0][29]:	mock grad = -0.064368315867702197, computed grad = -0.064368327893416927
Iteration: 849, Checking gradient for output layer W[33][1]:	mock grad = 0.021202031820516387, computed grad = 0.021202031812368915
Iteration: 850, Checking gradient for words E[42][42]:	mock grad = -0.012090484332011320, computed grad = -0.012090484306444658
Iteration: 850, Checking gradient for hidden W[39][42]:	mock grad = 0.005802093092810434, computed grad = 0.005802093109139498
Iteration: 850, Checking gradient for hidden b[0][46]:	mock grad = 0.160951436496165057, computed grad = 0.160951471728557977
Iteration: 850, Checking gradient for output layer W[55][1]:	mock grad = 0.175938197813862995, computed grad = 0.175938197030303695
Iteration: 851, Checking gradient for words E[374][28]:	mock grad = 0.026705299393525728, computed grad = 0.026705299425159642
Iteration: 851, Checking gradient for hidden W[75][44]:	mock grad = -0.012063944753559763, computed grad = -0.012063944766036328
Iteration: 851, Checking gradient for hidden b[0][10]:	mock grad = -0.090613082621066043, computed grad = -0.090613111387881973
Iteration: 851, Checking gradient for output layer W[27][0]:	mock grad = -0.049249366216169710, computed grad = -0.049249366170276171
Iteration: 852, Checking gradient for words E[133][0]:	mock grad = -0.007829454046703876, computed grad = -0.007829454094408120
Iteration: 852, Checking gradient for hidden W[74][48]:	mock grad = -0.005143926760958761, computed grad = -0.005143926801471873
Iteration: 852, Checking gradient for hidden b[0][25]:	mock grad = -0.004092239222597271, computed grad = -0.004092233578929847
Iteration: 852, Checking gradient for output layer W[148][0]:	mock grad = -0.164725071797683364, computed grad = -0.164725070528620482
Iteration: 853, Checking gradient for words E[242][46]:	mock grad = 0.007062201655111400, computed grad = 0.007062201714899697
Iteration: 853, Checking gradient for hidden W[211][14]:	mock grad = 0.002980748728897975, computed grad = 0.002980748719954134
Iteration: 853, Checking gradient for hidden b[0][1]:	mock grad = -0.102214639812608921, computed grad = -0.102214661980980892
Iteration: 853, Checking gradient for output layer W[63][0]:	mock grad = -0.112479795907033386, computed grad = -0.112479795140397326
Iteration: 854, Checking gradient for words E[254][32]:	mock grad = 0.005012453915792525, computed grad = 0.005012453935537941
Iteration: 854, Checking gradient for hidden W[204][21]:	mock grad = -0.003750514849326647, computed grad = -0.003750514864677942
Iteration: 854, Checking gradient for hidden b[0][33]:	mock grad = 0.083003282284443980, computed grad = 0.083003303716392682
Iteration: 854, Checking gradient for output layer W[108][1]:	mock grad = 0.143840005452172681, computed grad = 0.143840002754239527
Iteration: 855, Checking gradient for words E[102][5]:	mock grad = -0.030805464597244425, computed grad = -0.030805464707149264
Iteration: 855, Checking gradient for hidden W[246][24]:	mock grad = 0.012207074461784151, computed grad = 0.012207074557776965
Iteration: 855, Checking gradient for hidden b[0][2]:	mock grad = -0.056518324230897532, computed grad = -0.056518343115026688
Iteration: 855, Checking gradient for output layer W[143][0]:	mock grad = -0.150585956808640953, computed grad = -0.150585954234834979
Iteration: 856, Checking gradient for words E[102][37]:	mock grad = -0.027199566626168181, computed grad = -0.027199566715916244
Iteration: 856, Checking gradient for hidden W[237][43]:	mock grad = -0.001822669733864135, computed grad = -0.001822669739617684
Iteration: 856, Checking gradient for hidden b[0][18]:	mock grad = -0.079680272078885839, computed grad = -0.079680259831956660
Iteration: 856, Checking gradient for output layer W[3][1]:	mock grad = 0.026598775905162242, computed grad = 0.026598775902856479
Iteration: 857, Checking gradient for words E[231][30]:	mock grad = 0.000132558117399428, computed grad = 0.000132558105068844
Iteration: 857, Checking gradient for hidden W[193][14]:	mock grad = -0.013407249112057373, computed grad = -0.013407249247023219
Iteration: 857, Checking gradient for hidden b[0][34]:	mock grad = -0.016764140109815084, computed grad = -0.016764134459743875
Iteration: 857, Checking gradient for output layer W[41][1]:	mock grad = -0.052549987298394774, computed grad = -0.052549987218035957
Iteration: 858, Checking gradient for words E[342][20]:	mock grad = -0.024073456743489707, computed grad = -0.024073456775840007
Iteration: 858, Checking gradient for hidden W[228][3]:	mock grad = -0.009986740908768388, computed grad = -0.009986740947115267
Iteration: 858, Checking gradient for hidden b[0][17]:	mock grad = -0.020368951650223677, computed grad = -0.020368957447947790
Iteration: 858, Checking gradient for output layer W[62][0]:	mock grad = 0.158620057547353976, computed grad = 0.158620056184591546
current: 60, Cost = 0.388056, Correct(%) = 1, time = 4.14701
Iteration: 859, Checking gradient for words E[560][39]:	mock grad = 0.008724318811437204, computed grad = 0.008724318786416614
Iteration: 859, Checking gradient for hidden W[67][26]:	mock grad = -0.005620600070505022, computed grad = -0.005620600129195154
Iteration: 859, Checking gradient for hidden b[0][39]:	mock grad = 0.087782683268311512, computed grad = 0.087782712262645890
Iteration: 859, Checking gradient for output layer W[117][1]:	mock grad = -0.143497123764768642, computed grad = -0.143497122612670364
Iteration: 860, Checking gradient for words E[454][7]:	mock grad = 0.021266192966357123, computed grad = 0.021266193005935109
Iteration: 860, Checking gradient for hidden W[58][28]:	mock grad = 0.002071677231652025, computed grad = 0.002071677243553495
Iteration: 860, Checking gradient for hidden b[0][23]:	mock grad = 0.155339354053679601, computed grad = 0.155339390273282557
Iteration: 860, Checking gradient for output layer W[100][1]:	mock grad = -0.040932008441990630, computed grad = -0.040932008423512217
Iteration: 861, Checking gradient for words E[776][31]:	mock grad = -0.000711648522278008, computed grad = -0.000711648559516814
Iteration: 861, Checking gradient for hidden W[101][18]:	mock grad = -0.023292467462437960, computed grad = -0.023292467354297901
Iteration: 861, Checking gradient for hidden b[0][20]:	mock grad = -0.122712736585817161, computed grad = -0.122712772487914859
Iteration: 861, Checking gradient for output layer W[65][1]:	mock grad = -0.202149363192349885, computed grad = -0.202149361294533841
Iteration: 862, Checking gradient for words E[58][44]:	mock grad = -0.019666321062955516, computed grad = -0.019666321077731953
Iteration: 862, Checking gradient for hidden W[236][8]:	mock grad = 0.022747679127549381, computed grad = 0.022747679424327635
Iteration: 862, Checking gradient for hidden b[0][1]:	mock grad = -0.106663976820464690, computed grad = -0.106664000005255721
Iteration: 862, Checking gradient for output layer W[0][0]:	mock grad = -0.083602896430945428, computed grad = -0.083602896205023355
Iteration: 863, Checking gradient for words E[662][1]:	mock grad = -0.000851895273434877, computed grad = -0.000851895260220814
Iteration: 863, Checking gradient for hidden W[235][4]:	mock grad = -0.007366576938588887, computed grad = -0.007366577042097633
Iteration: 863, Checking gradient for hidden b[0][10]:	mock grad = -0.091579815491238392, computed grad = -0.091579843924075324
Iteration: 863, Checking gradient for output layer W[14][0]:	mock grad = 0.007072813089453733, computed grad = 0.007072813089351224
Iteration: 864, Checking gradient for words E[316][12]:	mock grad = 0.021060528233918241, computed grad = 0.021060528330610236
Iteration: 864, Checking gradient for hidden W[185][2]:	mock grad = 0.005732106130529324, computed grad = 0.005732106123629539
Iteration: 864, Checking gradient for hidden b[0][28]:	mock grad = -0.014458544205925294, computed grad = -0.014458563302871402
Iteration: 864, Checking gradient for output layer W[125][0]:	mock grad = -0.071851388708604746, computed grad = -0.071851388605766051
Iteration: 865, Checking gradient for words E[667][29]:	mock grad = 0.003057183034227418, computed grad = 0.003057183013900554
Iteration: 865, Checking gradient for hidden W[119][41]:	mock grad = -0.005968383367693786, computed grad = -0.005968383401787580
Iteration: 865, Checking gradient for hidden b[0][1]:	mock grad = -0.077976870971663281, computed grad = -0.077976881580058541
Iteration: 865, Checking gradient for output layer W[11][0]:	mock grad = -0.022841105290799524, computed grad = -0.022841105277668496
Iteration: 866, Checking gradient for words E[776][32]:	mock grad = -0.010428516207472427, computed grad = -0.010428516232334144
Iteration: 866, Checking gradient for hidden W[101][36]:	mock grad = -0.001963396219384350, computed grad = -0.001963396298361164
Iteration: 866, Checking gradient for hidden b[0][24]:	mock grad = -0.074397676569054605, computed grad = -0.074397674249171528
Iteration: 866, Checking gradient for output layer W[38][0]:	mock grad = -0.000209315346305639, computed grad = -0.000209315346350316
Iteration: 867, Checking gradient for words E[360][16]:	mock grad = 0.015890955236796955, computed grad = 0.015890955235630708
Iteration: 867, Checking gradient for hidden W[224][1]:	mock grad = 0.001978654094431098, computed grad = -0.004746875394308640
Iteration: 867, Checking gradient for hidden b[0][45]:	mock grad = 0.007221595581724527, computed grad = 0.007221594779904942
Iteration: 867, Checking gradient for output layer W[117][0]:	mock grad = -0.150056399468634361, computed grad = -0.150056398207024683
Iteration: 868, Checking gradient for words E[474][34]:	mock grad = 0.005254509189256407, computed grad = 0.005254509192432449
Iteration: 868, Checking gradient for hidden W[7][21]:	mock grad = 0.002553779148012358, computed grad = 0.002553779175526366
Iteration: 868, Checking gradient for hidden b[0][20]:	mock grad = -0.094246110872858724, computed grad = -0.094246138313203137
Iteration: 868, Checking gradient for output layer W[136][1]:	mock grad = 0.134434138107292211, computed grad = 0.134434136458715242
current: 70, Cost = 0.296004, Correct(%) = 1, time = 4.75914
Iteration: 869, Checking gradient for words E[320][3]:	mock grad = -0.002128279442625836, computed grad = -0.002128279477438097
Iteration: 869, Checking gradient for hidden W[197][37]:	mock grad = 0.000614353122430078, computed grad = 0.000614353164249676
Iteration: 869, Checking gradient for hidden b[0][19]:	mock grad = 0.036985372035308606, computed grad = 0.036985373647490868
Iteration: 869, Checking gradient for output layer W[9][1]:	mock grad = -0.035078715606373656, computed grad = -0.035078715566620740
Iteration: 870, Checking gradient for words E[684][33]:	mock grad = 0.040717382562088789, computed grad = 0.040717382567247898
Iteration: 870, Checking gradient for hidden W[111][25]:	mock grad = -0.002033263838047139, computed grad = -0.002033263842130109
Iteration: 870, Checking gradient for hidden b[0][9]:	mock grad = 0.063253266858548374, computed grad = 0.063253278737910937
Iteration: 870, Checking gradient for output layer W[149][1]:	mock grad = 0.048694159013512595, computed grad = 0.048694158984335920
Iteration: 871, Checking gradient for words E[410][5]:	mock grad = 0.045118607186450665, computed grad = 0.045118607292478088
Iteration: 871, Checking gradient for hidden W[235][48]:	mock grad = -0.013528965718240249, computed grad = -0.013528965811907757
Iteration: 871, Checking gradient for hidden b[0][45]:	mock grad = 0.010400683569472546, computed grad = 0.010400685854856947
Iteration: 871, Checking gradient for output layer W[107][1]:	mock grad = 0.028327170095793219, computed grad = 0.028327170089106058
Iteration: 872, Checking gradient for words E[403][39]:	mock grad = 0.006751676412702912, computed grad = 0.006751676429304767
Iteration: 872, Checking gradient for hidden W[91][39]:	mock grad = 0.000881091022836289, computed grad = 0.000881091037234985
Iteration: 872, Checking gradient for hidden b[0][33]:	mock grad = 0.110325856912590892, computed grad = 0.110325885004659202
Iteration: 872, Checking gradient for output layer W[70][0]:	mock grad = 0.146369021746944883, computed grad = 0.146369020863543309
Iteration: 873, Checking gradient for words E[102][33]:	mock grad = 0.023995784948327437, computed grad = 0.023995785075470469
Iteration: 873, Checking gradient for hidden W[85][30]:	mock grad = -0.008623137609714560, computed grad = -0.008623137780476704
Iteration: 873, Checking gradient for hidden b[0][37]:	mock grad = 0.060017019117508674, computed grad = 0.060017027203638038
Iteration: 873, Checking gradient for output layer W[122][1]:	mock grad = -0.170877920441681042, computed grad = -0.170877919626558156
Iteration: 874, Checking gradient for words E[304][46]:	mock grad = 0.000661600179641120, computed grad = 0.000661600222020508
Iteration: 874, Checking gradient for hidden W[29][16]:	mock grad = 0.006546403099216835, computed grad = 0.006546403157147248
Iteration: 874, Checking gradient for hidden b[0][47]:	mock grad = 0.095070614729686476, computed grad = 0.095070640077045090
Iteration: 874, Checking gradient for output layer W[136][0]:	mock grad = -0.167899916654334902, computed grad = -0.167899915668914296
Iteration: 875, Checking gradient for words E[341][26]:	mock grad = -0.023067473885890477, computed grad = -0.023425700841488789
Iteration: 875, Checking gradient for hidden W[32][29]:	mock grad = -0.016486769566004256, computed grad = -0.016486769862210551
Iteration: 875, Checking gradient for hidden b[0][20]:	mock grad = -0.092185829343421899, computed grad = -0.092185853617226338
Iteration: 875, Checking gradient for output layer W[117][0]:	mock grad = -0.163897641732135835, computed grad = -0.163897639213671703
Iteration: 876, Checking gradient for words E[167][20]:	mock grad = -0.023663217862152086, computed grad = -0.023663217853298217
Iteration: 876, Checking gradient for hidden W[223][11]:	mock grad = -0.001262594776368919, computed grad = -0.001262594782903792
Iteration: 876, Checking gradient for hidden b[0][23]:	mock grad = 0.142690064004968864, computed grad = 0.142690091489601267
Iteration: 876, Checking gradient for output layer W[63][1]:	mock grad = 0.122251765065722884, computed grad = 0.122251764362280718
Iteration: 877, Checking gradient for words E[698][10]:	mock grad = 0.031341207380519709, computed grad = 0.031341207288020867
Iteration: 877, Checking gradient for hidden W[108][48]:	mock grad = -0.009598982609104745, computed grad = -0.009598982624539386
Iteration: 877, Checking gradient for hidden b[0][13]:	mock grad = -0.103440980132485372, computed grad = -0.103441015142263559
Iteration: 877, Checking gradient for output layer W[57][0]:	mock grad = -0.021465396529418701, computed grad = -0.021465396520421058
Iteration: 878, Checking gradient for words E[25][28]:	mock grad = 0.020532953104751561, computed grad = 0.020532953095803351
Iteration: 878, Checking gradient for hidden W[67][4]:	mock grad = 0.001996879223614689, computed grad = 0.001996879219382347
Iteration: 878, Checking gradient for hidden b[0][22]:	mock grad = -0.041193895297858552, computed grad = -0.041193901697854141
Iteration: 878, Checking gradient for output layer W[62][0]:	mock grad = 0.103986924999410757, computed grad = 0.103986924486362139
current: 80, Cost = 0.326868, Correct(%) = 1, time = 5.24586
Iteration: 879, Checking gradient for words E[318][14]:	mock grad = -0.015974402401031096, computed grad = -0.015974402423708275
Iteration: 879, Checking gradient for hidden W[77][41]:	mock grad = 0.009125681306454414, computed grad = 0.009125681351182954
Iteration: 879, Checking gradient for hidden b[0][49]:	mock grad = 0.033913997357776227, computed grad = 0.033914003715244730
Iteration: 879, Checking gradient for output layer W[128][0]:	mock grad = 0.010065746526788377, computed grad = 0.010065746526082624
Iteration: 880, Checking gradient for words E[707][5]:	mock grad = 0.000714241122468096, computed grad = 0.000714241155543247
Iteration: 880, Checking gradient for hidden W[15][0]:	mock grad = 0.007095088913749992, computed grad = 0.007095088936165710
Iteration: 880, Checking gradient for hidden b[0][43]:	mock grad = 0.011576170927424245, computed grad = 0.011576174268257631
Iteration: 880, Checking gradient for output layer W[5][0]:	mock grad = 0.046839875901677486, computed grad = 0.046839875855316987
Iteration: 881, Checking gradient for words E[197][37]:	mock grad = 0.006964407406190309, computed grad = 0.006964407421575745
Iteration: 881, Checking gradient for hidden W[55][21]:	mock grad = -0.000661416280273164, computed grad = -0.000661416279395636
Iteration: 881, Checking gradient for hidden b[0][28]:	mock grad = 0.027457523278012763, computed grad = 0.027457545720833629
Iteration: 881, Checking gradient for output layer W[94][0]:	mock grad = -0.186401074868031147, computed grad = -0.186401071386873562
Iteration: 882, Checking gradient for words E[249][1]:	mock grad = 0.006892174359757197, computed grad = 0.006892174394835152
Iteration: 882, Checking gradient for hidden W[9][45]:	mock grad = 0.002304817372733581, computed grad = 0.002304817393222333
Iteration: 882, Checking gradient for hidden b[0][12]:	mock grad = 0.071861545791973569, computed grad = 0.071861568132960782
Iteration: 882, Checking gradient for output layer W[13][1]:	mock grad = -0.045013094862961234, computed grad = -0.045013094822771806
Iteration: 883, Checking gradient for words E[234][20]:	mock grad = -0.007790017690273432, computed grad = -0.007790017668967938
Iteration: 883, Checking gradient for hidden W[2][41]:	mock grad = -0.000696032433467053, computed grad = -0.000696032424344534
Iteration: 883, Checking gradient for hidden b[0][49]:	mock grad = 0.033830152788710821, computed grad = 0.033830152357847074
Iteration: 883, Checking gradient for output layer W[117][1]:	mock grad = -0.161002429487688836, computed grad = -0.161002428245062418
Iteration: 884, Checking gradient for words E[475][0]:	mock grad = -0.011249841757238199, computed grad = -0.011249841792597443
Iteration: 884, Checking gradient for hidden W[237][8]:	mock grad = 0.000690939026021242, computed grad = 0.000690939017483838
Iteration: 884, Checking gradient for hidden b[0][35]:	mock grad = 0.010688666071984043, computed grad = 0.010688665102561102
Iteration: 884, Checking gradient for output layer W[96][1]:	mock grad = 0.145710789092673432, computed grad = 0.145710787282287119
Iteration: 885, Checking gradient for words E[571][2]:	mock grad = 0.003304553420180323, computed grad = 0.003304553427995942
Iteration: 885, Checking gradient for hidden W[13][25]:	mock grad = -0.002574916843606223, computed grad = -0.002574916845740681
Iteration: 885, Checking gradient for hidden b[0][47]:	mock grad = 0.068084602361212498, computed grad = 0.068084616012489385
Iteration: 885, Checking gradient for output layer W[60][1]:	mock grad = -0.058002429613651652, computed grad = -0.058002429497615082
Iteration: 886, Checking gradient for words E[48][27]:	mock grad = -0.038813967770362501, computed grad = -0.038813967735642739
Iteration: 886, Checking gradient for hidden W[213][5]:	mock grad = -0.002171154240337980, computed grad = -0.002171154183194330
Iteration: 886, Checking gradient for hidden b[0][26]:	mock grad = -0.028988986799094452, computed grad = -0.028988990463901642
Iteration: 886, Checking gradient for output layer W[55][1]:	mock grad = 0.163376041266732885, computed grad = 0.163376039274247264
Iteration: 887, Checking gradient for words E[184][19]:	mock grad = -0.025219453731406549, computed grad = -0.025219453799316942
Iteration: 887, Checking gradient for hidden W[109][16]:	mock grad = -0.001252319034139582, computed grad = -0.001252319092486955
Iteration: 887, Checking gradient for hidden b[0][19]:	mock grad = -0.037264659683705625, computed grad = -0.037264660590441555
Iteration: 887, Checking gradient for output layer W[98][0]:	mock grad = -0.000727122459076579, computed grad = -0.000727122459131122
Iteration: 888, Checking gradient for words E[58][15]:	mock grad = 0.006972611447303434, computed grad = 0.006972611440313336
Iteration: 888, Checking gradient for hidden W[69][26]:	mock grad = -0.002981087405290461, computed grad = -0.002981087409639570
Iteration: 888, Checking gradient for hidden b[0][7]:	mock grad = -0.068535642850947598, computed grad = -0.068535646744450493
Iteration: 888, Checking gradient for output layer W[45][0]:	mock grad = -0.016321123302409646, computed grad = -0.016321123300383277
current: 90, Cost = 0.378231, Correct(%) = 1, time = 6.20062
Iteration: 889, Checking gradient for words E[119][14]:	mock grad = -0.001851417632120489, computed grad = -0.001851417571786387
Iteration: 889, Checking gradient for hidden W[175][48]:	mock grad = 0.000486037792984950, computed grad = 0.000486037870757648
Iteration: 889, Checking gradient for hidden b[0][3]:	mock grad = 0.032709041906803238, computed grad = 0.032709040943778307
Iteration: 889, Checking gradient for output layer W[118][1]:	mock grad = 0.036934845193625776, computed grad = 0.036934845172167691
Iteration: 890, Checking gradient for words E[705][21]:	mock grad = 0.001486673087497392, computed grad = 0.001486673103452010
Iteration: 890, Checking gradient for hidden W[161][35]:	mock grad = 0.004120576602817394, computed grad = 0.004120576644714120
Iteration: 890, Checking gradient for hidden b[0][30]:	mock grad = 0.004253492790984303, computed grad = 0.004253500070219464
Iteration: 890, Checking gradient for output layer W[28][0]:	mock grad = -0.075225100687609237, computed grad = -0.075225100414627724
Iteration: 891, Checking gradient for words E[750][19]:	mock grad = 0.018801413039531978, computed grad = 0.018801413120516412
Iteration: 891, Checking gradient for hidden W[20][37]:	mock grad = -0.002938593999385342, computed grad = -0.002938593953395843
Iteration: 891, Checking gradient for hidden b[0][48]:	mock grad = 0.085097168867981354, computed grad = 0.085097168665587697
Iteration: 891, Checking gradient for output layer W[93][0]:	mock grad = -0.023900696648870845, computed grad = -0.023900696643640487
Iteration: 892, Checking gradient for words E[109][22]:	mock grad = -0.016028617782049004, computed grad = -0.016028617814009025
Iteration: 892, Checking gradient for hidden W[201][36]:	mock grad = -0.000002904612328436, computed grad = -0.000002904619965176
Iteration: 892, Checking gradient for hidden b[0][15]:	mock grad = -0.078731479888877187, computed grad = -0.078731494546570954
Iteration: 892, Checking gradient for output layer W[22][0]:	mock grad = 0.047419892198696800, computed grad = 0.047419892108476372
Iteration: 893, Checking gradient for words E[645][31]:	mock grad = 0.009688003731522787, computed grad = 0.009688003752884405
Iteration: 893, Checking gradient for hidden W[207][17]:	mock grad = 0.001491323058044891, computed grad = 0.001491323092085410
Iteration: 893, Checking gradient for hidden b[0][10]:	mock grad = -0.085025486476625334, computed grad = -0.085025512718358190
Iteration: 893, Checking gradient for output layer W[29][1]:	mock grad = 0.040355556281068683, computed grad = 0.040355556251301744
Iteration: 894, Checking gradient for words E[611][8]:	mock grad = 0.009380528540936517, computed grad = 0.009380528583249118
Iteration: 894, Checking gradient for hidden W[235][39]:	mock grad = -0.006113909656574323, computed grad = -0.006113909710884878
Iteration: 894, Checking gradient for hidden b[0][6]:	mock grad = -0.032064703606532463, computed grad = -0.032064714142449130
Iteration: 894, Checking gradient for output layer W[81][1]:	mock grad = -0.030880472562055949, computed grad = -0.030880472536767185
Iteration: 895, Checking gradient for words E[554][14]:	mock grad = -0.009034229127929949, computed grad = -0.007386518362808620
Iteration: 895, Checking gradient for hidden W[247][31]:	mock grad = -0.019909652974214875, computed grad = -0.019909653405482779
Iteration: 895, Checking gradient for hidden b[0][16]:	mock grad = -0.018687737390193915, computed grad = -0.018687742940999726
Iteration: 895, Checking gradient for output layer W[116][1]:	mock grad = 0.154125007455402363, computed grad = 0.154125006267723241
Iteration: 896, Checking gradient for words E[603][7]:	mock grad = 0.015087777451627815, computed grad = 0.015087777454282823
Iteration: 896, Checking gradient for hidden W[193][14]:	mock grad = 0.016224018082294789, computed grad = 0.016224018302148573
Iteration: 896, Checking gradient for hidden b[0][35]:	mock grad = 0.010860521313632576, computed grad = 0.010860519886600034
Iteration: 896, Checking gradient for output layer W[49][1]:	mock grad = 0.039760311521946923, computed grad = 0.039760311489543759
Iteration: 897, Checking gradient for words E[411][35]:	mock grad = -0.003290278796586055, computed grad = -0.003284418891886132
Iteration: 897, Checking gradient for hidden W[226][41]:	mock grad = 0.006382094604850952, computed grad = 0.006382094651427240
Iteration: 897, Checking gradient for hidden b[0][24]:	mock grad = 0.117794127729997644, computed grad = 0.117794155212670448
Iteration: 897, Checking gradient for output layer W[117][1]:	mock grad = 0.185022570830639665, computed grad = 0.185022570091390670
Iteration: 898, Checking gradient for words E[69][14]:	mock grad = -0.000065182744535264, computed grad = -0.000065182733646524
Iteration: 898, Checking gradient for hidden W[13][25]:	mock grad = -0.002809474821924285, computed grad = -0.002809474916715669
Iteration: 898, Checking gradient for hidden b[0][37]:	mock grad = -0.044056107579931725, computed grad = -0.044056119059885226
Iteration: 898, Checking gradient for output layer W[65][0]:	mock grad = 0.151714599416402640, computed grad = 0.151714597168615656
current: 100, Cost = 0.380797, Correct(%) = 1, time = 6.98364
Iteration: 899, Checking gradient for words E[25][11]:	mock grad = 0.034365726131274643, computed grad = 0.035445438057151092
Iteration: 899, Checking gradient for hidden W[113][36]:	mock grad = 0.003373714711540909, computed grad = 0.003373714718103364
Iteration: 899, Checking gradient for hidden b[0][40]:	mock grad = 0.105363977128875019, computed grad = 0.105364005216561182
Iteration: 899, Checking gradient for output layer W[31][0]:	mock grad = 0.035035883846928906, computed grad = 0.035035883829048542
current: 9, Correct(%) = 1, time = 7.03485
Dev start.
Dev finished. Total time taken is: 0.634352
dev:
Accuracy:	P=100/100=1
Test start.
Test finished. Total time taken is: 0.545457
test:
Accuracy:	P=63/100=0.63
##### Iteration 9
random: 0, 99
Iteration: 900, Checking gradient for words E[34][34]:	mock grad = -0.017573867024250900, computed grad = -0.017573867087726510
Iteration: 900, Checking gradient for hidden W[113][23]:	mock grad = -0.005450496370018598, computed grad = -0.005450496369187667
Iteration: 900, Checking gradient for hidden b[0][12]:	mock grad = 0.056257215607563582, computed grad = 0.056257233694563084
Iteration: 900, Checking gradient for output layer W[121][1]:	mock grad = -0.056664578639148422, computed grad = -0.056664578420545647
Iteration: 901, Checking gradient for words E[48][2]:	mock grad = 0.011719405816540407, computed grad = 0.009947877312571990
Iteration: 901, Checking gradient for hidden W[140][47]:	mock grad = -0.013285431276682802, computed grad = -0.013285431298809759
Iteration: 901, Checking gradient for hidden b[0][27]:	mock grad = 0.038741870995395722, computed grad = 0.038741895263953753
Iteration: 901, Checking gradient for output layer W[14][1]:	mock grad = -0.004589227340839885, computed grad = -0.004589227340796647
Iteration: 902, Checking gradient for words E[119][32]:	mock grad = 0.008790362599514490, computed grad = 0.008790362647476414
Iteration: 902, Checking gradient for hidden W[238][16]:	mock grad = -0.003979875522724940, computed grad = -0.003979875545296511
Iteration: 902, Checking gradient for hidden b[0][2]:	mock grad = -0.050461568676524182, computed grad = -0.050461581232761336
Iteration: 902, Checking gradient for output layer W[136][0]:	mock grad = -0.181604275555830030, computed grad = -0.181604270921954469
Iteration: 903, Checking gradient for words E[109][32]:	mock grad = -0.015483333360266638, computed grad = -0.015483333391621286
Iteration: 903, Checking gradient for hidden W[96][22]:	mock grad = 0.006018484998954188, computed grad = 0.006018485014185048
Iteration: 903, Checking gradient for hidden b[0][36]:	mock grad = 0.006619070624463985, computed grad = 0.006619064050748780
Iteration: 903, Checking gradient for output layer W[143][0]:	mock grad = -0.166864421845630551, computed grad = -0.166864420378077999
Iteration: 904, Checking gradient for words E[159][32]:	mock grad = 0.012794391480497325, computed grad = 0.012794391492783382
Iteration: 904, Checking gradient for hidden W[9][2]:	mock grad = 0.006236930920083816, computed grad = 0.006236930906005304
Iteration: 904, Checking gradient for hidden b[0][38]:	mock grad = -0.059518216618703601, computed grad = -0.059518224371718317
Iteration: 904, Checking gradient for output layer W[124][1]:	mock grad = 0.037420868115173311, computed grad = 0.037420868091837575
Iteration: 905, Checking gradient for words E[164][0]:	mock grad = 0.008981939466440325, computed grad = 0.008981939509103866
Iteration: 905, Checking gradient for hidden W[230][40]:	mock grad = -0.000457109574025782, computed grad = -0.000457109576104790
Iteration: 905, Checking gradient for hidden b[0][15]:	mock grad = 0.083472905139214681, computed grad = 0.083472919919309135
Iteration: 905, Checking gradient for output layer W[10][1]:	mock grad = 0.027322327642020916, computed grad = 0.027322327631056843
Iteration: 906, Checking gradient for words E[48][11]:	mock grad = -0.000493160555503636, computed grad = -0.000493160607501496
Iteration: 906, Checking gradient for hidden W[93][6]:	mock grad = -0.003801243776385688, computed grad = -0.003801243817015333
Iteration: 906, Checking gradient for hidden b[0][46]:	mock grad = -0.106602313450054886, computed grad = -0.106602336071902934
Iteration: 906, Checking gradient for output layer W[74][1]:	mock grad = -0.077586229097009740, computed grad = -0.077586228566154020
Iteration: 907, Checking gradient for words E[69][32]:	mock grad = -0.018298395804305922, computed grad = -0.018298395927711444
Iteration: 907, Checking gradient for hidden W[229][21]:	mock grad = -0.006643000490580420, computed grad = -0.006643000656694343
Iteration: 907, Checking gradient for hidden b[0][47]:	mock grad = -0.105352506663602963, computed grad = -0.105352533469375578
Iteration: 907, Checking gradient for output layer W[93][1]:	mock grad = -0.087458625064873452, computed grad = -0.087458624979832450
Iteration: 908, Checking gradient for words E[296][7]:	mock grad = 0.005584580471479361, computed grad = 0.005584580500782494
Iteration: 908, Checking gradient for hidden W[248][40]:	mock grad = -0.002900508194708662, computed grad = -0.002900508224004053
Iteration: 908, Checking gradient for hidden b[0][17]:	mock grad = -0.017063856996568649, computed grad = -0.017063861403469112
Iteration: 908, Checking gradient for output layer W[32][0]:	mock grad = 0.100430438683851708, computed grad = 0.100430438027662786
current: 10, Cost = 0.370192, Correct(%) = 1, time = 0.774179
Iteration: 909, Checking gradient for words E[305][3]:	mock grad = -0.005108413529852918, computed grad = -0.005108413619491837
Iteration: 909, Checking gradient for hidden W[117][28]:	mock grad = -0.007971656995425791, computed grad = -0.007971657083548002
Iteration: 909, Checking gradient for hidden b[0][37]:	mock grad = 0.049953829835025498, computed grad = 0.049953837829158687
Iteration: 909, Checking gradient for output layer W[27][1]:	mock grad = -0.050653515758558587, computed grad = -0.050653515698993463
Iteration: 910, Checking gradient for words E[318][29]:	mock grad = 0.002274149656672497, computed grad = 0.002274149646495933
Iteration: 910, Checking gradient for hidden W[106][38]:	mock grad = -0.001167696112303052, computed grad = -0.001167696131934552
Iteration: 910, Checking gradient for hidden b[0][19]:	mock grad = 0.026569744426691022, computed grad = 0.026569738209157739
Iteration: 910, Checking gradient for output layer W[135][1]:	mock grad = 0.102657005829565051, computed grad = 0.102657004813710553
Iteration: 911, Checking gradient for words E[259][2]:	mock grad = 0.015251174181940552, computed grad = 0.015251174189540053
Iteration: 911, Checking gradient for hidden W[78][38]:	mock grad = -0.003175182594128412, computed grad = -0.003175182611910293
Iteration: 911, Checking gradient for hidden b[0][3]:	mock grad = -0.043152095812182267, computed grad = -0.043152106702559034
Iteration: 911, Checking gradient for output layer W[81][0]:	mock grad = -0.079072356365228469, computed grad = -0.079072356238007638
Iteration: 912, Checking gradient for words E[776][2]:	mock grad = -0.002047569135382732, computed grad = -0.002047569095843212
Iteration: 912, Checking gradient for hidden W[135][7]:	mock grad = -0.004339934571079285, computed grad = -0.004339934524422226
Iteration: 912, Checking gradient for hidden b[0][36]:	mock grad = -0.001450161900312663, computed grad = -0.001450172029765821
Iteration: 912, Checking gradient for output layer W[109][1]:	mock grad = 0.051565551602406057, computed grad = 0.051565551497090328
Iteration: 913, Checking gradient for words E[373][20]:	mock grad = 0.039370651969822257, computed grad = 0.039370652075117647
Iteration: 913, Checking gradient for hidden W[108][34]:	mock grad = 0.000158362084945463, computed grad = 0.000158362091519191
Iteration: 913, Checking gradient for hidden b[0][42]:	mock grad = -0.056930907527841246, computed grad = -0.056930920270275048
Iteration: 913, Checking gradient for output layer W[9][0]:	mock grad = -0.050336931209649105, computed grad = -0.050336931119232327
Iteration: 914, Checking gradient for words E[102][34]:	mock grad = 0.013373119613135920, computed grad = 0.013373119700351339
Iteration: 914, Checking gradient for hidden W[65][42]:	mock grad = -0.014412702302124547, computed grad = -0.014412702374591973
Iteration: 914, Checking gradient for hidden b[0][2]:	mock grad = -0.094553297194932551, computed grad = -0.094553342378386612
Iteration: 914, Checking gradient for output layer W[115][0]:	mock grad = 0.023233910857900497, computed grad = 0.023233910856870058
Iteration: 915, Checking gradient for words E[382][20]:	mock grad = 0.020675054031721318, computed grad = 0.019963526193661988
Iteration: 915, Checking gradient for hidden W[226][36]:	mock grad = 0.004618722216354065, computed grad = 0.004618722223109257
Iteration: 915, Checking gradient for hidden b[0][5]:	mock grad = -0.018940567373837425, computed grad = -0.018940570379730220
Iteration: 915, Checking gradient for output layer W[34][1]:	mock grad = 0.027156613465278978, computed grad = 0.027156613455783102
Iteration: 916, Checking gradient for words E[387][43]:	mock grad = 0.012071052191947595, computed grad = 0.012071052245352265
Iteration: 916, Checking gradient for hidden W[189][3]:	mock grad = 0.007217844804480178, computed grad = 0.007217844880834862
Iteration: 916, Checking gradient for hidden b[0][35]:	mock grad = -0.012366669512653417, computed grad = -0.012366668134454474
Iteration: 916, Checking gradient for output layer W[63][1]:	mock grad = -0.018353385069030947, computed grad = -0.018353385066614415
Iteration: 917, Checking gradient for words E[354][28]:	mock grad = -0.025441808142290245, computed grad = -0.025441808170506980
Iteration: 917, Checking gradient for hidden W[231][42]:	mock grad = -0.015464133473613861, computed grad = -0.015464133887411946
Iteration: 917, Checking gradient for hidden b[0][37]:	mock grad = -0.037686040170514801, computed grad = -0.037686050504338535
Iteration: 917, Checking gradient for output layer W[18][0]:	mock grad = 0.074603569582748497, computed grad = 0.074603569085907609
Iteration: 918, Checking gradient for words E[91][8]:	mock grad = 0.005941423000005441, computed grad = 0.005941423020134977
Iteration: 918, Checking gradient for hidden W[170][30]:	mock grad = -0.005744186363593196, computed grad = -0.005744186402402300
Iteration: 918, Checking gradient for hidden b[0][14]:	mock grad = 0.051781849561977067, computed grad = 0.051781867412943779
Iteration: 918, Checking gradient for output layer W[119][0]:	mock grad = -0.135064007810048459, computed grad = -0.135064005631375550
current: 20, Cost = 0.459975, Correct(%) = 1, time = 1.43447
Iteration: 919, Checking gradient for words E[440][20]:	mock grad = -0.017071104953469973, computed grad = -0.014181222597585103
Iteration: 919, Checking gradient for hidden W[151][47]:	mock grad = -0.016891448467892811, computed grad = -0.016891448622912423
Iteration: 919, Checking gradient for hidden b[0][48]:	mock grad = 0.104947154638962425, computed grad = 0.104947162286919665
Iteration: 919, Checking gradient for output layer W[94][1]:	mock grad = 0.183278079569942909, computed grad = 0.183278078318588916
Iteration: 920, Checking gradient for words E[68][38]:	mock grad = 0.008676354469189240, computed grad = 0.008676354550881537
Iteration: 920, Checking gradient for hidden W[49][0]:	mock grad = -0.001547881994190536, computed grad = -0.001547882030514455
Iteration: 920, Checking gradient for hidden b[0][49]:	mock grad = -0.034473302683662999, computed grad = -0.034473303580521086
Iteration: 920, Checking gradient for output layer W[40][0]:	mock grad = -0.013089530654930570, computed grad = -0.013089530653577144
Iteration: 921, Checking gradient for words E[776][28]:	mock grad = -0.055947032617104098, computed grad = -0.055947032386888952
Iteration: 921, Checking gradient for hidden W[106][28]:	mock grad = -0.014106431639293593, computed grad = -0.014106431776536473
Iteration: 921, Checking gradient for hidden b[0][13]:	mock grad = 0.156665651410398121, computed grad = 0.156665696836840784
Iteration: 921, Checking gradient for output layer W[111][1]:	mock grad = -0.005252368536656782, computed grad = -0.005252368536614646
Iteration: 922, Checking gradient for words E[351][5]:	mock grad = -0.008395915572223522, computed grad = -0.008395915593563846
Iteration: 922, Checking gradient for hidden W[73][6]:	mock grad = -0.002509594852068142, computed grad = -0.002509594883253084
Iteration: 922, Checking gradient for hidden b[0][42]:	mock grad = 0.058049703985763035, computed grad = 0.058049721798598589
Iteration: 922, Checking gradient for output layer W[78][0]:	mock grad = 0.121173549599146169, computed grad = 0.121173548210011681
Iteration: 923, Checking gradient for words E[457][38]:	mock grad = -0.017226470715475672, computed grad = -0.017226470710072335
Iteration: 923, Checking gradient for hidden W[60][19]:	mock grad = -0.005014933543212585, computed grad = -0.005014933531778312
Iteration: 923, Checking gradient for hidden b[0][26]:	mock grad = 0.032141226253362687, computed grad = 0.032141235575188810
Iteration: 923, Checking gradient for output layer W[72][0]:	mock grad = 0.096131916655195493, computed grad = 0.096131916159549941
Iteration: 924, Checking gradient for words E[433][35]:	mock grad = 0.029391875563417047, computed grad = 0.029391875602510914
Iteration: 924, Checking gradient for hidden W[96][2]:	mock grad = 0.004927998483311713, computed grad = 0.004927998507345347
Iteration: 924, Checking gradient for hidden b[0][13]:	mock grad = -0.121795178403771853, computed grad = -0.121795211239508136
Iteration: 924, Checking gradient for output layer W[83][0]:	mock grad = -0.151706329917511429, computed grad = -0.151706328421090636
Iteration: 925, Checking gradient for words E[25][37]:	mock grad = 0.011463636877651062, computed grad = 0.011463636937021194
Iteration: 925, Checking gradient for hidden W[16][44]:	mock grad = 0.010237583405292572, computed grad = 0.010237583650544444
Iteration: 925, Checking gradient for hidden b[0][39]:	mock grad = 0.080104361707772354, computed grad = 0.080104383773800913
Iteration: 925, Checking gradient for output layer W[106][1]:	mock grad = -0.047863040141898905, computed grad = -0.047863040091251689
Iteration: 926, Checking gradient for words E[152][41]:	mock grad = -0.012677047386666640, computed grad = -0.012677047401745762
Iteration: 926, Checking gradient for hidden W[95][42]:	mock grad = 0.003827175304560448, computed grad = 0.003827175312359833
Iteration: 926, Checking gradient for hidden b[0][3]:	mock grad = 0.025031109428197418, computed grad = 0.025031108727976505
Iteration: 926, Checking gradient for output layer W[122][0]:	mock grad = -0.096017194919917337, computed grad = -0.096017193979660853
Iteration: 927, Checking gradient for words E[69][36]:	mock grad = 0.062847058716480886, computed grad = 0.064473411229030664
Iteration: 927, Checking gradient for hidden W[84][6]:	mock grad = 0.002430251799967476, computed grad = 0.002430251818252376
Iteration: 927, Checking gradient for hidden b[0][46]:	mock grad = 0.183747512527393519, computed grad = 0.183747564265178132
Iteration: 927, Checking gradient for output layer W[55][1]:	mock grad = 0.075497774472327306, computed grad = 0.075497774402598750
Iteration: 928, Checking gradient for words E[490][24]:	mock grad = 0.003208527187575916, computed grad = 0.003208527183411674
Iteration: 928, Checking gradient for hidden W[38][47]:	mock grad = -0.004274047376934265, computed grad = -0.004274047416308908
Iteration: 928, Checking gradient for hidden b[0][47]:	mock grad = -0.053453260022401361, computed grad = -0.053453264194754331
Iteration: 928, Checking gradient for output layer W[70][0]:	mock grad = -0.096510537741206770, computed grad = -0.096510536905020261
current: 30, Cost = 0.340461, Correct(%) = 1, time = 2.13592
Iteration: 929, Checking gradient for words E[143][26]:	mock grad = -0.033094403819405871, computed grad = -0.034658035474252336
Iteration: 929, Checking gradient for hidden W[172][39]:	mock grad = 0.006403599763699086, computed grad = 0.006403599789779747
Iteration: 929, Checking gradient for hidden b[0][4]:	mock grad = -0.023814746441364054, computed grad = -0.023814759434374923
Iteration: 929, Checking gradient for output layer W[115][1]:	mock grad = 0.078466523509346242, computed grad = 0.078466523218427703
Iteration: 930, Checking gradient for words E[156][13]:	mock grad = 0.018550208671863633, computed grad = 0.018550208598909195
Iteration: 930, Checking gradient for hidden W[32][19]:	mock grad = -0.005153429718102176, computed grad = -0.005153429764525085
Iteration: 930, Checking gradient for hidden b[0][14]:	mock grad = -0.046972600946854071, computed grad = -0.046972616924454175
Iteration: 930, Checking gradient for output layer W[46][0]:	mock grad = -0.036528279073438918, computed grad = -0.036528279041787383
Iteration: 931, Checking gradient for words E[119][25]:	mock grad = -0.011275317790243600, computed grad = -0.011275317670824302
Iteration: 931, Checking gradient for hidden W[240][28]:	mock grad = 0.009616535880330179, computed grad = 0.009616535945275178
Iteration: 931, Checking gradient for hidden b[0][46]:	mock grad = 0.164658332215517067, computed grad = 0.164658376060865447
Iteration: 931, Checking gradient for output layer W[38][1]:	mock grad = -0.025014136434131640, computed grad = -0.025014136431185607
Iteration: 932, Checking gradient for words E[379][27]:	mock grad = 0.017352699237443980, computed grad = 0.016960201498580546
Iteration: 932, Checking gradient for hidden W[116][20]:	mock grad = -0.000770129702787337, computed grad = -0.000770129733206570
Iteration: 932, Checking gradient for hidden b[0][26]:	mock grad = -0.029937490004600109, computed grad = -0.029937497542531655
Iteration: 932, Checking gradient for output layer W[11][1]:	mock grad = 0.057962941150874192, computed grad = 0.057962941021965364
Iteration: 933, Checking gradient for words E[453][3]:	mock grad = -0.020449732461652603, computed grad = -0.014383744531368639
Iteration: 933, Checking gradient for hidden W[134][2]:	mock grad = -0.011564393598673117, computed grad = -0.011564393597040328
Iteration: 933, Checking gradient for hidden b[0][8]:	mock grad = -0.037910329556495315, computed grad = -0.037910351307193280
Iteration: 933, Checking gradient for output layer W[93][0]:	mock grad = -0.016062938801353344, computed grad = -0.016062938797740702
Iteration: 934, Checking gradient for words E[48][14]:	mock grad = 0.062122729577118729, computed grad = 0.062122729586285036
Iteration: 934, Checking gradient for hidden W[143][43]:	mock grad = 0.001284823583580508, computed grad = 0.001284823586677742
Iteration: 934, Checking gradient for hidden b[0][18]:	mock grad = 0.055480390455969752, computed grad = 0.055480383395414709
Iteration: 934, Checking gradient for output layer W[79][0]:	mock grad = -0.073387112255851594, computed grad = -0.073387112152705880
Iteration: 935, Checking gradient for words E[201][24]:	mock grad = -0.023716662477080153, computed grad = -0.023716662557669556
Iteration: 935, Checking gradient for hidden W[76][44]:	mock grad = 0.015159020981508897, computed grad = 0.015159021113219739
Iteration: 935, Checking gradient for hidden b[0][47]:	mock grad = -0.091509043747939334, computed grad = -0.091509067896952984
Iteration: 935, Checking gradient for output layer W[72][0]:	mock grad = -0.020645072701858114, computed grad = -0.020645072699298245
Iteration: 936, Checking gradient for words E[48][16]:	mock grad = -0.002982542954860223, computed grad = -0.002817886546406020
Iteration: 936, Checking gradient for hidden W[118][0]:	mock grad = 0.007448408860327715, computed grad = 0.007448408968996261
Iteration: 936, Checking gradient for hidden b[0][25]:	mock grad = 0.010174462553302899, computed grad = 0.010174462184558711
Iteration: 936, Checking gradient for output layer W[108][1]:	mock grad = -0.116952921463464588, computed grad = -0.116952921171296173
Iteration: 937, Checking gradient for words E[23][39]:	mock grad = 0.023539554085433290, computed grad = 0.023539554082600070
Iteration: 937, Checking gradient for hidden W[170][17]:	mock grad = -0.001746556350101791, computed grad = -0.001746556347665857
Iteration: 937, Checking gradient for hidden b[0][32]:	mock grad = -0.053573563436215554, computed grad = -0.053573580541492249
Iteration: 937, Checking gradient for output layer W[80][0]:	mock grad = 0.077817621482262345, computed grad = 0.077817621245964336
Iteration: 938, Checking gradient for words E[488][15]:	mock grad = 0.025390747330572561, computed grad = 0.025390747449650510
Iteration: 938, Checking gradient for hidden W[122][22]:	mock grad = -0.007526501083265913, computed grad = -0.007526501193105486
Iteration: 938, Checking gradient for hidden b[0][35]:	mock grad = 0.014222932636115404, computed grad = 0.014222932267619129
Iteration: 938, Checking gradient for output layer W[25][0]:	mock grad = -0.076813260170061071, computed grad = -0.076813260045554277
current: 40, Cost = 0.288363, Correct(%) = 1, time = 2.57281
Iteration: 939, Checking gradient for words E[31][21]:	mock grad = 0.008114666124298120, computed grad = 0.008114666168886720
Iteration: 939, Checking gradient for hidden W[205][42]:	mock grad = 0.001403784344572445, computed grad = 0.001403784346213074
Iteration: 939, Checking gradient for hidden b[0][13]:	mock grad = 0.088243945101068588, computed grad = 0.088243962100257867
Iteration: 939, Checking gradient for output layer W[102][1]:	mock grad = 0.115389624232742349, computed grad = 0.115389622706837960
Iteration: 940, Checking gradient for words E[776][49]:	mock grad = 0.014966021084972247, computed grad = 0.014966021115980986
Iteration: 940, Checking gradient for hidden W[108][31]:	mock grad = -0.006752634158030402, computed grad = -0.006752634184133978
Iteration: 940, Checking gradient for hidden b[0][3]:	mock grad = 0.028731335535714386, computed grad = 0.028731337061089532
Iteration: 940, Checking gradient for output layer W[84][1]:	mock grad = -0.095220607456425466, computed grad = -0.095220606628366108
Iteration: 941, Checking gradient for words E[558][28]:	mock grad = -0.000282882013730168, computed grad = -0.000282882003086769
Iteration: 941, Checking gradient for hidden W[42][47]:	mock grad = 0.003103988221508924, computed grad = 0.003103988247865958
Iteration: 941, Checking gradient for hidden b[0][9]:	mock grad = -0.061038618229120045, computed grad = -0.061038616303661183
Iteration: 941, Checking gradient for output layer W[128][0]:	mock grad = 0.014049463312154620, computed grad = 0.014049463311721223
Iteration: 942, Checking gradient for words E[34][2]:	mock grad = -0.002972719271104873, computed grad = -0.002972719195343780
Iteration: 942, Checking gradient for hidden W[182][46]:	mock grad = -0.020315833602124744, computed grad = -0.020315833786744363
Iteration: 942, Checking gradient for hidden b[0][9]:	mock grad = -0.073618271736297647, computed grad = -0.073618287616254691
Iteration: 942, Checking gradient for output layer W[109][0]:	mock grad = 0.006929955147144673, computed grad = 0.006929955147054494
Iteration: 943, Checking gradient for words E[156][18]:	mock grad = 0.033851320000000129, computed grad = 0.033851320156397489
Iteration: 943, Checking gradient for hidden W[6][2]:	mock grad = -0.008997353353967785, computed grad = -0.008997353470761313
Iteration: 943, Checking gradient for hidden b[0][35]:	mock grad = 0.017978784112443469, computed grad = 0.017978786374993658
Iteration: 943, Checking gradient for output layer W[97][0]:	mock grad = -0.179609016116966114, computed grad = -0.179609015272239297
Iteration: 944, Checking gradient for words E[323][9]:	mock grad = 0.002944267157717340, computed grad = 0.002944267129185545
Iteration: 944, Checking gradient for hidden W[12][28]:	mock grad = -0.002996360652124386, computed grad = -0.002996360608966634
Iteration: 944, Checking gradient for hidden b[0][6]:	mock grad = -0.028820327721890138, computed grad = -0.028820337932627137
Iteration: 944, Checking gradient for output layer W[19][0]:	mock grad = -0.031225822044372853, computed grad = -0.031225822001578796
Iteration: 945, Checking gradient for words E[519][2]:	mock grad = 0.049073671460064805, computed grad = 0.049073671596489225
Iteration: 945, Checking gradient for hidden W[40][41]:	mock grad = -0.005785148706083376, computed grad = -0.005785148734756589
Iteration: 945, Checking gradient for hidden b[0][31]:	mock grad = -0.041081967574990719, computed grad = -0.041081976451398279
Iteration: 945, Checking gradient for output layer W[35][1]:	mock grad = 0.106670881936443696, computed grad = 0.106670881567962117
Iteration: 946, Checking gradient for words E[78][7]:	mock grad = 0.004767609389749738, computed grad = 0.004767609389238774
Iteration: 946, Checking gradient for hidden W[95][19]:	mock grad = -0.005659470257157739, computed grad = -0.009154681102158224
Iteration: 946, Checking gradient for hidden b[0][37]:	mock grad = 0.045217133628566719, computed grad = 0.045217144231932335
Iteration: 946, Checking gradient for output layer W[92][1]:	mock grad = 0.107404721887666144, computed grad = 0.107404720632096523
Iteration: 947, Checking gradient for words E[597][37]:	mock grad = -0.026087930885682065, computed grad = -0.026087930992395821
Iteration: 947, Checking gradient for hidden W[131][41]:	mock grad = -0.005298249439206204, computed grad = -0.005298249449690009
Iteration: 947, Checking gradient for hidden b[0][46]:	mock grad = -0.126911943846202968, computed grad = -0.126911971625022962
Iteration: 947, Checking gradient for output layer W[33][1]:	mock grad = -0.017689197003151191, computed grad = -0.017689196999290252
Iteration: 948, Checking gradient for words E[320][33]:	mock grad = -0.005409760765184846, computed grad = -0.005409760771665025
Iteration: 948, Checking gradient for hidden W[56][29]:	mock grad = 0.012970349323460084, computed grad = 0.012970349328944785
Iteration: 948, Checking gradient for hidden b[0][18]:	mock grad = 0.051982147736689965, computed grad = 0.051982150806652458
Iteration: 948, Checking gradient for output layer W[48][1]:	mock grad = -0.083737956121293644, computed grad = -0.083737955605856063
current: 50, Cost = 0.289684, Correct(%) = 1, time = 3.46177
Iteration: 949, Checking gradient for words E[42][28]:	mock grad = 0.002396473202215255, computed grad = 0.002396473199008967
Iteration: 949, Checking gradient for hidden W[194][19]:	mock grad = -0.006972071071642549, computed grad = -0.006972071103115940
Iteration: 949, Checking gradient for hidden b[0][39]:	mock grad = 0.062173885892802261, computed grad = 0.062173899073206140
Iteration: 949, Checking gradient for output layer W[141][1]:	mock grad = -0.094491770439741885, computed grad = -0.094491769612761234
Iteration: 950, Checking gradient for words E[615][31]:	mock grad = 0.021544645784049576, computed grad = 0.021544645935297098
Iteration: 950, Checking gradient for hidden W[186][34]:	mock grad = 0.020062324405184517, computed grad = 0.020062324584403296
Iteration: 950, Checking gradient for hidden b[0][29]:	mock grad = -0.096322608973659074, computed grad = -0.096322631528609245
Iteration: 950, Checking gradient for output layer W[108][1]:	mock grad = -0.183835859920433720, computed grad = -0.183835858868839824
Iteration: 951, Checking gradient for words E[48][17]:	mock grad = -0.023152932386749647, computed grad = -0.023152932388451768
Iteration: 951, Checking gradient for hidden W[76][18]:	mock grad = 0.010359565509016067, computed grad = 0.010359565672911866
Iteration: 951, Checking gradient for hidden b[0][46]:	mock grad = -0.139436927184016790, computed grad = -0.139436956859416400
Iteration: 951, Checking gradient for output layer W[106][0]:	mock grad = -0.080835116834820919, computed grad = -0.080835116604707422
Iteration: 952, Checking gradient for words E[42][11]:	mock grad = -0.018249716851020947, computed grad = -0.016811505632834803
Iteration: 952, Checking gradient for hidden W[76][27]:	mock grad = 0.003571054145723451, computed grad = 0.003571054156085478
Iteration: 952, Checking gradient for hidden b[0][46]:	mock grad = -0.149251681969497785, computed grad = -0.149251714470629154
Iteration: 952, Checking gradient for output layer W[108][1]:	mock grad = 0.173764628765754603, computed grad = 0.173764627019883516
Iteration: 953, Checking gradient for words E[102][28]:	mock grad = 0.027213987170315868, computed grad = 0.027213987209227186
Iteration: 953, Checking gradient for hidden W[11][37]:	mock grad = -0.002983896787045470, computed grad = -0.002983896774641365
Iteration: 953, Checking gradient for hidden b[0][34]:	mock grad = -0.012799113161876541, computed grad = -0.012799105086398635
Iteration: 953, Checking gradient for output layer W[34][1]:	mock grad = 0.011353213037856857, computed grad = 0.011353213036954258
Iteration: 954, Checking gradient for words E[60][3]:	mock grad = 0.004039796640709081, computed grad = 0.004039796670348770
Iteration: 954, Checking gradient for hidden W[182][6]:	mock grad = -0.002934854840014323, computed grad = -0.002934854867866890
Iteration: 954, Checking gradient for hidden b[0][46]:	mock grad = -0.111931777704826763, computed grad = -0.111931799810630409
Iteration: 954, Checking gradient for output layer W[32][0]:	mock grad = 0.070075478403458513, computed grad = 0.070075478051352683
Iteration: 955, Checking gradient for words E[637][47]:	mock grad = -0.005999926849353798, computed grad = -0.005999926856677440
Iteration: 955, Checking gradient for hidden W[111][32]:	mock grad = 0.006361029029566811, computed grad = 0.006361029052250388
Iteration: 955, Checking gradient for hidden b[0][41]:	mock grad = 0.030982742064311619, computed grad = 0.030982749898945228
Iteration: 955, Checking gradient for output layer W[58][0]:	mock grad = 0.042287514671779380, computed grad = 0.042287514607829424
Iteration: 956, Checking gradient for words E[460][35]:	mock grad = -0.012976528753705363, computed grad = -0.012976529110796400
Iteration: 956, Checking gradient for hidden W[82][26]:	mock grad = 0.016943725199725090, computed grad = 0.016943725408671662
Iteration: 956, Checking gradient for hidden b[0][11]:	mock grad = 0.113277520424670541, computed grad = 0.113277547275279653
Iteration: 956, Checking gradient for output layer W[144][0]:	mock grad = -0.143270336278034094, computed grad = -0.143270335870436888
Iteration: 957, Checking gradient for words E[371][18]:	mock grad = -0.004214801697988158, computed grad = -0.004214801711112186
Iteration: 957, Checking gradient for hidden W[121][47]:	mock grad = 0.015521653428440807, computed grad = 0.015521653472370806
Iteration: 957, Checking gradient for hidden b[0][46]:	mock grad = 0.123607878250386882, computed grad = 0.123607897804504402
Iteration: 957, Checking gradient for output layer W[140][1]:	mock grad = -0.130917852673800850, computed grad = -0.130917851312100403
Iteration: 958, Checking gradient for words E[25][13]:	mock grad = 0.013039523687335963, computed grad = 0.013039523696554239
Iteration: 958, Checking gradient for hidden W[38][0]:	mock grad = -0.008993447969674051, computed grad = -0.008993447977236042
Iteration: 958, Checking gradient for hidden b[0][27]:	mock grad = -0.033991863724047722, computed grad = -0.033991874720125305
Iteration: 958, Checking gradient for output layer W[0][1]:	mock grad = -0.077248831846066235, computed grad = -0.077248831671857726
current: 60, Cost = 0.371814, Correct(%) = 1, time = 4.15305
Iteration: 959, Checking gradient for words E[647][16]:	mock grad = -0.008483674396542185, computed grad = -0.008483674415238479
Iteration: 959, Checking gradient for hidden W[19][37]:	mock grad = 0.008306881280145673, computed grad = 0.008306881309946256
Iteration: 959, Checking gradient for hidden b[0][24]:	mock grad = -0.075895088448740822, computed grad = -0.075895083642645203
Iteration: 959, Checking gradient for output layer W[14][1]:	mock grad = 0.006745821977821587, computed grad = 0.006745821977641232
Iteration: 960, Checking gradient for words E[455][25]:	mock grad = 0.006312025848081992, computed grad = 0.006312025858516077
Iteration: 960, Checking gradient for hidden W[249][21]:	mock grad = -0.006321580414275774, computed grad = -0.006321580483657957
Iteration: 960, Checking gradient for hidden b[0][38]:	mock grad = 0.076745500149438817, computed grad = 0.076745522151826659
Iteration: 960, Checking gradient for output layer W[70][1]:	mock grad = 0.150553954521848166, computed grad = 0.150553953516087136
Iteration: 961, Checking gradient for words E[656][12]:	mock grad = 0.013575743889965297, computed grad = 0.013563385817219447
Iteration: 961, Checking gradient for hidden W[158][26]:	mock grad = -0.002704750851789672, computed grad = -0.002704750886481771
Iteration: 961, Checking gradient for hidden b[0][38]:	mock grad = -0.075905312816204606, computed grad = -0.075905332508128892
Iteration: 961, Checking gradient for output layer W[62][0]:	mock grad = 0.093535791708454497, computed grad = 0.093535791483672942
Iteration: 962, Checking gradient for words E[102][23]:	mock grad = 0.016607406786633971, computed grad = 0.016607406812384040
Iteration: 962, Checking gradient for hidden W[22][16]:	mock grad = 0.002168512586081528, computed grad = 0.002168512601650787
Iteration: 962, Checking gradient for hidden b[0][32]:	mock grad = 0.055844857248715796, computed grad = 0.055844875775873846
Iteration: 962, Checking gradient for output layer W[110][1]:	mock grad = -0.109500041184679153, computed grad = -0.109500040607547228
Iteration: 963, Checking gradient for words E[393][30]:	mock grad = 0.013735497951694597, computed grad = 0.013735497949006992
Iteration: 963, Checking gradient for hidden W[184][36]:	mock grad = -0.006445771983276671, computed grad = -0.006445772000711764
Iteration: 963, Checking gradient for hidden b[0][24]:	mock grad = 0.097942305054277146, computed grad = 0.097942332027761744
Iteration: 963, Checking gradient for output layer W[81][0]:	mock grad = 0.010818729419492801, computed grad = 0.010818729419000512
Iteration: 964, Checking gradient for words E[563][36]:	mock grad = 0.002943149841699855, computed grad = 0.000673304057189844
Iteration: 964, Checking gradient for hidden W[21][29]:	mock grad = -0.008345709219953479, computed grad = -0.008345709235696131
Iteration: 964, Checking gradient for hidden b[0][29]:	mock grad = 0.090144532234909258, computed grad = 0.090144563837310357
Iteration: 964, Checking gradient for output layer W[20][1]:	mock grad = -0.006459761234006711, computed grad = -0.006459761233932914
Iteration: 965, Checking gradient for words E[650][48]:	mock grad = 0.016760629396872506, computed grad = 0.016760629428540060
Iteration: 965, Checking gradient for hidden W[223][25]:	mock grad = -0.006810639891913217, computed grad = -0.006810639946575427
Iteration: 965, Checking gradient for hidden b[0][4]:	mock grad = 0.014303518293845574, computed grad = 0.014303522096067398
Iteration: 965, Checking gradient for output layer W[16][0]:	mock grad = 0.039597844239819269, computed grad = 0.039597844163539397
Iteration: 966, Checking gradient for words E[215][13]:	mock grad = -0.015087211991393623, computed grad = -0.017314759608138369
Iteration: 966, Checking gradient for hidden W[183][27]:	mock grad = -0.009721750228758319, computed grad = -0.009721750270133861
Iteration: 966, Checking gradient for hidden b[0][31]:	mock grad = -0.036871842843261282, computed grad = -0.036871853626878663
Iteration: 966, Checking gradient for output layer W[146][0]:	mock grad = 0.076561095999322371, computed grad = 0.076561095718412020
Iteration: 967, Checking gradient for words E[69][42]:	mock grad = 0.005566963840603067, computed grad = 0.005566963858503090
Iteration: 967, Checking gradient for hidden W[50][27]:	mock grad = -0.019538537837565650, computed grad = -0.019538537925170921
Iteration: 967, Checking gradient for hidden b[0][34]:	mock grad = 0.031437020507979474, computed grad = 0.031437030559809363
Iteration: 967, Checking gradient for output layer W[78][1]:	mock grad = -0.117052497942748968, computed grad = -0.117052497262398164
Iteration: 968, Checking gradient for words E[119][37]:	mock grad = -0.000845608545685383, computed grad = -0.000845608573364207
Iteration: 968, Checking gradient for hidden W[7][39]:	mock grad = 0.011871358022413903, computed grad = 0.011871358130704187
Iteration: 968, Checking gradient for hidden b[0][32]:	mock grad = -0.047961581889771621, computed grad = -0.047961596836613961
Iteration: 968, Checking gradient for output layer W[40][0]:	mock grad = -0.022887704815371945, computed grad = -0.022887704806059690
current: 70, Cost = 0.28446, Correct(%) = 1, time = 4.76763
Iteration: 969, Checking gradient for words E[25][11]:	mock grad = 0.018071731583746420, computed grad = 0.018071731602194465
Iteration: 969, Checking gradient for hidden W[216][7]:	mock grad = -0.008914652092101294, computed grad = -0.008914652135187556
Iteration: 969, Checking gradient for hidden b[0][10]:	mock grad = -0.069159933631102177, computed grad = -0.069159956847760717
Iteration: 969, Checking gradient for output layer W[40][1]:	mock grad = 0.026420128530657561, computed grad = 0.026420128511530930
Iteration: 970, Checking gradient for words E[335][26]:	mock grad = 0.024141932673638067, computed grad = 0.024141932609891822
Iteration: 970, Checking gradient for hidden W[46][23]:	mock grad = -0.000635533664666843, computed grad = -0.000635533672319344
Iteration: 970, Checking gradient for hidden b[0][3]:	mock grad = 0.043455752749582688, computed grad = 0.043455759464814274
Iteration: 970, Checking gradient for output layer W[141][0]:	mock grad = -0.184723877141190762, computed grad = -0.184723875319828756
Iteration: 971, Checking gradient for words E[686][4]:	mock grad = -0.014258539179301533, computed grad = -0.014258539259286615
Iteration: 971, Checking gradient for hidden W[237][23]:	mock grad = 0.000088424098015905, computed grad = 0.000088424091622197
Iteration: 971, Checking gradient for hidden b[0][16]:	mock grad = -0.019096587436928658, computed grad = -0.019096593371353596
Iteration: 971, Checking gradient for output layer W[38][0]:	mock grad = 0.019226374601999385, computed grad = 0.019226374599688973
Iteration: 972, Checking gradient for words E[34][12]:	mock grad = 0.009503903628332866, computed grad = 0.009503903689079520
Iteration: 972, Checking gradient for hidden W[57][38]:	mock grad = -0.002981537435192694, computed grad = -0.002981537434690455
Iteration: 972, Checking gradient for hidden b[0][12]:	mock grad = -0.078227895071697473, computed grad = -0.078227919866382445
Iteration: 972, Checking gradient for output layer W[95][1]:	mock grad = -0.087474432314132988, computed grad = -0.087474432104020172
Iteration: 973, Checking gradient for words E[333][17]:	mock grad = 0.004070986187293402, computed grad = 0.004070986172844489
Iteration: 973, Checking gradient for hidden W[14][31]:	mock grad = 0.014929838470761903, computed grad = 0.014929838614168115
Iteration: 973, Checking gradient for hidden b[0][38]:	mock grad = 0.086586468334509670, computed grad = 0.086586495832391586
Iteration: 973, Checking gradient for output layer W[30][0]:	mock grad = -0.005787869634982235, computed grad = -0.005787869634941412
Iteration: 974, Checking gradient for words E[609][46]:	mock grad = -0.026539358064542329, computed grad = -0.025462336570185509
Iteration: 974, Checking gradient for hidden W[48][38]:	mock grad = -0.008791851189821065, computed grad = -0.014920007620676091
Iteration: 974, Checking gradient for hidden b[0][19]:	mock grad = 0.053481794133475313, computed grad = 0.053481798856581650
Iteration: 974, Checking gradient for output layer W[104][1]:	mock grad = 0.046926614121062249, computed grad = 0.046926614096270386
Iteration: 975, Checking gradient for words E[148][8]:	mock grad = 0.012831504053512077, computed grad = 0.012831504080119707
Iteration: 975, Checking gradient for hidden W[160][15]:	mock grad = 0.002348693312725914, computed grad = 0.002348693318310431
Iteration: 975, Checking gradient for hidden b[0][13]:	mock grad = 0.103777448548503237, computed grad = 0.103777470371057196
Iteration: 975, Checking gradient for output layer W[50][0]:	mock grad = 0.166480715120514988, computed grad = 0.166480712120318064
Iteration: 976, Checking gradient for words E[697][37]:	mock grad = -0.018507332909978302, computed grad = -0.018507332929385237
Iteration: 976, Checking gradient for hidden W[182][24]:	mock grad = -0.002925403180170649, computed grad = -0.002925403220991599
Iteration: 976, Checking gradient for hidden b[0][44]:	mock grad = 0.002846319059668367, computed grad = 0.002846314731730478
Iteration: 976, Checking gradient for output layer W[49][1]:	mock grad = 0.045660729504209963, computed grad = 0.045660729462523204
Iteration: 977, Checking gradient for words E[94][1]:	mock grad = 0.070026735422806796, computed grad = 0.070026735008950416
Iteration: 977, Checking gradient for hidden W[40][0]:	mock grad = -0.005080714906874340, computed grad = -0.005080714933330517
Iteration: 977, Checking gradient for hidden b[0][40]:	mock grad = 0.087504531417259557, computed grad = 0.087504559699511042
Iteration: 977, Checking gradient for output layer W[122][0]:	mock grad = 0.111819566434273598, computed grad = 0.111819565042300395
Iteration: 978, Checking gradient for words E[69][36]:	mock grad = 0.008982987145217392, computed grad = 0.008982987219225870
Iteration: 978, Checking gradient for hidden W[146][26]:	mock grad = -0.003710258711875358, computed grad = -0.003710258724322473
Iteration: 978, Checking gradient for hidden b[0][24]:	mock grad = 0.092322546716677723, computed grad = 0.092322572512638076
Iteration: 978, Checking gradient for output layer W[131][0]:	mock grad = -0.133521164082356369, computed grad = -0.133521162883662836
current: 80, Cost = 0.314207, Correct(%) = 1, time = 5.2512
Iteration: 979, Checking gradient for words E[396][4]:	mock grad = -0.008633103828176170, computed grad = -0.008633103886537118
Iteration: 979, Checking gradient for hidden W[132][47]:	mock grad = 0.011931747944171622, computed grad = 0.011931747988666497
Iteration: 979, Checking gradient for hidden b[0][35]:	mock grad = 0.010455499660555478, computed grad = 0.010455498960623604
Iteration: 979, Checking gradient for output layer W[83][1]:	mock grad = 0.161557542664697573, computed grad = 0.161557539411744527
Iteration: 980, Checking gradient for words E[48][24]:	mock grad = 0.031354972213210042, computed grad = 0.031354972251397502
Iteration: 980, Checking gradient for hidden W[48][4]:	mock grad = 0.000497185475206718, computed grad = 0.000497185471493457
Iteration: 980, Checking gradient for hidden b[0][48]:	mock grad = -0.081993607752656184, computed grad = -0.081993624305332302
Iteration: 980, Checking gradient for output layer W[104][0]:	mock grad = -0.059810719024860903, computed grad = -0.059810718917857907
Iteration: 981, Checking gradient for words E[776][30]:	mock grad = -0.000536803102413330, computed grad = -0.000536803108656941
Iteration: 981, Checking gradient for hidden W[111][20]:	mock grad = -0.004981144019672312, computed grad = -0.004981144033766626
Iteration: 981, Checking gradient for hidden b[0][22]:	mock grad = 0.034557672323076716, computed grad = 0.034557675297259019
Iteration: 981, Checking gradient for output layer W[121][0]:	mock grad = 0.059916825396660878, computed grad = 0.059916825264949548
Iteration: 982, Checking gradient for words E[6][43]:	mock grad = 0.000605306076206791, computed grad = 0.000605306091886742
Iteration: 982, Checking gradient for hidden W[246][15]:	mock grad = 0.011637834198413577, computed grad = 0.011637834294378513
Iteration: 982, Checking gradient for hidden b[0][43]:	mock grad = -0.011197235137372363, computed grad = -0.011197238381242418
Iteration: 982, Checking gradient for output layer W[109][0]:	mock grad = 0.055853893272755917, computed grad = 0.055853893182998111
Iteration: 983, Checking gradient for words E[720][16]:	mock grad = -0.011012362286011435, computed grad = -0.014978420446897560
Iteration: 983, Checking gradient for hidden W[95][18]:	mock grad = 0.018005364452206019, computed grad = 0.018005364551051787
Iteration: 983, Checking gradient for hidden b[0][8]:	mock grad = 0.041098422176089056, computed grad = 0.041098451038444686
Iteration: 983, Checking gradient for output layer W[73][1]:	mock grad = 0.015452563990214774, computed grad = 0.015452563988900622
Iteration: 984, Checking gradient for words E[514][31]:	mock grad = 0.001699102935043273, computed grad = 0.001699102936755034
Iteration: 984, Checking gradient for hidden W[124][4]:	mock grad = -0.000570125216869943, computed grad = -0.000570125221004257
Iteration: 984, Checking gradient for hidden b[0][16]:	mock grad = 0.017195818944631958, computed grad = 0.017195824877826248
Iteration: 984, Checking gradient for output layer W[138][0]:	mock grad = 0.107284203340163309, computed grad = 0.107284202507403864
Iteration: 985, Checking gradient for words E[304][28]:	mock grad = 0.002248849696490085, computed grad = 0.002248849693435546
Iteration: 985, Checking gradient for hidden W[233][8]:	mock grad = -0.015355424131113793, computed grad = -0.015355424075655273
Iteration: 985, Checking gradient for hidden b[0][35]:	mock grad = -0.008867876279350373, computed grad = -0.008867873995751356
Iteration: 985, Checking gradient for output layer W[59][0]:	mock grad = 0.136289473048362142, computed grad = 0.136289471278399743
Iteration: 986, Checking gradient for words E[10][15]:	mock grad = -0.026118232495209170, computed grad = -0.026118232377983393
Iteration: 986, Checking gradient for hidden W[158][11]:	mock grad = -0.004428786760879433, computed grad = -0.004428786794291282
Iteration: 986, Checking gradient for hidden b[0][22]:	mock grad = 0.038328773424828322, computed grad = 0.038328778029548212
Iteration: 986, Checking gradient for output layer W[67][0]:	mock grad = -0.092558100680484312, computed grad = -0.092558100268623239
Iteration: 987, Checking gradient for words E[424][6]:	mock grad = -0.010535193033262003, computed grad = -0.010535192988403483
Iteration: 987, Checking gradient for hidden W[88][35]:	mock grad = -0.002910167820197040, computed grad = -0.002910167826996735
Iteration: 987, Checking gradient for hidden b[0][35]:	mock grad = 0.011560007933952843, computed grad = 0.011560007074658905
Iteration: 987, Checking gradient for output layer W[97][1]:	mock grad = 0.181911147436764065, computed grad = 0.181911144377885026
Iteration: 988, Checking gradient for words E[58][1]:	mock grad = -0.010625997248242536, computed grad = -0.010625997302020941
Iteration: 988, Checking gradient for hidden W[109][48]:	mock grad = 0.012718445916604093, computed grad = 0.012718445984648779
Iteration: 988, Checking gradient for hidden b[0][43]:	mock grad = 0.010788064581657153, computed grad = 0.010788067651240149
Iteration: 988, Checking gradient for output layer W[33][0]:	mock grad = 0.030074751711545167, computed grad = 0.030074751697388352
current: 90, Cost = 0.362897, Correct(%) = 1, time = 6.19728
Iteration: 989, Checking gradient for words E[166][15]:	mock grad = -0.012266224833246131, computed grad = -0.012266224918397533
Iteration: 989, Checking gradient for hidden W[214][18]:	mock grad = 0.006377168598525884, computed grad = 0.006377168516947763
Iteration: 989, Checking gradient for hidden b[0][9]:	mock grad = 0.057600493740289638, computed grad = 0.057600507288785859
Iteration: 989, Checking gradient for output layer W[85][0]:	mock grad = 0.202455682453167585, computed grad = 0.202455678388329841
Iteration: 990, Checking gradient for words E[472][18]:	mock grad = 0.008444751232628001, computed grad = 0.008444751253623999
Iteration: 990, Checking gradient for hidden W[111][9]:	mock grad = 0.006236183302726905, computed grad = 0.006236183277218599
Iteration: 990, Checking gradient for hidden b[0][31]:	mock grad = -0.034734783576628736, computed grad = -0.034734792561060462
Iteration: 990, Checking gradient for output layer W[38][0]:	mock grad = -0.006805168529772088, computed grad = -0.006805168529528454
Iteration: 991, Checking gradient for words E[124][44]:	mock grad = -0.000911169896117281, computed grad = -0.000911169928427521
Iteration: 991, Checking gradient for hidden W[133][20]:	mock grad = 0.013438240589302985, computed grad = 0.013438240786386597
Iteration: 991, Checking gradient for hidden b[0][41]:	mock grad = -0.036983199434847958, computed grad = -0.036983208235863214
Iteration: 991, Checking gradient for output layer W[4][0]:	mock grad = -0.016361019969712842, computed grad = -0.016361019967869560
Iteration: 992, Checking gradient for words E[736][8]:	mock grad = 0.013503987616070301, computed grad = 0.013503987670593184
Iteration: 992, Checking gradient for hidden W[95][14]:	mock grad = 0.009643180244045535, computed grad = 0.009643180376711337
Iteration: 992, Checking gradient for hidden b[0][40]:	mock grad = 0.084828077527326906, computed grad = 0.084828101681274984
Iteration: 992, Checking gradient for output layer W[143][1]:	mock grad = -0.155752676489900876, computed grad = -0.155752672767276862
Iteration: 993, Checking gradient for words E[156][7]:	mock grad = 0.004790059974307814, computed grad = 0.005786270703842957
Iteration: 993, Checking gradient for hidden W[149][37]:	mock grad = -0.005792061327669629, computed grad = -0.005792061377750136
Iteration: 993, Checking gradient for hidden b[0][37]:	mock grad = -0.053581297316085408, computed grad = -0.053581317396685271
Iteration: 993, Checking gradient for output layer W[75][1]:	mock grad = -0.177470096417803314, computed grad = -0.177470093577765664
Iteration: 994, Checking gradient for words E[345][47]:	mock grad = 0.004385382600585741, computed grad = 0.004385382572936545
Iteration: 994, Checking gradient for hidden W[217][27]:	mock grad = 0.001947424217912941, computed grad = 0.001947424209732030
Iteration: 994, Checking gradient for hidden b[0][40]:	mock grad = -0.077647902223337839, computed grad = -0.077647917654563192
Iteration: 994, Checking gradient for output layer W[63][0]:	mock grad = 0.051004867687581834, computed grad = 0.051004867558779553
Iteration: 995, Checking gradient for words E[198][9]:	mock grad = 0.004795399457580007, computed grad = 0.004795399511519571
Iteration: 995, Checking gradient for hidden W[27][31]:	mock grad = 0.010948197017385253, computed grad = 0.010948197073954391
Iteration: 995, Checking gradient for hidden b[0][27]:	mock grad = -0.038207137179974149, computed grad = -0.038207152033894083
Iteration: 995, Checking gradient for output layer W[23][1]:	mock grad = 0.081529482912706674, computed grad = 0.081529482714433901
Iteration: 996, Checking gradient for words E[683][13]:	mock grad = -0.009624836808080905, computed grad = -0.009624836832654840
Iteration: 996, Checking gradient for hidden W[82][42]:	mock grad = 0.013864527157514406, computed grad = 0.013864527235262353
Iteration: 996, Checking gradient for hidden b[0][32]:	mock grad = 0.052594054329502260, computed grad = 0.052594072112289836
Iteration: 996, Checking gradient for output layer W[91][1]:	mock grad = 0.093893141744127995, computed grad = 0.093893141247626086
Iteration: 997, Checking gradient for words E[776][30]:	mock grad = -0.001046202058124779, computed grad = -0.001046202050425750
Iteration: 997, Checking gradient for hidden W[167][22]:	mock grad = -0.006362973202944655, computed grad = -0.006362973235638570
Iteration: 997, Checking gradient for hidden b[0][42]:	mock grad = 0.086121833211522114, computed grad = 0.086121861448249057
Iteration: 997, Checking gradient for output layer W[124][0]:	mock grad = 0.026453822641409053, computed grad = 0.026453822638807360
Iteration: 998, Checking gradient for words E[42][21]:	mock grad = -0.004385403048839942, computed grad = -0.004385403077978893
Iteration: 998, Checking gradient for hidden W[214][33]:	mock grad = -0.004924335226613730, computed grad = -0.004924335242408586
Iteration: 998, Checking gradient for hidden b[0][48]:	mock grad = -0.068475727481781545, computed grad = -0.068475738011870221
Iteration: 998, Checking gradient for output layer W[138][0]:	mock grad = -0.071087759052346211, computed grad = -0.071087758792668404
current: 100, Cost = 0.368541, Correct(%) = 1, time = 6.98184
Iteration: 999, Checking gradient for words E[7][12]:	mock grad = -0.022438360036541871, computed grad = -0.022438360059688731
Iteration: 999, Checking gradient for hidden W[51][4]:	mock grad = -0.010916972104951794, computed grad = -0.010916972266223277
Iteration: 999, Checking gradient for hidden b[0][33]:	mock grad = -0.086964259400829080, computed grad = -0.086964270135057226
Iteration: 999, Checking gradient for output layer W[14][0]:	mock grad = 0.003477669279911000, computed grad = 0.003477669279941861
current: 10, Correct(%) = 1, time = 7.03285
Dev start.
Dev finished. Total time taken is: 0.634908
dev:
Accuracy:	P=100/100=1
Test start.
Test finished. Total time taken is: 0.544771
test:
Accuracy:	P=64/100=0.64
##### Iteration 10
random: 0, 99
Iteration: 1000, Checking gradient for words E[14][48]:	mock grad = 0.006035963330514038, computed grad = 0.006035963322216796
Iteration: 1000, Checking gradient for hidden W[45][49]:	mock grad = 0.004213963732763171, computed grad = 0.004213963729274687
Iteration: 1000, Checking gradient for hidden b[0][35]:	mock grad = 0.009485369242501207, computed grad = 0.009485369229239236
Iteration: 1000, Checking gradient for output layer W[16][0]:	mock grad = 0.034029587370087988, computed grad = 0.034029587316668046
Iteration: 1001, Checking gradient for words E[91][12]:	mock grad = 0.017766196434371162, computed grad = 0.017766196416081761
Iteration: 1001, Checking gradient for hidden W[115][5]:	mock grad = 0.000298945889126712, computed grad = 0.000298945808151218
Iteration: 1001, Checking gradient for hidden b[0][25]:	mock grad = 0.004469287931241350, computed grad = 0.004469284509691134
Iteration: 1001, Checking gradient for output layer W[92][1]:	mock grad = 0.119301116740799795, computed grad = 0.119301115600295909
Iteration: 1002, Checking gradient for words E[104][34]:	mock grad = 0.006504569648679848, computed grad = 0.007074682859901960
Iteration: 1002, Checking gradient for hidden W[211][14]:	mock grad = -0.002944153130790550, computed grad = -0.002944153097234524
Iteration: 1002, Checking gradient for hidden b[0][4]:	mock grad = -0.016017885873575999, computed grad = -0.016017891781382562
Iteration: 1002, Checking gradient for output layer W[22][1]:	mock grad = 0.035963520288728734, computed grad = 0.035963520249059612
Iteration: 1003, Checking gradient for words E[140][45]:	mock grad = 0.029194308373003963, computed grad = 0.029194308384583183
Iteration: 1003, Checking gradient for hidden W[84][28]:	mock grad = -0.000639994901463492, computed grad = -0.000639994859867361
Iteration: 1003, Checking gradient for hidden b[0][9]:	mock grad = 0.046972809375545044, computed grad = 0.046972810531588138
Iteration: 1003, Checking gradient for output layer W[110][1]:	mock grad = 0.130575909623342756, computed grad = 0.130575908816340736
Iteration: 1004, Checking gradient for words E[157][47]:	mock grad = 0.002253996265799696, computed grad = 0.002253996315274131
Iteration: 1004, Checking gradient for hidden W[225][7]:	mock grad = -0.004711828952119745, computed grad = -0.004711828962819046
Iteration: 1004, Checking gradient for hidden b[0][19]:	mock grad = 0.032836321129492418, computed grad = 0.032836314655544541
Iteration: 1004, Checking gradient for output layer W[142][1]:	mock grad = 0.127332668519497538, computed grad = 0.127332667469752553
Iteration: 1005, Checking gradient for words E[180][48]:	mock grad = -0.000110130093128324, computed grad = -0.000110130091336074
Iteration: 1005, Checking gradient for hidden W[218][43]:	mock grad = 0.001511152377342562, computed grad = 0.001511152387881341
Iteration: 1005, Checking gradient for hidden b[0][37]:	mock grad = -0.047141746200435319, computed grad = -0.047141759974990149
Iteration: 1005, Checking gradient for output layer W[133][1]:	mock grad = 0.132059712180832012, computed grad = 0.132059710820382453
Iteration: 1006, Checking gradient for words E[216][13]:	mock grad = -0.003757189674075123, computed grad = -0.003757189667247487
Iteration: 1006, Checking gradient for hidden W[149][35]:	mock grad = -0.000821217063906587, computed grad = -0.000821217094844619
Iteration: 1006, Checking gradient for hidden b[0][36]:	mock grad = -0.007704869002866932, computed grad = -0.007704881407796168
Iteration: 1006, Checking gradient for output layer W[94][1]:	mock grad = -0.148411360514771884, computed grad = -0.148411356457497617
Iteration: 1007, Checking gradient for words E[69][19]:	mock grad = 0.026917881848192993, computed grad = 0.026917881907236062
Iteration: 1007, Checking gradient for hidden W[7][43]:	mock grad = 0.002864466552093825, computed grad = 0.002864466587459914
Iteration: 1007, Checking gradient for hidden b[0][45]:	mock grad = -0.013321502375596150, computed grad = -0.013321506626556337
Iteration: 1007, Checking gradient for output layer W[105][0]:	mock grad = 0.048608579863551604, computed grad = 0.048608579847449047
Iteration: 1008, Checking gradient for words E[288][42]:	mock grad = 0.020177120682113303, computed grad = 0.020177120703306822
Iteration: 1008, Checking gradient for hidden W[5][49]:	mock grad = -0.015535095350127648, computed grad = -0.015535095470840731
Iteration: 1008, Checking gradient for hidden b[0][5]:	mock grad = 0.014104069143899833, computed grad = 0.014104068199671327
Iteration: 1008, Checking gradient for output layer W[42][1]:	mock grad = 0.037597691774859943, computed grad = 0.037597691736525184
current: 10, Cost = 0.360802, Correct(%) = 1, time = 0.770754
Iteration: 1009, Checking gradient for words E[148][9]:	mock grad = 0.007960989343969160, computed grad = 0.007960989312744752
Iteration: 1009, Checking gradient for hidden W[171][0]:	mock grad = -0.000241192551320157, computed grad = -0.000241192554484196
Iteration: 1009, Checking gradient for hidden b[0][20]:	mock grad = 0.089252319138533043, computed grad = 0.089252332682309449
Iteration: 1009, Checking gradient for output layer W[81][1]:	mock grad = 0.049549880875604524, computed grad = 0.049549880814803132
Iteration: 1010, Checking gradient for words E[309][13]:	mock grad = -0.014762227688730478, computed grad = -0.014762227684568420
Iteration: 1010, Checking gradient for hidden W[78][36]:	mock grad = -0.009552945959168113, computed grad = -0.009552946000851570
Iteration: 1010, Checking gradient for hidden b[0][21]:	mock grad = 0.042871625017648629, computed grad = 0.042871635297898308
Iteration: 1010, Checking gradient for output layer W[122][1]:	mock grad = 0.083233956897216244, computed grad = 0.083233956297639272
Iteration: 1011, Checking gradient for words E[73][1]:	mock grad = -0.007094246880445798, computed grad = -0.007094246895413232
Iteration: 1011, Checking gradient for hidden W[79][37]:	mock grad = -0.019490462836030398, computed grad = -0.019490463064113703
Iteration: 1011, Checking gradient for hidden b[0][36]:	mock grad = -0.014525963232925321, computed grad = -0.014525965198540696
Iteration: 1011, Checking gradient for output layer W[92][1]:	mock grad = 0.035621356761772471, computed grad = 0.035621356748965549
Iteration: 1012, Checking gradient for words E[348][6]:	mock grad = 0.010939590259662069, computed grad = 0.010939590316281483
Iteration: 1012, Checking gradient for hidden W[249][2]:	mock grad = 0.006407774232747299, computed grad = 0.006407774299052776
Iteration: 1012, Checking gradient for hidden b[0][12]:	mock grad = -0.060568566855934147, computed grad = -0.060568586794848438
Iteration: 1012, Checking gradient for output layer W[42][0]:	mock grad = -0.029762708146363615, computed grad = -0.029762708124106932
Iteration: 1013, Checking gradient for words E[28][36]:	mock grad = 0.012822590240629816, computed grad = 0.012822590302831305
Iteration: 1013, Checking gradient for hidden W[0][11]:	mock grad = -0.000855471200211788, computed grad = -0.000855471198001821
Iteration: 1013, Checking gradient for hidden b[0][47]:	mock grad = -0.058475965495713433, computed grad = -0.058475971634232092
Iteration: 1013, Checking gradient for output layer W[117][0]:	mock grad = 0.131469121282895562, computed grad = 0.131469119419580532
Iteration: 1014, Checking gradient for words E[69][20]:	mock grad = 0.041104815889536628, computed grad = 0.041104815913536902
Iteration: 1014, Checking gradient for hidden W[75][39]:	mock grad = 0.009346452429559449, computed grad = 0.009346452486728768
Iteration: 1014, Checking gradient for hidden b[0][24]:	mock grad = 0.125452527633462818, computed grad = 0.125452559618126569
Iteration: 1014, Checking gradient for output layer W[75][1]:	mock grad = -0.207885720048706091, computed grad = -0.207885719253590978
Iteration: 1015, Checking gradient for words E[382][28]:	mock grad = 0.010286602511844212, computed grad = 0.010169198038993525
Iteration: 1015, Checking gradient for hidden W[98][11]:	mock grad = -0.000670113602607714, computed grad = -0.000670113573354589
Iteration: 1015, Checking gradient for hidden b[0][13]:	mock grad = -0.121519629349559066, computed grad = -0.121519666061565973
Iteration: 1015, Checking gradient for output layer W[108][1]:	mock grad = -0.078183424703143212, computed grad = -0.078183424451507763
Iteration: 1016, Checking gradient for words E[396][15]:	mock grad = -0.041856850876514917, computed grad = -0.041856850814350526
Iteration: 1016, Checking gradient for hidden W[148][47]:	mock grad = 0.011856939322069104, computed grad = 0.011856939433254700
Iteration: 1016, Checking gradient for hidden b[0][47]:	mock grad = 0.079553368573070937, computed grad = 0.079553388950273779
Iteration: 1016, Checking gradient for output layer W[32][1]:	mock grad = -0.103905887981259326, computed grad = -0.103905887497686977
Iteration: 1017, Checking gradient for words E[402][7]:	mock grad = -0.014130394357242393, computed grad = -0.014130394332498382
Iteration: 1017, Checking gradient for hidden W[76][48]:	mock grad = 0.001512237427864216, computed grad = 0.001863698036153122
Iteration: 1017, Checking gradient for hidden b[0][40]:	mock grad = -0.072887739516608319, computed grad = -0.072887755351824263
Iteration: 1017, Checking gradient for output layer W[47][1]:	mock grad = -0.040320869213905652, computed grad = -0.040320869126849193
Iteration: 1018, Checking gradient for words E[152][43]:	mock grad = 0.000287013008887227, computed grad = 0.000287013022960064
Iteration: 1018, Checking gradient for hidden W[102][42]:	mock grad = -0.012258345061966214, computed grad = -0.012258345155590766
Iteration: 1018, Checking gradient for hidden b[0][19]:	mock grad = 0.030664820873077536, computed grad = 0.030664817626098566
Iteration: 1018, Checking gradient for output layer W[118][1]:	mock grad = 0.026266707405381284, computed grad = 0.026266707387810884
current: 20, Cost = 0.45314, Correct(%) = 1, time = 1.43333
Iteration: 1019, Checking gradient for words E[102][18]:	mock grad = 0.047086521923700442, computed grad = 0.047086522023163574
Iteration: 1019, Checking gradient for hidden W[141][9]:	mock grad = -0.006087358632916873, computed grad = -0.006087358635221697
Iteration: 1019, Checking gradient for hidden b[0][2]:	mock grad = 0.066448751696229769, computed grad = 0.066448786216396272
Iteration: 1019, Checking gradient for output layer W[70][0]:	mock grad = -0.089506227625962831, computed grad = -0.089506227470711699
Iteration: 1020, Checking gradient for words E[442][24]:	mock grad = 0.021884134774857733, computed grad = 0.021884134830933859
Iteration: 1020, Checking gradient for hidden W[189][20]:	mock grad = -0.008382651917077100, computed grad = -0.008382652026790867
Iteration: 1020, Checking gradient for hidden b[0][7]:	mock grad = -0.067870409638348672, computed grad = -0.067870416948674961
Iteration: 1020, Checking gradient for output layer W[141][1]:	mock grad = 0.113559789591516358, computed grad = 0.113559788646792767
Iteration: 1021, Checking gradient for words E[284][38]:	mock grad = 0.061426514725404502, computed grad = 0.061426514555224229
Iteration: 1021, Checking gradient for hidden W[36][7]:	mock grad = 0.002415058796750857, computed grad = 0.002415058798824825
Iteration: 1021, Checking gradient for hidden b[0][22]:	mock grad = -0.055017451606520495, computed grad = -0.055017465925892947
Iteration: 1021, Checking gradient for output layer W[90][0]:	mock grad = 0.060799162746316693, computed grad = 0.060799162708807156
Iteration: 1022, Checking gradient for words E[377][21]:	mock grad = -0.015922525331529824, computed grad = -0.015922525423499388
Iteration: 1022, Checking gradient for hidden W[161][2]:	mock grad = -0.013320912650632843, computed grad = -0.013320912593617023
Iteration: 1022, Checking gradient for hidden b[0][30]:	mock grad = -0.002189809854641345, computed grad = -0.002189814298925329
Iteration: 1022, Checking gradient for output layer W[123][0]:	mock grad = -0.143676182635554817, computed grad = -0.143676180069213039
Iteration: 1023, Checking gradient for words E[457][44]:	mock grad = 0.033454532240156931, computed grad = 0.033454532299225445
Iteration: 1023, Checking gradient for hidden W[228][21]:	mock grad = -0.000662163685188899, computed grad = -0.000662163688749960
Iteration: 1023, Checking gradient for hidden b[0][3]:	mock grad = 0.031271634413798655, computed grad = 0.031271635008645576
Iteration: 1023, Checking gradient for output layer W[3][1]:	mock grad = 0.010595629271886731, computed grad = 0.010595629271172215
Iteration: 1024, Checking gradient for words E[58][18]:	mock grad = 0.015395173873944401, computed grad = 0.015395173944995347
Iteration: 1024, Checking gradient for hidden W[201][31]:	mock grad = -0.005340871028519656, computed grad = -0.005340871031158084
Iteration: 1024, Checking gradient for hidden b[0][2]:	mock grad = 0.054261462635457614, computed grad = 0.054261490166969385
Iteration: 1024, Checking gradient for output layer W[61][0]:	mock grad = -0.148793596525698124, computed grad = -0.148793594963886361
Iteration: 1025, Checking gradient for words E[306][3]:	mock grad = -0.000824081507672103, computed grad = -0.000824081539916533
Iteration: 1025, Checking gradient for hidden W[163][34]:	mock grad = 0.004683636489616338, computed grad = 0.004683636497752333
Iteration: 1025, Checking gradient for hidden b[0][44]:	mock grad = 0.008607939332855308, computed grad = 0.008607942607671731
Iteration: 1025, Checking gradient for output layer W[39][1]:	mock grad = 0.050719813980526496, computed grad = 0.050719813915491900
Iteration: 1026, Checking gradient for words E[469][28]:	mock grad = -0.004505123621573182, computed grad = -0.004505123655305039
Iteration: 1026, Checking gradient for hidden W[8][21]:	mock grad = -0.006550805489224931, computed grad = -0.006550805538875882
Iteration: 1026, Checking gradient for hidden b[0][24]:	mock grad = 0.067273805872680725, computed grad = 0.067273822717539219
Iteration: 1026, Checking gradient for output layer W[32][0]:	mock grad = 0.073897775229314622, computed grad = 0.073897774744291947
Iteration: 1027, Checking gradient for words E[487][7]:	mock grad = -0.016284242644881708, computed grad = -0.016284242698746461
Iteration: 1027, Checking gradient for hidden W[27][48]:	mock grad = 0.016768745093709292, computed grad = 0.016768745105086039
Iteration: 1027, Checking gradient for hidden b[0][37]:	mock grad = 0.059675737071163137, computed grad = 0.059675745649549611
Iteration: 1027, Checking gradient for output layer W[70][0]:	mock grad = -0.102025557625312624, computed grad = -0.102025557432547834
Iteration: 1028, Checking gradient for words E[489][39]:	mock grad = -0.005576692596737098, computed grad = -0.005576692600056905
Iteration: 1028, Checking gradient for hidden W[168][24]:	mock grad = 0.003685562867256076, computed grad = 0.003685562862803254
Iteration: 1028, Checking gradient for hidden b[0][14]:	mock grad = -0.040074827594077655, computed grad = -0.040074844166316792
Iteration: 1028, Checking gradient for output layer W[44][0]:	mock grad = -0.009258462524586175, computed grad = -0.009258462523759918
current: 30, Cost = 0.331051, Correct(%) = 1, time = 2.12504
Iteration: 1029, Checking gradient for words E[255][33]:	mock grad = -0.009881599160016519, computed grad = -0.009881599218028917
Iteration: 1029, Checking gradient for hidden W[204][34]:	mock grad = 0.014326091821281750, computed grad = 0.014326092002790497
Iteration: 1029, Checking gradient for hidden b[0][13]:	mock grad = 0.106068778492424132, computed grad = 0.106068802640823037
Iteration: 1029, Checking gradient for output layer W[17][1]:	mock grad = 0.025789426754574407, computed grad = 0.025789426743287415
Iteration: 1030, Checking gradient for words E[512][49]:	mock grad = -0.010190482065158823, computed grad = -0.013910635737425021
Iteration: 1030, Checking gradient for hidden W[108][35]:	mock grad = -0.002130568832137136, computed grad = -0.002130568861928547
Iteration: 1030, Checking gradient for hidden b[0][32]:	mock grad = 0.049730506554240828, computed grad = 0.049730523357550196
Iteration: 1030, Checking gradient for output layer W[82][1]:	mock grad = 0.165602217568922017, computed grad = 0.165602214328831404
Iteration: 1031, Checking gradient for words E[102][21]:	mock grad = 0.019342176937081623, computed grad = 0.019342177061122186
Iteration: 1031, Checking gradient for hidden W[199][48]:	mock grad = 0.014054969525251204, computed grad = 0.014054969644573465
Iteration: 1031, Checking gradient for hidden b[0][11]:	mock grad = -0.107634325687583354, computed grad = -0.107634357066253114
Iteration: 1031, Checking gradient for output layer W[28][1]:	mock grad = 0.076174568988285341, computed grad = 0.076174568894355615
Iteration: 1032, Checking gradient for words E[515][31]:	mock grad = 0.002708380998472926, computed grad = 0.002397021750974209
Iteration: 1032, Checking gradient for hidden W[128][27]:	mock grad = 0.000897277059735968, computed grad = 0.000897277032366714
Iteration: 1032, Checking gradient for hidden b[0][2]:	mock grad = 0.049988776074927355, computed grad = 0.049988802244364715
Iteration: 1032, Checking gradient for output layer W[120][1]:	mock grad = -0.054147779249308359, computed grad = -0.054147779135860274
Iteration: 1033, Checking gradient for words E[215][36]:	mock grad = -0.003574493429414627, computed grad = -0.003574493414408905
Iteration: 1033, Checking gradient for hidden W[238][14]:	mock grad = -0.015489842794841335, computed grad = -0.015489842800033670
Iteration: 1033, Checking gradient for hidden b[0][5]:	mock grad = 0.009632856412644664, computed grad = 0.009632852386682199
Iteration: 1033, Checking gradient for output layer W[140][1]:	mock grad = 0.120337511276741305, computed grad = 0.120337509657125405
Iteration: 1034, Checking gradient for words E[68][19]:	mock grad = -0.024162451399356577, computed grad = -0.024162451567741557
Iteration: 1034, Checking gradient for hidden W[33][1]:	mock grad = 0.000784785005258914, computed grad = 0.000784785024263769
Iteration: 1034, Checking gradient for hidden b[0][44]:	mock grad = 0.004743220725633712, computed grad = 0.004743217592385092
Iteration: 1034, Checking gradient for output layer W[52][1]:	mock grad = -0.020849904469211200, computed grad = -0.020849904466541183
Iteration: 1035, Checking gradient for words E[530][48]:	mock grad = 0.022612415052747359, computed grad = 0.022612415113655224
Iteration: 1035, Checking gradient for hidden W[69][11]:	mock grad = -0.000154258693402243, computed grad = -0.000154258722961905
Iteration: 1035, Checking gradient for hidden b[0][48]:	mock grad = 0.080269226932633275, computed grad = 0.080269221285680592
Iteration: 1035, Checking gradient for output layer W[95][0]:	mock grad = -0.023673986851824314, computed grad = -0.023673986847487036
Iteration: 1036, Checking gradient for words E[156][32]:	mock grad = 0.011718446157438223, computed grad = 0.011718446155387176
Iteration: 1036, Checking gradient for hidden W[212][16]:	mock grad = 0.001994979371922057, computed grad = 0.001994979382303627
Iteration: 1036, Checking gradient for hidden b[0][33]:	mock grad = -0.127549296298323789, computed grad = -0.127549331244175390
Iteration: 1036, Checking gradient for output layer W[77][1]:	mock grad = 0.016308692536326497, computed grad = 0.016308692535443373
Iteration: 1037, Checking gradient for words E[42][30]:	mock grad = -0.009462803897714034, computed grad = -0.009462803941088701
Iteration: 1037, Checking gradient for hidden W[139][47]:	mock grad = 0.001186094789357162, computed grad = 0.001186094900139194
Iteration: 1037, Checking gradient for hidden b[0][45]:	mock grad = 0.008263937185032377, computed grad = 0.008263938412722877
Iteration: 1037, Checking gradient for output layer W[104][1]:	mock grad = 0.080650065200277288, computed grad = 0.080650064910666674
Iteration: 1038, Checking gradient for words E[197][16]:	mock grad = 0.016396545977542809, computed grad = 0.016396546053832228
Iteration: 1038, Checking gradient for hidden W[83][22]:	mock grad = 0.000650536776697352, computed grad = 0.000650536791799529
Iteration: 1038, Checking gradient for hidden b[0][22]:	mock grad = 0.050426171975481715, computed grad = 0.050426184910971325
Iteration: 1038, Checking gradient for output layer W[128][0]:	mock grad = 0.034135211734370730, computed grad = 0.034135211722175339
current: 40, Cost = 0.276195, Correct(%) = 1, time = 2.5594
Iteration: 1039, Checking gradient for words E[25][32]:	mock grad = 0.004018659875121289, computed grad = 0.004018659871071618
Iteration: 1039, Checking gradient for hidden W[144][35]:	mock grad = -0.000959467904099176, computed grad = -0.000959467890203759
Iteration: 1039, Checking gradient for hidden b[0][30]:	mock grad = -0.003982930098761495, computed grad = -0.003982936069647301
Iteration: 1039, Checking gradient for output layer W[10][0]:	mock grad = -0.020364454881877103, computed grad = -0.020364454872389859
Iteration: 1040, Checking gradient for words E[172][12]:	mock grad = 0.013577744196996422, computed grad = 0.013577744201974207
Iteration: 1040, Checking gradient for hidden W[208][44]:	mock grad = 0.002783888363305786, computed grad = 0.002783888363837786
Iteration: 1040, Checking gradient for hidden b[0][41]:	mock grad = 0.027391081338479228, computed grad = 0.027391087128021820
Iteration: 1040, Checking gradient for output layer W[96][1]:	mock grad = -0.148166936413812733, computed grad = -0.148166932933104206
Iteration: 1041, Checking gradient for words E[24][34]:	mock grad = -0.018671527996005910, computed grad = -0.018582078179733470
Iteration: 1041, Checking gradient for hidden W[94][33]:	mock grad = 0.011544308562816408, computed grad = 0.011544308709903297
Iteration: 1041, Checking gradient for hidden b[0][23]:	mock grad = 0.173107605334371639, computed grad = 0.173107643460159805
Iteration: 1041, Checking gradient for output layer W[109][0]:	mock grad = 0.046356138245579048, computed grad = 0.046356138228708800
Iteration: 1042, Checking gradient for words E[34][23]:	mock grad = 0.063955349798666683, computed grad = 0.063955349856313168
Iteration: 1042, Checking gradient for hidden W[27][34]:	mock grad = -0.013904909954287614, computed grad = -0.013904910282170512
Iteration: 1042, Checking gradient for hidden b[0][12]:	mock grad = 0.080505635448674395, computed grad = 0.080505659812691185
Iteration: 1042, Checking gradient for output layer W[141][1]:	mock grad = -0.101345234874650192, computed grad = -0.101345234606604592
Iteration: 1043, Checking gradient for words E[564][31]:	mock grad = -0.005276665204279052, computed grad = -0.002384731704254988
Iteration: 1043, Checking gradient for hidden W[142][4]:	mock grad = -0.004143479929052951, computed grad = -0.004143479958580312
Iteration: 1043, Checking gradient for hidden b[0][1]:	mock grad = -0.123307833409580514, computed grad = -0.123307854956469856
Iteration: 1043, Checking gradient for output layer W[75][0]:	mock grad = -0.164857045427291293, computed grad = -0.164857044666150115
Iteration: 1044, Checking gradient for words E[152][33]:	mock grad = 0.000402441055494251, computed grad = 0.000402441056039036
Iteration: 1044, Checking gradient for hidden W[84][34]:	mock grad = 0.003343252151857823, computed grad = 0.003343252159166310
Iteration: 1044, Checking gradient for hidden b[0][42]:	mock grad = 0.044417797772158818, computed grad = 0.044417810834132723
Iteration: 1044, Checking gradient for output layer W[105][1]:	mock grad = 0.077035153231486131, computed grad = 0.077035152511291796
Iteration: 1045, Checking gradient for words E[222][5]:	mock grad = -0.001361169268049300, computed grad = -0.001361169321935723
Iteration: 1045, Checking gradient for hidden W[148][10]:	mock grad = -0.006861763784643538, computed grad = -0.006861763751543629
Iteration: 1045, Checking gradient for hidden b[0][16]:	mock grad = 0.019778022763955461, computed grad = 0.019778029237314424
Iteration: 1045, Checking gradient for output layer W[43][1]:	mock grad = -0.066495492916540666, computed grad = -0.066495492816368310
Iteration: 1046, Checking gradient for words E[78][22]:	mock grad = 0.000927960076946288, computed grad = 0.000927960073951331
Iteration: 1046, Checking gradient for hidden W[132][0]:	mock grad = -0.009798244356096220, computed grad = -0.009798244370472246
Iteration: 1046, Checking gradient for hidden b[0][9]:	mock grad = -0.034571817892570245, computed grad = -0.034571813833168735
Iteration: 1046, Checking gradient for output layer W[82][0]:	mock grad = -0.157364500232515248, computed grad = -0.157364495838437285
Iteration: 1047, Checking gradient for words E[447][29]:	mock grad = 0.012649381960466721, computed grad = 0.011675062855228324
Iteration: 1047, Checking gradient for hidden W[33][18]:	mock grad = -0.010519212883058904, computed grad = -0.010519212843946427
Iteration: 1047, Checking gradient for hidden b[0][33]:	mock grad = 0.080823210884234387, computed grad = 0.080823226318456676
Iteration: 1047, Checking gradient for output layer W[125][0]:	mock grad = -0.079411317638000689, computed grad = -0.079411317247500640
Iteration: 1048, Checking gradient for words E[403][34]:	mock grad = -0.003874829141964353, computed grad = -0.003874829152964448
Iteration: 1048, Checking gradient for hidden W[169][3]:	mock grad = 0.004188975358809799, computed grad = 0.004188975363544815
Iteration: 1048, Checking gradient for hidden b[0][2]:	mock grad = 0.046612979856752723, computed grad = 0.046613005472856094
Iteration: 1048, Checking gradient for output layer W[82][0]:	mock grad = -0.149513392842304249, computed grad = -0.149513389632185800
current: 50, Cost = 0.279261, Correct(%) = 1, time = 3.44519
Iteration: 1049, Checking gradient for words E[585][42]:	mock grad = -0.001178241708588157, computed grad = -0.001178241694109810
Iteration: 1049, Checking gradient for hidden W[21][6]:	mock grad = 0.000059378674033272, computed grad = 0.000059378692745971
Iteration: 1049, Checking gradient for hidden b[0][48]:	mock grad = 0.057550992346583341, computed grad = 0.057550987434401776
Iteration: 1049, Checking gradient for output layer W[13][0]:	mock grad = 0.039760364183794028, computed grad = 0.039760364115342887
Iteration: 1050, Checking gradient for words E[381][13]:	mock grad = 0.003385040703268727, computed grad = 0.003385040749523489
Iteration: 1050, Checking gradient for hidden W[54][13]:	mock grad = -0.003587218674300230, computed grad = -0.003587218678890590
Iteration: 1050, Checking gradient for hidden b[0][28]:	mock grad = 0.024968986895718137, computed grad = 0.024969012696385757
Iteration: 1050, Checking gradient for output layer W[109][1]:	mock grad = -0.020140177083155519, computed grad = -0.020140177081528165
Iteration: 1051, Checking gradient for words E[102][29]:	mock grad = -0.016421473722189051, computed grad = -0.016421473778317847
Iteration: 1051, Checking gradient for hidden W[104][16]:	mock grad = -0.002028699078837626, computed grad = -0.002028699073286374
Iteration: 1051, Checking gradient for hidden b[0][4]:	mock grad = -0.019728734822921368, computed grad = -0.019728742905940327
Iteration: 1051, Checking gradient for output layer W[36][1]:	mock grad = 0.016845709585100455, computed grad = 0.016845709582777001
Iteration: 1052, Checking gradient for words E[133][12]:	mock grad = -0.014877493774212436, computed grad = -0.014877493824595574
Iteration: 1052, Checking gradient for hidden W[200][10]:	mock grad = 0.008948847659118186, computed grad = 0.008948847759985596
Iteration: 1052, Checking gradient for hidden b[0][23]:	mock grad = -0.149922728738594024, computed grad = -0.149922776759549070
Iteration: 1052, Checking gradient for output layer W[102][1]:	mock grad = 0.177941462109559856, computed grad = 0.177941459978748218
Iteration: 1053, Checking gradient for words E[421][39]:	mock grad = -0.012673110543426347, computed grad = -0.012673110568595339
Iteration: 1053, Checking gradient for hidden W[11][33]:	mock grad = -0.002741893869007139, computed grad = -0.002741893877567237
Iteration: 1053, Checking gradient for hidden b[0][34]:	mock grad = -0.012656273486538749, computed grad = -0.012656265733405893
Iteration: 1053, Checking gradient for output layer W[22][1]:	mock grad = -0.040398303021355053, computed grad = -0.040398302977262004
Iteration: 1054, Checking gradient for words E[60][36]:	mock grad = 0.004016440611737115, computed grad = 0.004016440607580623
Iteration: 1054, Checking gradient for hidden W[29][36]:	mock grad = 0.001462846783290006, computed grad = 0.001462846739004861
Iteration: 1054, Checking gradient for hidden b[0][15]:	mock grad = 0.070365649078418802, computed grad = 0.070365661944977953
Iteration: 1054, Checking gradient for output layer W[74][1]:	mock grad = -0.087992282996801618, computed grad = -0.087992282207603717
Iteration: 1055, Checking gradient for words E[156][19]:	mock grad = -0.007581100531117713, computed grad = -0.007581100526373639
Iteration: 1055, Checking gradient for hidden W[140][44]:	mock grad = 0.002433297411286173, computed grad = 0.002433297416458164
Iteration: 1055, Checking gradient for hidden b[0][43]:	mock grad = 0.009572439026772628, computed grad = 0.009572441837477557
Iteration: 1055, Checking gradient for output layer W[77][1]:	mock grad = -0.060660676532264812, computed grad = -0.060660676322420913
Iteration: 1056, Checking gradient for words E[49][30]:	mock grad = 0.030661415551136439, computed grad = 0.030661415639195541
Iteration: 1056, Checking gradient for hidden W[245][49]:	mock grad = 0.026768669515425669, computed grad = 0.026768669687046012
Iteration: 1056, Checking gradient for hidden b[0][16]:	mock grad = -0.022950063668453202, computed grad = -0.022950071697806232
Iteration: 1056, Checking gradient for output layer W[105][1]:	mock grad = 0.029714260792845781, computed grad = 0.029714260788806662
Iteration: 1057, Checking gradient for words E[10][46]:	mock grad = -0.014839056886067903, computed grad = -0.014839056890788904
Iteration: 1057, Checking gradient for hidden W[236][33]:	mock grad = -0.003660553250073306, computed grad = -0.003660553256355946
Iteration: 1057, Checking gradient for hidden b[0][3]:	mock grad = -0.036915443712121077, computed grad = -0.036915455328584167
Iteration: 1057, Checking gradient for output layer W[39][0]:	mock grad = -0.050657955284277856, computed grad = -0.050657955198755537
Iteration: 1058, Checking gradient for words E[102][21]:	mock grad = -0.019056489169444468, computed grad = -0.019056489225363373
Iteration: 1058, Checking gradient for hidden W[133][5]:	mock grad = -0.009811322185721005, computed grad = -0.009811322202591654
Iteration: 1058, Checking gradient for hidden b[0][12]:	mock grad = -0.073339012472029008, computed grad = -0.073339035941133085
Iteration: 1058, Checking gradient for output layer W[107][1]:	mock grad = 0.097576910466551370, computed grad = 0.097576910073322073
current: 60, Cost = 0.358361, Correct(%) = 1, time = 4.1404
Iteration: 1059, Checking gradient for words E[560][22]:	mock grad = -0.048742305242571726, computed grad = -0.049023550447005536
Iteration: 1059, Checking gradient for hidden W[239][40]:	mock grad = 0.004762358468191064, computed grad = 0.004762358485582320
Iteration: 1059, Checking gradient for hidden b[0][9]:	mock grad = -0.047444063846990847, computed grad = -0.047444062104888649
Iteration: 1059, Checking gradient for output layer W[48][0]:	mock grad = 0.084098156722750561, computed grad = 0.084098156419111697
Iteration: 1060, Checking gradient for words E[119][25]:	mock grad = -0.048023353270087998, computed grad = -0.048023353393043117
Iteration: 1060, Checking gradient for hidden W[163][43]:	mock grad = -0.003332727988664130, computed grad = -0.003332728072466165
Iteration: 1060, Checking gradient for hidden b[0][14]:	mock grad = -0.056708234856706063, computed grad = -0.056708257137207348
Iteration: 1060, Checking gradient for output layer W[50][1]:	mock grad = 0.184549718532323048, computed grad = 0.184549716523040364
Iteration: 1061, Checking gradient for words E[456][44]:	mock grad = 0.004153171914433873, computed grad = 0.004153171914430985
Iteration: 1061, Checking gradient for hidden W[34][44]:	mock grad = 0.002398538407444306, computed grad = 0.002398538354897465
Iteration: 1061, Checking gradient for hidden b[0][48]:	mock grad = -0.088233390494241171, computed grad = -0.088233405990870609
Iteration: 1061, Checking gradient for output layer W[133][0]:	mock grad = -0.098887670998498711, computed grad = -0.098887670693807800
Iteration: 1062, Checking gradient for words E[400][32]:	mock grad = 0.021140002178066108, computed grad = 0.021140002204224483
Iteration: 1062, Checking gradient for hidden W[178][28]:	mock grad = 0.012049579598494287, computed grad = 0.012049579614030376
Iteration: 1062, Checking gradient for hidden b[0][0]:	mock grad = -0.156685341531931366, computed grad = -0.156685375947534761
Iteration: 1062, Checking gradient for output layer W[42][0]:	mock grad = 0.041921453282967036, computed grad = 0.041921453246444439
Iteration: 1063, Checking gradient for words E[661][0]:	mock grad = 0.007075024696073795, computed grad = 0.007075024776396772
Iteration: 1063, Checking gradient for hidden W[25][2]:	mock grad = -0.015412569099454432, computed grad = -0.015412569189079187
Iteration: 1063, Checking gradient for hidden b[0][32]:	mock grad = -0.056018985130829035, computed grad = -0.056019002751244169
Iteration: 1063, Checking gradient for output layer W[53][0]:	mock grad = 0.122556917521110531, computed grad = 0.122556916699949545
Iteration: 1064, Checking gradient for words E[18][5]:	mock grad = -0.003810312015406492, computed grad = -0.003810312048740306
Iteration: 1064, Checking gradient for hidden W[176][12]:	mock grad = -0.001658983034530381, computed grad = -0.001658983047571034
Iteration: 1064, Checking gradient for hidden b[0][35]:	mock grad = -0.011745318929140547, computed grad = -0.011745316729286919
Iteration: 1064, Checking gradient for output layer W[37][1]:	mock grad = 0.049931015505377818, computed grad = 0.049931015463305833
Iteration: 1065, Checking gradient for words E[3][28]:	mock grad = -0.013322944596855235, computed grad = -0.013322944650960868
Iteration: 1065, Checking gradient for hidden W[0][26]:	mock grad = -0.007164987249508314, computed grad = -0.007164987281071783
Iteration: 1065, Checking gradient for hidden b[0][32]:	mock grad = 0.036026912599046446, computed grad = 0.036026918272099723
Iteration: 1065, Checking gradient for output layer W[112][0]:	mock grad = 0.072549594706095011, computed grad = 0.072549594179832741
Iteration: 1066, Checking gradient for words E[102][12]:	mock grad = 0.001063621807489978, computed grad = 0.001063621782329898
Iteration: 1066, Checking gradient for hidden W[176][18]:	mock grad = -0.013773193323990407, computed grad = -0.013773193826581126
Iteration: 1066, Checking gradient for hidden b[0][18]:	mock grad = 0.052896515878086969, computed grad = 0.052896519439864211
Iteration: 1066, Checking gradient for output layer W[101][1]:	mock grad = -0.134350117167447713, computed grad = -0.134350115507549245
Iteration: 1067, Checking gradient for words E[672][43]:	mock grad = -0.036037763553292690, computed grad = -0.036037763632282685
Iteration: 1067, Checking gradient for hidden W[51][48]:	mock grad = 0.014527605772474139, computed grad = 0.014527605858380296
Iteration: 1067, Checking gradient for hidden b[0][47]:	mock grad = 0.064641419055905258, computed grad = 0.064641425306743425
Iteration: 1067, Checking gradient for output layer W[90][0]:	mock grad = 0.068442620222736172, computed grad = 0.068442620070369609
Iteration: 1068, Checking gradient for words E[474][1]:	mock grad = -0.010133352563085340, computed grad = -0.010133352535176778
Iteration: 1068, Checking gradient for hidden W[28][34]:	mock grad = -0.012973084748929553, computed grad = -0.012973084956040083
Iteration: 1068, Checking gradient for hidden b[0][30]:	mock grad = 0.002702556740979967, computed grad = 0.002702557147526936
Iteration: 1068, Checking gradient for output layer W[71][0]:	mock grad = 0.109876641181061441, computed grad = 0.109876640033957459
current: 70, Cost = 0.274813, Correct(%) = 1, time = 4.75391
Iteration: 1069, Checking gradient for words E[681][6]:	mock grad = -0.013200589393802531, computed grad = -0.013200589427464958
Iteration: 1069, Checking gradient for hidden W[5][15]:	mock grad = 0.002532645143132894, computed grad = 0.002532645146441754
Iteration: 1069, Checking gradient for hidden b[0][48]:	mock grad = -0.072264502032765021, computed grad = -0.072264523307011344
Iteration: 1069, Checking gradient for output layer W[77][0]:	mock grad = 0.090134494604376947, computed grad = 0.090134493770274882
Iteration: 1070, Checking gradient for words E[448][6]:	mock grad = 0.018067435957475064, computed grad = 0.018067436002229910
Iteration: 1070, Checking gradient for hidden W[249][25]:	mock grad = 0.000241361135383533, computed grad = 0.000241361130552869
Iteration: 1070, Checking gradient for hidden b[0][22]:	mock grad = -0.045065729986609737, computed grad = -0.045065737607697234
Iteration: 1070, Checking gradient for output layer W[26][1]:	mock grad = 0.058353909053010522, computed grad = 0.058353908988489814
Iteration: 1071, Checking gradient for words E[69][39]:	mock grad = -0.030321995531851220, computed grad = -0.030321995612389387
Iteration: 1071, Checking gradient for hidden W[19][24]:	mock grad = -0.006515825789893404, computed grad = -0.006515825789792069
Iteration: 1071, Checking gradient for hidden b[0][39]:	mock grad = -0.092048043299541860, computed grad = -0.092048081676496168
Iteration: 1071, Checking gradient for output layer W[42][0]:	mock grad = -0.035529885843904596, computed grad = -0.035529885827322173
Iteration: 1072, Checking gradient for words E[689][31]:	mock grad = 0.002893318419988633, computed grad = 0.002893318407093566
Iteration: 1072, Checking gradient for hidden W[14][9]:	mock grad = -0.000528194162685569, computed grad = -0.000528194166904387
Iteration: 1072, Checking gradient for hidden b[0][37]:	mock grad = -0.050626874013998124, computed grad = -0.050626886818639277
Iteration: 1072, Checking gradient for output layer W[110][1]:	mock grad = 0.142605858938937846, computed grad = 0.142605857926294433
Iteration: 1073, Checking gradient for words E[166][7]:	mock grad = -0.021369907285739842, computed grad = -0.021369907405126204
Iteration: 1073, Checking gradient for hidden W[142][15]:	mock grad = -0.001198633646587455, computed grad = -0.001198633645519585
Iteration: 1073, Checking gradient for hidden b[0][16]:	mock grad = 0.023209434480836455, computed grad = 0.023209443248078679
Iteration: 1073, Checking gradient for output layer W[135][0]:	mock grad = -0.019867029890818788, computed grad = -0.019867029889215973
Iteration: 1074, Checking gradient for words E[68][1]:	mock grad = 0.001028063190577910, computed grad = 0.001028063122030038
Iteration: 1074, Checking gradient for hidden W[145][16]:	mock grad = -0.009297091722398898, computed grad = -0.009297091931755382
Iteration: 1074, Checking gradient for hidden b[0][8]:	mock grad = -0.040573290104234383, computed grad = -0.040573313340351323
Iteration: 1074, Checking gradient for output layer W[8][0]:	mock grad = -0.054287935256469577, computed grad = -0.054287935214226715
Iteration: 1075, Checking gradient for words E[104][8]:	mock grad = -0.009208446239833412, computed grad = -0.009208446269761080
Iteration: 1075, Checking gradient for hidden W[112][14]:	mock grad = -0.000229026626386020, computed grad = -0.000229026625402621
Iteration: 1075, Checking gradient for hidden b[0][7]:	mock grad = -0.064707914613132411, computed grad = -0.064707921002632868
Iteration: 1075, Checking gradient for output layer W[90][1]:	mock grad = -0.100029944546359273, computed grad = -0.100029943812873376
Iteration: 1076, Checking gradient for words E[697][29]:	mock grad = 0.002170664612027862, computed grad = 0.002170664629365742
Iteration: 1076, Checking gradient for hidden W[149][1]:	mock grad = -0.010508535416065001, computed grad = -0.010508535499511376
Iteration: 1076, Checking gradient for hidden b[0][31]:	mock grad = -0.036119272851076190, computed grad = -0.036119279831955980
Iteration: 1076, Checking gradient for output layer W[125][1]:	mock grad = 0.011651392835099195, computed grad = 0.011651392834381576
Iteration: 1077, Checking gradient for words E[94][40]:	mock grad = 0.012550774740632820, computed grad = 0.012550774757858117
Iteration: 1077, Checking gradient for hidden W[19][23]:	mock grad = 0.016936786935650705, computed grad = 0.016936786977707969
Iteration: 1077, Checking gradient for hidden b[0][2]:	mock grad = 0.038132440342147911, computed grad = 0.038132454736091587
Iteration: 1077, Checking gradient for output layer W[126][0]:	mock grad = 0.055022948499183766, computed grad = 0.055022948318326624
Iteration: 1078, Checking gradient for words E[48][36]:	mock grad = 0.017593711448826710, computed grad = 0.019506190402643918
Iteration: 1078, Checking gradient for hidden W[193][38]:	mock grad = -0.005534036207316406, computed grad = -0.005534036240701255
Iteration: 1078, Checking gradient for hidden b[0][48]:	mock grad = -0.079030879708424928, computed grad = -0.079030894757663728
Iteration: 1078, Checking gradient for output layer W[97][1]:	mock grad = -0.093529826547505968, computed grad = -0.093529826101301713
current: 80, Cost = 0.305376, Correct(%) = 1, time = 5.23748
Iteration: 1079, Checking gradient for words E[320][47]:	mock grad = -0.004620148591344408, computed grad = -0.004620148612331508
Iteration: 1079, Checking gradient for hidden W[122][48]:	mock grad = -0.008623977927529891, computed grad = -0.008623977950980872
Iteration: 1079, Checking gradient for hidden b[0][23]:	mock grad = 0.115633993373082111, computed grad = 0.115634012437852332
Iteration: 1079, Checking gradient for output layer W[21][0]:	mock grad = 0.011774023404881317, computed grad = 0.011774023403559984
Iteration: 1080, Checking gradient for words E[315][13]:	mock grad = 0.023786994157004582, computed grad = 0.023786994207054719
Iteration: 1080, Checking gradient for hidden W[146][35]:	mock grad = -0.000067441689038938, computed grad = -0.000067441686934813
Iteration: 1080, Checking gradient for hidden b[0][15]:	mock grad = 0.090906701836979131, computed grad = 0.090906723458754571
Iteration: 1080, Checking gradient for output layer W[73][0]:	mock grad = 0.104324326304083836, computed grad = 0.104324325681584104
Iteration: 1081, Checking gradient for words E[646][41]:	mock grad = -0.010325293042207084, computed grad = -0.010325293013617334
Iteration: 1081, Checking gradient for hidden W[3][40]:	mock grad = 0.003024008428387237, computed grad = 0.003024008425592279
Iteration: 1081, Checking gradient for hidden b[0][5]:	mock grad = -0.008520446092175060, computed grad = -0.008520439737203308
Iteration: 1081, Checking gradient for output layer W[94][1]:	mock grad = 0.169051290328403603, computed grad = 0.169051286939847889
Iteration: 1082, Checking gradient for words E[68][30]:	mock grad = -0.011902906557281767, computed grad = -0.011902906567019094
Iteration: 1082, Checking gradient for hidden W[25][12]:	mock grad = 0.006618946627734434, computed grad = 0.006618946666596191
Iteration: 1082, Checking gradient for hidden b[0][40]:	mock grad = 0.093696352685190831, computed grad = 0.093696376631211054
Iteration: 1082, Checking gradient for output layer W[52][0]:	mock grad = -0.034397042493888863, computed grad = -0.034397042469849835
Iteration: 1083, Checking gradient for words E[34][27]:	mock grad = -0.001971685163887349, computed grad = -0.001971685201191205
Iteration: 1083, Checking gradient for hidden W[201][3]:	mock grad = -0.001898493444896232, computed grad = -0.000927132392650504
Iteration: 1083, Checking gradient for hidden b[0][39]:	mock grad = 0.084511584390795091, computed grad = 0.084511606255307695
Iteration: 1083, Checking gradient for output layer W[13][0]:	mock grad = 0.036326467360903170, computed grad = 0.036326467342433617
Iteration: 1084, Checking gradient for words E[729][3]:	mock grad = -0.026464062002629074, computed grad = -0.026464061985263052
Iteration: 1084, Checking gradient for hidden W[102][49]:	mock grad = 0.001702571623185545, computed grad = 0.001702571642366133
Iteration: 1084, Checking gradient for hidden b[0][22]:	mock grad = 0.032768188102494378, computed grad = 0.032768190810423356
Iteration: 1084, Checking gradient for output layer W[40][1]:	mock grad = -0.024301724876746311, computed grad = -0.024301724865649570
Iteration: 1085, Checking gradient for words E[107][6]:	mock grad = 0.018079628949368987, computed grad = 0.018079629002429879
Iteration: 1085, Checking gradient for hidden W[68][46]:	mock grad = 0.004396170915993425, computed grad = 0.004396170957794932
Iteration: 1085, Checking gradient for hidden b[0][9]:	mock grad = 0.043600478972255496, computed grad = 0.043600484383909781
Iteration: 1085, Checking gradient for output layer W[84][1]:	mock grad = -0.114331526540306649, computed grad = -0.114331525357654562
Iteration: 1086, Checking gradient for words E[3][41]:	mock grad = -0.010999631454344794, computed grad = -0.010999631481739887
Iteration: 1086, Checking gradient for hidden W[10][15]:	mock grad = -0.002465980039195736, computed grad = -0.002465980036642337
Iteration: 1086, Checking gradient for hidden b[0][18]:	mock grad = 0.059036719102567581, computed grad = 0.059036722175363085
Iteration: 1086, Checking gradient for output layer W[135][0]:	mock grad = -0.027939731899351283, computed grad = -0.027939731886805191
Iteration: 1087, Checking gradient for words E[250][28]:	mock grad = 0.003950723013235180, computed grad = 0.002290333059032231
Iteration: 1087, Checking gradient for hidden W[64][24]:	mock grad = -0.008454823964026970, computed grad = -0.008454823907526313
Iteration: 1087, Checking gradient for hidden b[0][36]:	mock grad = -0.006457179281493808, computed grad = -0.006457174863422670
Iteration: 1087, Checking gradient for output layer W[80][0]:	mock grad = -0.079335450977796595, computed grad = -0.079335450698599916
Iteration: 1088, Checking gradient for words E[776][35]:	mock grad = -0.001839012175347543, computed grad = -0.001839012148611156
Iteration: 1088, Checking gradient for hidden W[175][12]:	mock grad = 0.006445068546939714, computed grad = 0.006445068591199063
Iteration: 1088, Checking gradient for hidden b[0][6]:	mock grad = -0.037236895790132563, computed grad = -0.037236908322809986
Iteration: 1088, Checking gradient for output layer W[149][1]:	mock grad = 0.096026435617152517, computed grad = 0.096026435090775158
current: 90, Cost = 0.350818, Correct(%) = 1, time = 6.18439
Iteration: 1089, Checking gradient for words E[652][1]:	mock grad = 0.000715872339018819, computed grad = 0.000715872375234193
Iteration: 1089, Checking gradient for hidden W[83][25]:	mock grad = -0.000413050862124731, computed grad = -0.000413050849937123
Iteration: 1089, Checking gradient for hidden b[0][39]:	mock grad = -0.082932913529343288, computed grad = -0.082932947863373532
Iteration: 1089, Checking gradient for output layer W[91][0]:	mock grad = 0.007935112894486007, computed grad = 0.007935112894207084
Iteration: 1090, Checking gradient for words E[166][23]:	mock grad = -0.003510833413322700, computed grad = -0.003510833411141346
Iteration: 1090, Checking gradient for hidden W[10][36]:	mock grad = -0.006961964644752738, computed grad = -0.006961964690474184
Iteration: 1090, Checking gradient for hidden b[0][8]:	mock grad = 0.028530165678913866, computed grad = 0.028530184983646199
Iteration: 1090, Checking gradient for output layer W[61][1]:	mock grad = 0.132504221159041036, computed grad = 0.132504219298654108
Iteration: 1091, Checking gradient for words E[199][21]:	mock grad = -0.006146042515170480, computed grad = -0.004480911158579667
Iteration: 1091, Checking gradient for hidden W[143][47]:	mock grad = -0.007930481169621251, computed grad = -0.007930481216698438
Iteration: 1091, Checking gradient for hidden b[0][10]:	mock grad = 0.082806742412011580, computed grad = 0.082806756133646048
Iteration: 1091, Checking gradient for output layer W[61][1]:	mock grad = 0.174141933709542895, computed grad = 0.174141931241353076
Iteration: 1092, Checking gradient for words E[129][16]:	mock grad = 0.000286911301189408, computed grad = 0.000286911300869186
Iteration: 1092, Checking gradient for hidden W[45][44]:	mock grad = 0.015190446303686711, computed grad = 0.015190446602685200
Iteration: 1092, Checking gradient for hidden b[0][12]:	mock grad = 0.055262243766562857, computed grad = 0.055262260280887029
Iteration: 1092, Checking gradient for output layer W[68][0]:	mock grad = -0.115129404962610593, computed grad = -0.115129403270825048
Iteration: 1093, Checking gradient for words E[759][38]:	mock grad = -0.001959011445284986, computed grad = -0.001959011444531315
Iteration: 1093, Checking gradient for hidden W[152][24]:	mock grad = -0.003084534521191618, computed grad = -0.003084534553778742
Iteration: 1093, Checking gradient for hidden b[0][1]:	mock grad = 0.100193807778370392, computed grad = 0.100193846428026853
Iteration: 1093, Checking gradient for output layer W[122][0]:	mock grad = -0.093005934419132519, computed grad = -0.093005933963218185
Iteration: 1094, Checking gradient for words E[74][28]:	mock grad = -0.015639132523004307, computed grad = -0.015639132568163107
Iteration: 1094, Checking gradient for hidden W[184][11]:	mock grad = -0.005243291201106892, computed grad = -0.005243291212278240
Iteration: 1094, Checking gradient for hidden b[0][32]:	mock grad = -0.043758938514854018, computed grad = -0.043758953288646693
Iteration: 1094, Checking gradient for output layer W[128][1]:	mock grad = 0.046840459353880703, computed grad = 0.046840459240419505
Iteration: 1095, Checking gradient for words E[533][46]:	mock grad = 0.036011802710317342, computed grad = 0.036011802700729498
Iteration: 1095, Checking gradient for hidden W[218][48]:	mock grad = -0.010229329411443056, computed grad = -0.010229329456199180
Iteration: 1095, Checking gradient for hidden b[0][15]:	mock grad = 0.091292580739088525, computed grad = 0.091292600260203596
Iteration: 1095, Checking gradient for output layer W[8][1]:	mock grad = 0.032781803993103820, computed grad = 0.032781803978694263
Iteration: 1096, Checking gradient for words E[161][18]:	mock grad = -0.010011562228601223, computed grad = -0.010011562256681501
Iteration: 1096, Checking gradient for hidden W[145][36]:	mock grad = 0.003530761496617352, computed grad = 0.003530761503722822
Iteration: 1096, Checking gradient for hidden b[0][11]:	mock grad = -0.083535669633760801, computed grad = -0.083535694120224532
Iteration: 1096, Checking gradient for output layer W[59][1]:	mock grad = 0.194010853623832746, computed grad = 0.194010848647445305
Iteration: 1097, Checking gradient for words E[776][18]:	mock grad = -0.010386950499180481, computed grad = -0.010386950512907297
Iteration: 1097, Checking gradient for hidden W[53][23]:	mock grad = 0.017099176456147891, computed grad = 0.017099176885841744
Iteration: 1097, Checking gradient for hidden b[0][6]:	mock grad = -0.045751205141802975, computed grad = -0.045751218920344318
Iteration: 1097, Checking gradient for output layer W[16][0]:	mock grad = -0.000966950046671355, computed grad = -0.000966950046674602
Iteration: 1098, Checking gradient for words E[772][14]:	mock grad = 0.000490587621970739, computed grad = 0.000490587620927347
Iteration: 1098, Checking gradient for hidden W[142][39]:	mock grad = 0.014619978310370652, computed grad = 0.014619978429788330
Iteration: 1098, Checking gradient for hidden b[0][9]:	mock grad = 0.047348455465945349, computed grad = 0.047348464600870929
Iteration: 1098, Checking gradient for output layer W[23][1]:	mock grad = 0.056747373905319254, computed grad = 0.056747373758053318
current: 100, Cost = 0.354249, Correct(%) = 1, time = 6.96584
Iteration: 1099, Checking gradient for words E[118][23]:	mock grad = -0.001768391344941778, computed grad = -0.001768391355579147
Iteration: 1099, Checking gradient for hidden W[141][42]:	mock grad = 0.009793274536235019, computed grad = 0.009793274581858760
Iteration: 1099, Checking gradient for hidden b[0][21]:	mock grad = -0.050801031902775184, computed grad = -0.050801044063955234
Iteration: 1099, Checking gradient for output layer W[147][1]:	mock grad = -0.029504436814986601, computed grad = -0.029504436801309403
current: 11, Correct(%) = 1, time = 7.01703
Dev start.
Dev finished. Total time taken is: 0.645544
dev:
Accuracy:	P=100/100=1
Test start.
Test finished. Total time taken is: 0.544822
test:
Accuracy:	P=64/100=0.64
##### Iteration 11
random: 0, 99
Iteration: 1100, Checking gradient for words E[0][34]:	mock grad = 0.009099320272099032, computed grad = 0.009099320301205933
Iteration: 1100, Checking gradient for hidden W[29][32]:	mock grad = 0.012027790550983575, computed grad = 0.012027790813786436
Iteration: 1100, Checking gradient for hidden b[0][32]:	mock grad = 0.039718162560931081, computed grad = 0.039718174767194185
Iteration: 1100, Checking gradient for output layer W[81][1]:	mock grad = 0.082614127547731009, computed grad = 0.082614126715772354
Iteration: 1101, Checking gradient for words E[92][11]:	mock grad = 0.001451787491529855, computed grad = 0.001451787491538777
Iteration: 1101, Checking gradient for hidden W[174][11]:	mock grad = -0.005694481864798595, computed grad = -0.005694481889280038
Iteration: 1101, Checking gradient for hidden b[0][20]:	mock grad = 0.083060729714007353, computed grad = 0.083060743908841869
Iteration: 1101, Checking gradient for output layer W[91][1]:	mock grad = 0.049000242895586021, computed grad = 0.049000242810036940
Iteration: 1102, Checking gradient for words E[79][46]:	mock grad = -0.009398444998476307, computed grad = -0.010606856799491190
Iteration: 1102, Checking gradient for hidden W[172][2]:	mock grad = -0.011293227918096616, computed grad = -0.011293227941343992
Iteration: 1102, Checking gradient for hidden b[0][5]:	mock grad = 0.007461354853605107, computed grad = 0.007461348014723385
Iteration: 1102, Checking gradient for output layer W[85][0]:	mock grad = 0.173331853549729686, computed grad = 0.173331848715285119
Iteration: 1103, Checking gradient for words E[137][39]:	mock grad = 0.006370284311107977, computed grad = 0.006370284318484578
Iteration: 1103, Checking gradient for hidden W[86][14]:	mock grad = 0.023090204398251712, computed grad = 0.023090204439191655
Iteration: 1103, Checking gradient for hidden b[0][31]:	mock grad = 0.042395668995109936, computed grad = 0.042395685107124811
Iteration: 1103, Checking gradient for output layer W[119][0]:	mock grad = -0.136635128146461726, computed grad = -0.136635127096965070
Iteration: 1104, Checking gradient for words E[174][40]:	mock grad = 0.009439487674123281, computed grad = 0.009913580111355612
Iteration: 1104, Checking gradient for hidden W[128][39]:	mock grad = -0.010052700074675291, computed grad = -0.010052700186447898
Iteration: 1104, Checking gradient for hidden b[0][34]:	mock grad = 0.026964345649943544, computed grad = 0.026964352339713643
Iteration: 1104, Checking gradient for output layer W[125][1]:	mock grad = 0.015376546677797753, computed grad = 0.015376546675747593
Iteration: 1105, Checking gradient for words E[103][14]:	mock grad = 0.002264476629182388, computed grad = 0.002264476636393923
Iteration: 1105, Checking gradient for hidden W[61][28]:	mock grad = -0.000530948161825417, computed grad = -0.000530948126942480
Iteration: 1105, Checking gradient for hidden b[0][20]:	mock grad = -0.089386261895002539, computed grad = -0.089386285343287755
Iteration: 1105, Checking gradient for output layer W[64][0]:	mock grad = 0.172991467555172473, computed grad = 0.172991464262836070
Iteration: 1106, Checking gradient for words E[262][43]:	mock grad = 0.005251345891049697, computed grad = 0.005251345874533766
Iteration: 1106, Checking gradient for hidden W[25][16]:	mock grad = 0.000366611849100806, computed grad = 0.000366611827587178
Iteration: 1106, Checking gradient for hidden b[0][1]:	mock grad = 0.080264489629888747, computed grad = 0.080264523096863410
Iteration: 1106, Checking gradient for output layer W[81][1]:	mock grad = -0.044371749713445485, computed grad = -0.044371749595777214
Iteration: 1107, Checking gradient for words E[25][11]:	mock grad = 0.032669426304948734, computed grad = 0.032669426356070862
Iteration: 1107, Checking gradient for hidden W[52][5]:	mock grad = 0.000595616016896283, computed grad = 0.000595616193696193
Iteration: 1107, Checking gradient for hidden b[0][39]:	mock grad = 0.104089816034547145, computed grad = 0.104089854023862338
Iteration: 1107, Checking gradient for output layer W[134][1]:	mock grad = -0.100040892671682258, computed grad = -0.100040892516131602
Iteration: 1108, Checking gradient for words E[288][1]:	mock grad = -0.003558951142068612, computed grad = -0.003558951147531117
Iteration: 1108, Checking gradient for hidden W[53][7]:	mock grad = 0.006100230119349126, computed grad = 0.006100230179189138
Iteration: 1108, Checking gradient for hidden b[0][27]:	mock grad = -0.031893684614398010, computed grad = -0.031893698087702711
Iteration: 1108, Checking gradient for output layer W[78][1]:	mock grad = -0.078547543618606719, computed grad = -0.078547543236153733
current: 10, Cost = 0.35107, Correct(%) = 1, time = 0.76749
Iteration: 1109, Checking gradient for words E[303][27]:	mock grad = -0.007806335963866928, computed grad = -0.007806336036131329
Iteration: 1109, Checking gradient for hidden W[168][47]:	mock grad = 0.002073279645187220, computed grad = 0.002073279645649336
Iteration: 1109, Checking gradient for hidden b[0][12]:	mock grad = 0.070715637433327316, computed grad = 0.070715661439361394
Iteration: 1109, Checking gradient for output layer W[1][0]:	mock grad = 0.001720407879102215, computed grad = 0.001720407879133692
Iteration: 1110, Checking gradient for words E[312][37]:	mock grad = 0.007965271047932765, computed grad = 0.008047656703282932
Iteration: 1110, Checking gradient for hidden W[27][17]:	mock grad = -0.001798548861065008, computed grad = -0.001798548879150655
Iteration: 1110, Checking gradient for hidden b[0][21]:	mock grad = 0.041972221308028690, computed grad = 0.041972231427238653
Iteration: 1110, Checking gradient for output layer W[148][0]:	mock grad = -0.102964899437091306, computed grad = -0.102964898213403808
Iteration: 1111, Checking gradient for words E[325][22]:	mock grad = -0.000409540540036168, computed grad = -0.000409540528681594
Iteration: 1111, Checking gradient for hidden W[45][24]:	mock grad = 0.002028279356192098, computed grad = 0.002028279394961522
Iteration: 1111, Checking gradient for hidden b[0][27]:	mock grad = 0.037864119510205851, computed grad = 0.037864140065436147
Iteration: 1111, Checking gradient for output layer W[74][0]:	mock grad = -0.182879282817377531, computed grad = -0.182879280936008637
Iteration: 1112, Checking gradient for words E[354][36]:	mock grad = -0.019968658502927950, computed grad = -0.019968658482234902
Iteration: 1112, Checking gradient for hidden W[184][38]:	mock grad = 0.003019944882415304, computed grad = 0.003019944950029607
Iteration: 1112, Checking gradient for hidden b[0][36]:	mock grad = -0.001280413119253421, computed grad = -0.001280422770064971
Iteration: 1112, Checking gradient for output layer W[149][1]:	mock grad = 0.128777854767281319, computed grad = 0.128777852780368240
Iteration: 1113, Checking gradient for words E[235][8]:	mock grad = -0.012794055200798793, computed grad = -0.012794055164770323
Iteration: 1113, Checking gradient for hidden W[233][21]:	mock grad = -0.005453550114720151, computed grad = -0.005453550143793714
Iteration: 1113, Checking gradient for hidden b[0][8]:	mock grad = 0.029853150578129650, computed grad = 0.029853171121077077
Iteration: 1113, Checking gradient for output layer W[133][0]:	mock grad = 0.026780225654526868, computed grad = 0.026780225636965180
Iteration: 1114, Checking gradient for words E[118][16]:	mock grad = 0.043005052601685634, computed grad = 0.043005052689831007
Iteration: 1114, Checking gradient for hidden W[157][40]:	mock grad = -0.000721420680860341, computed grad = -0.000721420617856965
Iteration: 1114, Checking gradient for hidden b[0][27]:	mock grad = -0.051720070559702069, computed grad = -0.051720092590222561
Iteration: 1114, Checking gradient for output layer W[85][1]:	mock grad = -0.266482856615235519, computed grad = -0.266482854725863139
Iteration: 1115, Checking gradient for words E[378][7]:	mock grad = -0.007869195207982349, computed grad = -0.007869195328155599
Iteration: 1115, Checking gradient for hidden W[91][8]:	mock grad = 0.016126821523965162, computed grad = 0.016126821927106488
Iteration: 1115, Checking gradient for hidden b[0][30]:	mock grad = -0.001455720009618799, computed grad = -0.001455718743120475
Iteration: 1115, Checking gradient for output layer W[84][1]:	mock grad = 0.072870752178294529, computed grad = 0.072870751953248561
Iteration: 1116, Checking gradient for words E[396][10]:	mock grad = -0.010668414693326689, computed grad = -0.010475690831937789
Iteration: 1116, Checking gradient for hidden W[137][9]:	mock grad = -0.011350176250674027, computed grad = -0.011350176309139381
Iteration: 1116, Checking gradient for hidden b[0][7]:	mock grad = -0.065421494413131498, computed grad = -0.065421497088678254
Iteration: 1116, Checking gradient for output layer W[2][0]:	mock grad = -0.047186049822012244, computed grad = -0.047186049772478381
Iteration: 1117, Checking gradient for words E[398][6]:	mock grad = 0.005859392545853881, computed grad = 0.005926611544027519
Iteration: 1117, Checking gradient for hidden W[38][24]:	mock grad = -0.010758636251356757, computed grad = -0.010758636359308975
Iteration: 1117, Checking gradient for hidden b[0][27]:	mock grad = -0.023153654710478255, computed grad = -0.023153660516658965
Iteration: 1117, Checking gradient for output layer W[89][1]:	mock grad = -0.087787221686314032, computed grad = -0.087787220706810623
Iteration: 1118, Checking gradient for words E[198][6]:	mock grad = 0.005227565290921721, computed grad = 0.005227565304219586
Iteration: 1118, Checking gradient for hidden W[143][47]:	mock grad = -0.003957839637219740, computed grad = -0.003957839666211636
Iteration: 1118, Checking gradient for hidden b[0][48]:	mock grad = -0.063633745312124956, computed grad = -0.063633756661379659
Iteration: 1118, Checking gradient for output layer W[135][0]:	mock grad = -0.024767954457188823, computed grad = -0.024767954441175501
current: 20, Cost = 0.446154, Correct(%) = 1, time = 1.42625
Iteration: 1119, Checking gradient for words E[42][6]:	mock grad = -0.022289029899896784, computed grad = -0.024221117298179596
Iteration: 1119, Checking gradient for hidden W[223][38]:	mock grad = 0.003039235779545013, computed grad = 0.003039235799791528
Iteration: 1119, Checking gradient for hidden b[0][19]:	mock grad = -0.061658148933524393, computed grad = -0.061658167338900935
Iteration: 1119, Checking gradient for output layer W[132][1]:	mock grad = -0.018827521420738957, computed grad = -0.018827521419230206
Iteration: 1120, Checking gradient for words E[208][30]:	mock grad = 0.014739544955066908, computed grad = 0.014739544971950021
Iteration: 1120, Checking gradient for hidden W[92][32]:	mock grad = -0.012613930451682576, computed grad = -0.012613930675909267
Iteration: 1120, Checking gradient for hidden b[0][4]:	mock grad = -0.018275237933546018, computed grad = -0.018275245752228223
Iteration: 1120, Checking gradient for output layer W[93][0]:	mock grad = -0.020244133661473374, computed grad = -0.020244133655650715
Iteration: 1121, Checking gradient for words E[6][43]:	mock grad = -0.016013828152972032, computed grad = -0.016013828306480307
Iteration: 1121, Checking gradient for hidden W[81][18]:	mock grad = -0.020512253153970317, computed grad = -0.020512253064590712
Iteration: 1121, Checking gradient for hidden b[0][0]:	mock grad = 0.199405300692095944, computed grad = 0.199405363468156543
Iteration: 1121, Checking gradient for output layer W[49][1]:	mock grad = -0.077000445010994012, computed grad = -0.077000444930798648
Iteration: 1122, Checking gradient for words E[13][17]:	mock grad = 0.004168587701830484, computed grad = 0.004168587741665653
Iteration: 1122, Checking gradient for hidden W[60][14]:	mock grad = 0.011899899699074901, computed grad = 0.011899899582636563
Iteration: 1122, Checking gradient for hidden b[0][41]:	mock grad = 0.027243908384377846, computed grad = 0.027243913321864033
Iteration: 1122, Checking gradient for output layer W[87][0]:	mock grad = 0.000054607756844183, computed grad = 0.000054607756781926
Iteration: 1123, Checking gradient for words E[166][29]:	mock grad = -0.002709010356011232, computed grad = -0.002709010368835254
Iteration: 1123, Checking gradient for hidden W[106][37]:	mock grad = 0.012500329674147714, computed grad = 0.012500329868241228
Iteration: 1123, Checking gradient for hidden b[0][38]:	mock grad = -0.059891815776091928, computed grad = -0.059891829329315679
Iteration: 1123, Checking gradient for output layer W[30][0]:	mock grad = 0.010144027944608558, computed grad = 0.010144027943910429
Iteration: 1124, Checking gradient for words E[42][34]:	mock grad = -0.009566912557740226, computed grad = -0.009566912630061552
Iteration: 1124, Checking gradient for hidden W[160][47]:	mock grad = 0.003321171040349125, computed grad = 0.003321171029205090
Iteration: 1124, Checking gradient for hidden b[0][36]:	mock grad = -0.010990261447241245, computed grad = -0.010990261261972282
Iteration: 1124, Checking gradient for output layer W[134][1]:	mock grad = -0.113536487545723430, computed grad = -0.113536486783830992
Iteration: 1125, Checking gradient for words E[118][5]:	mock grad = 0.019604481688639819, computed grad = 0.019604481749305326
Iteration: 1125, Checking gradient for hidden W[2][18]:	mock grad = 0.007356529193036643, computed grad = 0.007356529263321981
Iteration: 1125, Checking gradient for hidden b[0][18]:	mock grad = 0.062406545429688487, computed grad = 0.062406550441405292
Iteration: 1125, Checking gradient for output layer W[126][0]:	mock grad = 0.101006354641303142, computed grad = 0.101006354088742312
Iteration: 1126, Checking gradient for words E[426][4]:	mock grad = -0.000450971883725337, computed grad = -0.000450971887818337
Iteration: 1126, Checking gradient for hidden W[178][39]:	mock grad = 0.002571941125378840, computed grad = 0.002571941134398839
Iteration: 1126, Checking gradient for hidden b[0][6]:	mock grad = -0.029024671728133944, computed grad = -0.029024681978446383
Iteration: 1126, Checking gradient for output layer W[25][0]:	mock grad = 0.060689452893919471, computed grad = 0.060689452596308792
Iteration: 1127, Checking gradient for words E[487][0]:	mock grad = -0.005063945770100720, computed grad = 0.000089216957663837
Iteration: 1127, Checking gradient for hidden W[216][11]:	mock grad = -0.001135266069651752, computed grad = -0.001135266069788053
Iteration: 1127, Checking gradient for hidden b[0][35]:	mock grad = 0.016842809189815400, computed grad = 0.016842810442244283
Iteration: 1127, Checking gradient for output layer W[48][1]:	mock grad = -0.141162891832080639, computed grad = -0.141162891269459856
Iteration: 1128, Checking gradient for words E[231][15]:	mock grad = 0.001023394160470925, computed grad = 0.001023394152313106
Iteration: 1128, Checking gradient for hidden W[49][26]:	mock grad = 0.003963143045598860, computed grad = 0.003963143053045814
Iteration: 1128, Checking gradient for hidden b[0][49]:	mock grad = 0.023237974340784007, computed grad = 0.023237974016912074
Iteration: 1128, Checking gradient for output layer W[11][0]:	mock grad = -0.020300559535252338, computed grad = -0.020300559525781903
current: 30, Cost = 0.322524, Correct(%) = 1, time = 2.12473
Iteration: 1129, Checking gradient for words E[504][48]:	mock grad = 0.004571633505007222, computed grad = 0.005591008387059347
Iteration: 1129, Checking gradient for hidden W[202][47]:	mock grad = 0.002516869100771535, computed grad = 0.002516869095078173
Iteration: 1129, Checking gradient for hidden b[0][24]:	mock grad = 0.086703826626233216, computed grad = 0.086703852964834807
Iteration: 1129, Checking gradient for output layer W[42][1]:	mock grad = 0.052574463405202776, computed grad = 0.052574463301705218
Iteration: 1130, Checking gradient for words E[511][24]:	mock grad = -0.059643114063534508, computed grad = -0.059643113828530318
Iteration: 1130, Checking gradient for hidden W[127][19]:	mock grad = -0.007164459313924176, computed grad = -0.007164459344600675
Iteration: 1130, Checking gradient for hidden b[0][28]:	mock grad = 0.021993778112766593, computed grad = 0.021993797853090219
Iteration: 1130, Checking gradient for output layer W[98][1]:	mock grad = -0.026988819798651198, computed grad = -0.026988819783159066
Iteration: 1131, Checking gradient for words E[119][23]:	mock grad = 0.068860490925304330, computed grad = 0.068860490922408729
Iteration: 1131, Checking gradient for hidden W[6][10]:	mock grad = 0.017110662515196218, computed grad = 0.017110662754771102
Iteration: 1131, Checking gradient for hidden b[0][4]:	mock grad = 0.026095015056737392, computed grad = 0.026095025814045283
Iteration: 1131, Checking gradient for output layer W[74][1]:	mock grad = 0.159437860772798645, computed grad = 0.159437859817026822
Iteration: 1132, Checking gradient for words E[194][2]:	mock grad = 0.025767929727221750, computed grad = 0.025767929762098012
Iteration: 1132, Checking gradient for hidden W[23][34]:	mock grad = 0.001573300044771520, computed grad = 0.001573300080151739
Iteration: 1132, Checking gradient for hidden b[0][44]:	mock grad = -0.006048109473910657, computed grad = -0.006048123313733958
Iteration: 1132, Checking gradient for output layer W[95][0]:	mock grad = -0.047249311172142816, computed grad = -0.047249311090906465
Iteration: 1133, Checking gradient for words E[106][46]:	mock grad = -0.007485777684279604, computed grad = -0.007485777668102237
Iteration: 1133, Checking gradient for hidden W[127][40]:	mock grad = 0.004216987741667566, computed grad = 0.004216987751029158
Iteration: 1133, Checking gradient for hidden b[0][47]:	mock grad = 0.060567835880598331, computed grad = 0.060567850550823872
Iteration: 1133, Checking gradient for output layer W[83][0]:	mock grad = 0.082479591509782013, computed grad = 0.082479590942474479
Iteration: 1134, Checking gradient for words E[330][18]:	mock grad = 0.013618024223560621, computed grad = 0.011766304430023466
Iteration: 1134, Checking gradient for hidden W[157][10]:	mock grad = 0.007745613167658982, computed grad = 0.007745613192860633
Iteration: 1134, Checking gradient for hidden b[0][43]:	mock grad = -0.012654387000016198, computed grad = -0.012654390693161124
Iteration: 1134, Checking gradient for output layer W[66][0]:	mock grad = -0.051529138080680381, computed grad = -0.051529138036879654
Iteration: 1135, Checking gradient for words E[156][41]:	mock grad = -0.063940485203889397, computed grad = -0.063940485374184619
Iteration: 1135, Checking gradient for hidden W[226][16]:	mock grad = 0.002328363549464196, computed grad = 0.002328363562618484
Iteration: 1135, Checking gradient for hidden b[0][42]:	mock grad = -0.071502257491501409, computed grad = -0.071502280701832580
Iteration: 1135, Checking gradient for output layer W[93][1]:	mock grad = -0.084029814402891700, computed grad = -0.084029814187741553
Iteration: 1136, Checking gradient for words E[156][47]:	mock grad = -0.044332550559383899, computed grad = -0.044332550555023581
Iteration: 1136, Checking gradient for hidden W[245][29]:	mock grad = -0.020759306161188817, computed grad = -0.020759306468035831
Iteration: 1136, Checking gradient for hidden b[0][31]:	mock grad = -0.048495716883700801, computed grad = -0.048495732308527099
Iteration: 1136, Checking gradient for output layer W[22][1]:	mock grad = -0.027327105022928455, computed grad = -0.027327105018511107
Iteration: 1137, Checking gradient for words E[69][24]:	mock grad = -0.005966341828367305, computed grad = -0.005966341783970785
Iteration: 1137, Checking gradient for hidden W[164][36]:	mock grad = -0.001501379495061350, computed grad = -0.001501379583661447
Iteration: 1137, Checking gradient for hidden b[0][33]:	mock grad = 0.090533956013438832, computed grad = 0.090533977901888579
Iteration: 1137, Checking gradient for output layer W[109][1]:	mock grad = 0.046445493039060093, computed grad = 0.046445492978128174
Iteration: 1138, Checking gradient for words E[48][37]:	mock grad = 0.002173750513950745, computed grad = 0.002173750562629567
Iteration: 1138, Checking gradient for hidden W[203][8]:	mock grad = 0.011719262739823266, computed grad = 0.011719262759249588
Iteration: 1138, Checking gradient for hidden b[0][43]:	mock grad = -0.012556962792259485, computed grad = -0.012556966484474495
Iteration: 1138, Checking gradient for output layer W[50][0]:	mock grad = -0.180460794436398952, computed grad = -0.180460792465793163
current: 40, Cost = 0.266347, Correct(%) = 1, time = 2.56267
Iteration: 1139, Checking gradient for words E[150][47]:	mock grad = -0.008624660821965335, computed grad = -0.008624660867935317
Iteration: 1139, Checking gradient for hidden W[50][8]:	mock grad = 0.003050722592962130, computed grad = 0.003050722599317130
Iteration: 1139, Checking gradient for hidden b[0][48]:	mock grad = -0.068916510029742861, computed grad = -0.068916528800872581
Iteration: 1139, Checking gradient for output layer W[64][1]:	mock grad = -0.125962133227713879, computed grad = -0.125962130742893752
Iteration: 1140, Checking gradient for words E[406][49]:	mock grad = 0.020591887005011467, computed grad = 0.019967731716904358
Iteration: 1140, Checking gradient for hidden W[169][1]:	mock grad = -0.005879098533251303, computed grad = -0.005879098548258262
Iteration: 1140, Checking gradient for hidden b[0][46]:	mock grad = -0.106539352271395327, computed grad = -0.106539375424015328
Iteration: 1140, Checking gradient for output layer W[121][0]:	mock grad = -0.113692419271921308, computed grad = -0.113692417547467381
Iteration: 1141, Checking gradient for words E[68][40]:	mock grad = 0.040915406035541002, computed grad = 0.040915406035895358
Iteration: 1141, Checking gradient for hidden W[84][34]:	mock grad = 0.011852615859370141, computed grad = 0.011852615997058957
Iteration: 1141, Checking gradient for hidden b[0][49]:	mock grad = 0.032153219015312873, computed grad = 0.032153214790218335
Iteration: 1141, Checking gradient for output layer W[32][0]:	mock grad = -0.149843996417986514, computed grad = -0.149843995775904404
Iteration: 1142, Checking gradient for words E[6][42]:	mock grad = -0.097807703353186559, computed grad = -0.097807703236921839
Iteration: 1142, Checking gradient for hidden W[3][49]:	mock grad = -0.015084201434562017, computed grad = -0.015084201536881829
Iteration: 1142, Checking gradient for hidden b[0][38]:	mock grad = 0.080441362722727394, computed grad = 0.080441388245705667
Iteration: 1142, Checking gradient for output layer W[143][1]:	mock grad = -0.157590977046651037, computed grad = -0.157590975941595740
Iteration: 1143, Checking gradient for words E[562][11]:	mock grad = 0.031161417223812826, computed grad = 0.031161417276625213
Iteration: 1143, Checking gradient for hidden W[46][28]:	mock grad = 0.027612312978680942, computed grad = 0.027612313128186645
Iteration: 1143, Checking gradient for hidden b[0][16]:	mock grad = 0.021997137951174084, computed grad = 0.021997144860249427
Iteration: 1143, Checking gradient for output layer W[108][1]:	mock grad = -0.131919854935075298, computed grad = -0.131919854500948480
Iteration: 1144, Checking gradient for words E[396][1]:	mock grad = 0.009944423644686617, computed grad = 0.009944423672397926
Iteration: 1144, Checking gradient for hidden W[47][39]:	mock grad = -0.006533555099719202, computed grad = -0.006533555157144507
Iteration: 1144, Checking gradient for hidden b[0][26]:	mock grad = 0.020260751444289848, computed grad = 0.020260755568483585
Iteration: 1144, Checking gradient for output layer W[146][1]:	mock grad = 0.000918114428635763, computed grad = 0.000918114428686284
Iteration: 1145, Checking gradient for words E[222][15]:	mock grad = 0.013737036007432923, computed grad = 0.013737036065421308
Iteration: 1145, Checking gradient for hidden W[35][4]:	mock grad = -0.005634472367649357, computed grad = -0.005634472383908769
Iteration: 1145, Checking gradient for hidden b[0][20]:	mock grad = 0.106782225462426217, computed grad = 0.106782258388077436
Iteration: 1145, Checking gradient for output layer W[85][0]:	mock grad = -0.180506454402457051, computed grad = -0.180506452154211738
Iteration: 1146, Checking gradient for words E[592][44]:	mock grad = -0.008353706116875914, computed grad = -0.008353706121597058
Iteration: 1146, Checking gradient for hidden W[165][34]:	mock grad = -0.003957160354312039, computed grad = -0.003957160357665481
Iteration: 1146, Checking gradient for hidden b[0][2]:	mock grad = 0.047283995291635961, computed grad = 0.047284022840156842
Iteration: 1146, Checking gradient for output layer W[98][0]:	mock grad = -0.053557080283456182, computed grad = -0.053557080094009439
Iteration: 1147, Checking gradient for words E[279][30]:	mock grad = 0.014135857628133630, computed grad = 0.014135857659398361
Iteration: 1147, Checking gradient for hidden W[230][1]:	mock grad = -0.014735796796450096, computed grad = -0.014735797142827344
Iteration: 1147, Checking gradient for hidden b[0][46]:	mock grad = -0.119530432681508314, computed grad = -0.119530458062163128
Iteration: 1147, Checking gradient for output layer W[32][0]:	mock grad = 0.084778449885886120, computed grad = 0.084778449360908795
Iteration: 1148, Checking gradient for words E[291][37]:	mock grad = -0.003521019729141317, computed grad = -0.003521019727342785
Iteration: 1148, Checking gradient for hidden W[230][5]:	mock grad = -0.003904863229098332, computed grad = -0.003904863252600056
Iteration: 1148, Checking gradient for hidden b[0][13]:	mock grad = -0.085003445582959580, computed grad = -0.085003456682945552
Iteration: 1148, Checking gradient for output layer W[15][1]:	mock grad = 0.067113172062122084, computed grad = 0.067113171747921252
current: 50, Cost = 0.267521, Correct(%) = 1, time = 3.44935
Iteration: 1149, Checking gradient for words E[529][0]:	mock grad = -0.009892813960826752, computed grad = -0.009959365637872843
Iteration: 1149, Checking gradient for hidden W[94][7]:	mock grad = 0.005675909232727427, computed grad = 0.005675909245729504
Iteration: 1149, Checking gradient for hidden b[0][3]:	mock grad = -0.031095677973547176, computed grad = -0.031095687985055881
Iteration: 1149, Checking gradient for output layer W[73][1]:	mock grad = 0.040036735896364917, computed grad = 0.040036735817606188
Iteration: 1150, Checking gradient for words E[49][28]:	mock grad = 0.029293505679206522, computed grad = 0.029293505768532221
Iteration: 1150, Checking gradient for hidden W[242][3]:	mock grad = -0.004551262369201226, computed grad = -0.004551262381616351
Iteration: 1150, Checking gradient for hidden b[0][2]:	mock grad = 0.062475553798846395, computed grad = 0.062475583415390244
Iteration: 1150, Checking gradient for output layer W[4][0]:	mock grad = -0.004345327538485266, computed grad = -0.004345327538473127
Iteration: 1151, Checking gradient for words E[54][3]:	mock grad = 0.006212648986592129, computed grad = 0.006212648990814952
Iteration: 1151, Checking gradient for hidden W[39][45]:	mock grad = -0.004838223655989626, computed grad = -0.004838223705516182
Iteration: 1151, Checking gradient for hidden b[0][16]:	mock grad = -0.017426167123857361, computed grad = -0.017426172997090886
Iteration: 1151, Checking gradient for output layer W[24][0]:	mock grad = 0.065122345889095934, computed grad = 0.065122345742090124
Iteration: 1152, Checking gradient for words E[107][14]:	mock grad = -0.007079599702308759, computed grad = -0.007079599757099863
Iteration: 1152, Checking gradient for hidden W[46][0]:	mock grad = -0.019343087119066960, computed grad = -0.019343087199736150
Iteration: 1152, Checking gradient for hidden b[0][34]:	mock grad = 0.025766858469583598, computed grad = 0.025766862294009464
Iteration: 1152, Checking gradient for output layer W[67][0]:	mock grad = 0.022785670831526605, computed grad = 0.022785670826503020
Iteration: 1153, Checking gradient for words E[776][46]:	mock grad = 0.007673799347657217, computed grad = 0.007673799430878082
Iteration: 1153, Checking gradient for hidden W[20][1]:	mock grad = 0.004232228877576460, computed grad = 0.004232228880997575
Iteration: 1153, Checking gradient for hidden b[0][3]:	mock grad = -0.027580051672554395, computed grad = -0.027580054267197902
Iteration: 1153, Checking gradient for output layer W[63][1]:	mock grad = 0.108975206131040059, computed grad = 0.108975205178547876
Iteration: 1154, Checking gradient for words E[630][48]:	mock grad = 0.007078394610865457, computed grad = 0.007078394598629484
Iteration: 1154, Checking gradient for hidden W[138][37]:	mock grad = -0.005626320811469299, computed grad = -0.005626320875646271
Iteration: 1154, Checking gradient for hidden b[0][17]:	mock grad = -0.013254235020632166, computed grad = -0.013254238072781279
Iteration: 1154, Checking gradient for output layer W[73][1]:	mock grad = -0.055866752134842157, computed grad = -0.055866751909367109
Iteration: 1155, Checking gradient for words E[776][19]:	mock grad = -0.010625629547122983, computed grad = -0.010625629516017459
Iteration: 1155, Checking gradient for hidden W[231][25]:	mock grad = -0.003924613114852926, computed grad = -0.003924613117446384
Iteration: 1155, Checking gradient for hidden b[0][15]:	mock grad = 0.069263851005391119, computed grad = 0.069263860424303617
Iteration: 1155, Checking gradient for output layer W[41][0]:	mock grad = -0.055209800373295925, computed grad = -0.055209800201181151
Iteration: 1156, Checking gradient for words E[435][33]:	mock grad = -0.021996084238889990, computed grad = -0.021996084295857456
Iteration: 1156, Checking gradient for hidden W[72][47]:	mock grad = 0.001021975425480326, computed grad = 0.001021975433989182
Iteration: 1156, Checking gradient for hidden b[0][29]:	mock grad = 0.102017371274798485, computed grad = 0.102017406220812554
Iteration: 1156, Checking gradient for output layer W[56][1]:	mock grad = -0.091861487156230659, computed grad = -0.091861487027454267
Iteration: 1157, Checking gradient for words E[20][27]:	mock grad = -0.012700484752992125, computed grad = -0.012700484767042700
Iteration: 1157, Checking gradient for hidden W[63][3]:	mock grad = 0.003023933230955622, computed grad = 0.003023933356377338
Iteration: 1157, Checking gradient for hidden b[0][25]:	mock grad = 0.005213678010734801, computed grad = 0.005213676071111248
Iteration: 1157, Checking gradient for output layer W[4][0]:	mock grad = -0.018002106203535151, computed grad = -0.018002106199361673
Iteration: 1158, Checking gradient for words E[541][27]:	mock grad = -0.007402986183402405, computed grad = -0.009080500270768010
Iteration: 1158, Checking gradient for hidden W[154][0]:	mock grad = 0.013174357470363551, computed grad = 0.013174357516736596
Iteration: 1158, Checking gradient for hidden b[0][7]:	mock grad = -0.070861906629832161, computed grad = -0.070861912173877259
Iteration: 1158, Checking gradient for output layer W[37][0]:	mock grad = -0.036157831100724902, computed grad = -0.036157831078790302
current: 60, Cost = 0.345485, Correct(%) = 1, time = 4.14094
Iteration: 1159, Checking gradient for words E[156][11]:	mock grad = -0.026045147905828792, computed grad = -0.026045147981832870
Iteration: 1159, Checking gradient for hidden W[19][45]:	mock grad = 0.000524236318827365, computed grad = 0.000524236302885060
Iteration: 1159, Checking gradient for hidden b[0][20]:	mock grad = 0.092484520854252406, computed grad = 0.092484539744939170
Iteration: 1159, Checking gradient for output layer W[102][0]:	mock grad = 0.125668854838695276, computed grad = 0.125668853697919830
Iteration: 1160, Checking gradient for words E[119][39]:	mock grad = 0.038534250899224798, computed grad = 0.038534251023121524
Iteration: 1160, Checking gradient for hidden W[91][16]:	mock grad = 0.005810609626566432, computed grad = 0.005810609695513390
Iteration: 1160, Checking gradient for hidden b[0][5]:	mock grad = -0.012606817480886878, computed grad = -0.012606813065536467
Iteration: 1160, Checking gradient for output layer W[0][1]:	mock grad = 0.065762186947398993, computed grad = 0.065762186848926069
Iteration: 1161, Checking gradient for words E[137][37]:	mock grad = -0.020821837299345169, computed grad = -0.022193803081507861
Iteration: 1161, Checking gradient for hidden W[50][35]:	mock grad = 0.001716273816870784, computed grad = 0.001716273814074628
Iteration: 1161, Checking gradient for hidden b[0][2]:	mock grad = -0.059270657436466534, computed grad = -0.059270674133196692
Iteration: 1161, Checking gradient for output layer W[13][0]:	mock grad = -0.065800553399192330, computed grad = -0.065800553296536030
Iteration: 1162, Checking gradient for words E[326][30]:	mock grad = -0.012091151880416762, computed grad = -0.012091151870797505
Iteration: 1162, Checking gradient for hidden W[165][14]:	mock grad = -0.019793868259648795, computed grad = -0.019793868537737160
Iteration: 1162, Checking gradient for hidden b[0][49]:	mock grad = 0.037311948454354926, computed grad = 0.037311956083176806
Iteration: 1162, Checking gradient for output layer W[125][1]:	mock grad = 0.011322552139897413, computed grad = 0.011322552139105165
Iteration: 1163, Checking gradient for words E[361][41]:	mock grad = -0.017475298110997128, computed grad = -0.017475298082108917
Iteration: 1163, Checking gradient for hidden W[152][44]:	mock grad = 0.023799327380336077, computed grad = 0.023799327415420665
Iteration: 1163, Checking gradient for hidden b[0][27]:	mock grad = -0.032618189074934412, computed grad = -0.032618199936115679
Iteration: 1163, Checking gradient for output layer W[34][1]:	mock grad = -0.020745619550538441, computed grad = -0.020745619546002517
Iteration: 1164, Checking gradient for words E[112][0]:	mock grad = -0.003092196142862047, computed grad = -0.003092196193690713
Iteration: 1164, Checking gradient for hidden W[209][18]:	mock grad = 0.008090404737176593, computed grad = 0.008090404771094718
Iteration: 1164, Checking gradient for hidden b[0][42]:	mock grad = 0.069099716536957789, computed grad = 0.069099738007417963
Iteration: 1164, Checking gradient for output layer W[21][1]:	mock grad = 0.009815506452737210, computed grad = 0.009815506452382687
Iteration: 1165, Checking gradient for words E[362][14]:	mock grad = -0.003316921890969549, computed grad = -0.003316921881323347
Iteration: 1165, Checking gradient for hidden W[157][21]:	mock grad = -0.004021665840306388, computed grad = -0.004021665910941359
Iteration: 1165, Checking gradient for hidden b[0][7]:	mock grad = 0.057162464570742477, computed grad = 0.057162478339355317
Iteration: 1165, Checking gradient for output layer W[45][0]:	mock grad = 0.011477888803734793, computed grad = 0.011477888801442329
Iteration: 1166, Checking gradient for words E[215][24]:	mock grad = -0.011739132903088034, computed grad = -0.011739132980263909
Iteration: 1166, Checking gradient for hidden W[130][29]:	mock grad = 0.001766621136267688, computed grad = 0.001766621135998682
Iteration: 1166, Checking gradient for hidden b[0][19]:	mock grad = -0.040833262415163052, computed grad = -0.040833270828993469
Iteration: 1166, Checking gradient for output layer W[14][1]:	mock grad = -0.001850700598798882, computed grad = -0.001850700598880913
Iteration: 1167, Checking gradient for words E[259][14]:	mock grad = 0.017071541224544795, computed grad = 0.017071541333488899
Iteration: 1167, Checking gradient for hidden W[209][9]:	mock grad = -0.013590325663559444, computed grad = -0.013590325828886616
Iteration: 1167, Checking gradient for hidden b[0][45]:	mock grad = 0.006602108027553566, computed grad = 0.006602107283647761
Iteration: 1167, Checking gradient for output layer W[148][1]:	mock grad = 0.161087417632266305, computed grad = 0.161087415394731537
Iteration: 1168, Checking gradient for words E[674][31]:	mock grad = -0.009281608602085267, computed grad = -0.009281608637307386
Iteration: 1168, Checking gradient for hidden W[19][13]:	mock grad = 0.015122517830673532, computed grad = 0.015122517911763778
Iteration: 1168, Checking gradient for hidden b[0][12]:	mock grad = -0.059157286573907619, computed grad = -0.059157304979122784
Iteration: 1168, Checking gradient for output layer W[13][0]:	mock grad = -0.050145422220176927, computed grad = -0.050145422099017969
current: 70, Cost = 0.266386, Correct(%) = 1, time = 4.75397
Iteration: 1169, Checking gradient for words E[148][24]:	mock grad = 0.010562271771957787, computed grad = 0.010562271813505313
Iteration: 1169, Checking gradient for hidden W[10][33]:	mock grad = -0.007020134379642773, computed grad = -0.007020134392430273
Iteration: 1169, Checking gradient for hidden b[0][12]:	mock grad = -0.054798037965125834, computed grad = -0.054798055478215285
Iteration: 1169, Checking gradient for output layer W[30][0]:	mock grad = -0.009925709110764069, computed grad = -0.009925709109524002
Iteration: 1170, Checking gradient for words E[377][34]:	mock grad = 0.033783341626569818, computed grad = 0.033783341749466997
Iteration: 1170, Checking gradient for hidden W[233][20]:	mock grad = -0.005655048835873089, computed grad = -0.005655048845036211
Iteration: 1170, Checking gradient for hidden b[0][48]:	mock grad = -0.097444822792974728, computed grad = -0.097444848844129558
Iteration: 1170, Checking gradient for output layer W[144][0]:	mock grad = -0.077910785826834639, computed grad = -0.077910785658449624
Iteration: 1171, Checking gradient for words E[78][13]:	mock grad = 0.066558226693130829, computed grad = 0.065865864676858787
Iteration: 1171, Checking gradient for hidden W[59][9]:	mock grad = 0.001670382444085572, computed grad = 0.001670382453057123
Iteration: 1171, Checking gradient for hidden b[0][47]:	mock grad = 0.079326997067447058, computed grad = 0.079327016175640586
Iteration: 1171, Checking gradient for output layer W[8][0]:	mock grad = -0.092964495545494463, computed grad = -0.092964495217882304
Iteration: 1172, Checking gradient for words E[169][4]:	mock grad = 0.003836666782053300, computed grad = 0.003836666831364094
Iteration: 1172, Checking gradient for hidden W[186][39]:	mock grad = 0.020166971506002662, computed grad = 0.020166971745595012
Iteration: 1172, Checking gradient for hidden b[0][2]:	mock grad = -0.045755062606289698, computed grad = -0.045755068607546329
Iteration: 1172, Checking gradient for output layer W[1][1]:	mock grad = 0.022604598283432198, computed grad = 0.022604598279080321
Iteration: 1173, Checking gradient for words E[231][12]:	mock grad = -0.077458321080137171, computed grad = -0.079093214454535932
Iteration: 1173, Checking gradient for hidden W[147][37]:	mock grad = 0.005924430401316183, computed grad = 0.005924430528397170
Iteration: 1173, Checking gradient for hidden b[0][43]:	mock grad = -0.013119735205957550, computed grad = -0.013119738717351840
Iteration: 1173, Checking gradient for output layer W[45][0]:	mock grad = 0.021416744555746714, computed grad = 0.021416744553413150
Iteration: 1174, Checking gradient for words E[73][45]:	mock grad = -0.013726716749234980, computed grad = -0.013726716734240731
Iteration: 1174, Checking gradient for hidden W[82][39]:	mock grad = 0.018984890597817383, computed grad = 0.018984890898251321
Iteration: 1174, Checking gradient for hidden b[0][22]:	mock grad = -0.051473875942259495, computed grad = -0.051473890432937701
Iteration: 1174, Checking gradient for output layer W[51][0]:	mock grad = 0.064211193556845680, computed grad = 0.064211193480489662
Iteration: 1175, Checking gradient for words E[341][18]:	mock grad = 0.016644059393250421, computed grad = 0.016644059426555564
Iteration: 1175, Checking gradient for hidden W[246][20]:	mock grad = 0.010387678095719055, computed grad = 0.010387678182047649
Iteration: 1175, Checking gradient for hidden b[0][12]:	mock grad = -0.062157696182035815, computed grad = -0.062157716796903337
Iteration: 1175, Checking gradient for output layer W[106][1]:	mock grad = 0.051902531116587713, computed grad = 0.051902531002726036
Iteration: 1176, Checking gradient for words E[289][0]:	mock grad = -0.038000803565724217, computed grad = -0.038000803731686031
Iteration: 1176, Checking gradient for hidden W[186][38]:	mock grad = 0.005941368721007478, computed grad = 0.005941368730074773
Iteration: 1176, Checking gradient for hidden b[0][33]:	mock grad = -0.089968344052032689, computed grad = -0.089968358377966712
Iteration: 1176, Checking gradient for output layer W[31][0]:	mock grad = 0.042926574121748828, computed grad = 0.042926574078421688
Iteration: 1177, Checking gradient for words E[94][2]:	mock grad = 0.004407048370458355, computed grad = 0.004407048299524239
Iteration: 1177, Checking gradient for hidden W[95][18]:	mock grad = -0.003769189977187448, computed grad = -0.003769189992328839
Iteration: 1177, Checking gradient for hidden b[0][7]:	mock grad = 0.069834353376846403, computed grad = 0.069834376196365622
Iteration: 1177, Checking gradient for output layer W[7][0]:	mock grad = -0.006315940357909389, computed grad = -0.006315940357599123
Iteration: 1178, Checking gradient for words E[54][28]:	mock grad = 0.000972104797741791, computed grad = -0.000152028774832051
Iteration: 1178, Checking gradient for hidden W[225][35]:	mock grad = 0.003348030977262662, computed grad = 0.003348030986339549
Iteration: 1178, Checking gradient for hidden b[0][39]:	mock grad = -0.080289683021816938, computed grad = -0.080289715926982044
Iteration: 1178, Checking gradient for output layer W[71][0]:	mock grad = 0.075859047589610373, computed grad = 0.075859047329484869
current: 80, Cost = 0.295458, Correct(%) = 1, time = 5.24169
Iteration: 1179, Checking gradient for words E[577][36]:	mock grad = 0.000405082995591499, computed grad = 0.000405082949815524
Iteration: 1179, Checking gradient for hidden W[181][6]:	mock grad = 0.000472640744764163, computed grad = 0.000472640758732663
Iteration: 1179, Checking gradient for hidden b[0][45]:	mock grad = -0.007551259989357906, computed grad = -0.007551261383897251
Iteration: 1179, Checking gradient for output layer W[89][0]:	mock grad = -0.117645712721742290, computed grad = -0.117645711214520374
Iteration: 1180, Checking gradient for words E[148][47]:	mock grad = 0.007813758057223508, computed grad = 0.007813757968451898
Iteration: 1180, Checking gradient for hidden W[131][33]:	mock grad = -0.003603251353151427, computed grad = -0.003603251418188227
Iteration: 1180, Checking gradient for hidden b[0][31]:	mock grad = 0.038613965479161738, computed grad = 0.038613980175671803
Iteration: 1180, Checking gradient for output layer W[122][0]:	mock grad = -0.078274730706345830, computed grad = -0.078274730422985606
Iteration: 1181, Checking gradient for words E[306][32]:	mock grad = -0.001000507764636893, computed grad = -0.001000507758569645
Iteration: 1181, Checking gradient for hidden W[241][21]:	mock grad = 0.002487890650187952, computed grad = 0.002487890653712739
Iteration: 1181, Checking gradient for hidden b[0][45]:	mock grad = -0.006846379984909445, computed grad = -0.006846380087620474
Iteration: 1181, Checking gradient for output layer W[56][0]:	mock grad = -0.173409761945797491, computed grad = -0.173409757878271592
Iteration: 1182, Checking gradient for words E[48][17]:	mock grad = 0.039691608151176672, computed grad = 0.039691608062323830
Iteration: 1182, Checking gradient for hidden W[111][45]:	mock grad = -0.000492230053245057, computed grad = -0.000492230039930797
Iteration: 1182, Checking gradient for hidden b[0][11]:	mock grad = -0.083927267881450662, computed grad = -0.083927292517166588
Iteration: 1182, Checking gradient for output layer W[24][1]:	mock grad = 0.052233557562286759, computed grad = 0.052233557467817764
Iteration: 1183, Checking gradient for words E[102][24]:	mock grad = -0.011350626985873191, computed grad = -0.011350627025474770
Iteration: 1183, Checking gradient for hidden W[182][12]:	mock grad = -0.008012868687234942, computed grad = -0.008012868876159667
Iteration: 1183, Checking gradient for hidden b[0][35]:	mock grad = 0.011857126533815210, computed grad = 0.011857125748446357
Iteration: 1183, Checking gradient for output layer W[129][0]:	mock grad = 0.134973355742223911, computed grad = 0.134973354701303616
Iteration: 1184, Checking gradient for words E[726][36]:	mock grad = 0.006592908234986927, computed grad = 0.006592908207264055
Iteration: 1184, Checking gradient for hidden W[127][5]:	mock grad = -0.001382556200524299, computed grad = -0.001382556152497419
Iteration: 1184, Checking gradient for hidden b[0][22]:	mock grad = 0.031512401609284924, computed grad = 0.031512404142150889
Iteration: 1184, Checking gradient for output layer W[62][1]:	mock grad = 0.129472677741854225, computed grad = 0.129472675834218148
Iteration: 1185, Checking gradient for words E[102][21]:	mock grad = 0.004538791627956007, computed grad = 0.004538791639387507
Iteration: 1185, Checking gradient for hidden W[185][18]:	mock grad = -0.002521155030382749, computed grad = -0.002521155054314641
Iteration: 1185, Checking gradient for hidden b[0][27]:	mock grad = -0.034465360175511828, computed grad = -0.034465375670501383
Iteration: 1185, Checking gradient for output layer W[61][1]:	mock grad = -0.134152106603058208, computed grad = -0.134152104469077216
Iteration: 1186, Checking gradient for words E[403][16]:	mock grad = 0.011990652137611191, computed grad = 0.011990652193149978
Iteration: 1186, Checking gradient for hidden W[242][35]:	mock grad = 0.005603548985849693, computed grad = 0.005603549038311182
Iteration: 1186, Checking gradient for hidden b[0][43]:	mock grad = -0.009999460460125009, computed grad = -0.009999463109632057
Iteration: 1186, Checking gradient for output layer W[32][1]:	mock grad = 0.087772975791183816, computed grad = 0.087772975357762517
Iteration: 1187, Checking gradient for words E[250][10]:	mock grad = 0.019368753411019846, computed grad = 0.019368753446865679
Iteration: 1187, Checking gradient for hidden W[172][34]:	mock grad = -0.011460831638249180, computed grad = -0.011460831661074656
Iteration: 1187, Checking gradient for hidden b[0][1]:	mock grad = -0.100501841132610847, computed grad = -0.100501863138378236
Iteration: 1187, Checking gradient for output layer W[126][0]:	mock grad = 0.129855914816678908, computed grad = 0.129855913482034624
Iteration: 1188, Checking gradient for words E[741][13]:	mock grad = 0.012243147959112299, computed grad = 0.012243147960229798
Iteration: 1188, Checking gradient for hidden W[107][16]:	mock grad = 0.000106457214849032, computed grad = 0.000106457224823814
Iteration: 1188, Checking gradient for hidden b[0][14]:	mock grad = 0.048444295343536004, computed grad = 0.048444307691467819
Iteration: 1188, Checking gradient for output layer W[123][0]:	mock grad = -0.147949524566926005, computed grad = -0.147949522437490155
current: 90, Cost = 0.339898, Correct(%) = 1, time = 6.18756
Iteration: 1189, Checking gradient for words E[445][20]:	mock grad = 0.035186715576951411, computed grad = 0.035186715610754468
Iteration: 1189, Checking gradient for hidden W[31][35]:	mock grad = 0.003923316640369601, computed grad = 0.003923316666703758
Iteration: 1189, Checking gradient for hidden b[0][36]:	mock grad = 0.005313955061009290, computed grad = 0.005313949110457461
Iteration: 1189, Checking gradient for output layer W[88][1]:	mock grad = -0.085846163440400636, computed grad = -0.085846163057421584
Iteration: 1190, Checking gradient for words E[739][22]:	mock grad = 0.000037927924717573, computed grad = 0.000037927935750477
Iteration: 1190, Checking gradient for hidden W[156][6]:	mock grad = 0.002076967100111959, computed grad = 0.002076967098203851
Iteration: 1190, Checking gradient for hidden b[0][37]:	mock grad = 0.040308339520483960, computed grad = 0.040308344672310148
Iteration: 1190, Checking gradient for output layer W[43][0]:	mock grad = 0.090324769998206822, computed grad = 0.090324769348036696
Iteration: 1191, Checking gradient for words E[74][36]:	mock grad = 0.000012973854074394, computed grad = 0.000012973896572210
Iteration: 1191, Checking gradient for hidden W[203][24]:	mock grad = -0.017531093101069706, computed grad = -0.017531093028314456
Iteration: 1191, Checking gradient for hidden b[0][32]:	mock grad = 0.054952594865237403, computed grad = 0.054952613285766568
Iteration: 1191, Checking gradient for output layer W[34][1]:	mock grad = -0.004579558347012069, computed grad = -0.004579558346992262
Iteration: 1192, Checking gradient for words E[102][37]:	mock grad = -0.014783792147032848, computed grad = -0.014783792129108258
Iteration: 1192, Checking gradient for hidden W[110][47]:	mock grad = 0.007074071864843212, computed grad = 0.007074071898298793
Iteration: 1192, Checking gradient for hidden b[0][49]:	mock grad = 0.027935843268017679, computed grad = 0.027935847526224807
Iteration: 1192, Checking gradient for output layer W[39][1]:	mock grad = 0.026241507890406579, computed grad = 0.026241507867913079
Iteration: 1193, Checking gradient for words E[757][16]:	mock grad = -0.018138753710816857, computed grad = -0.018798267359153572
Iteration: 1193, Checking gradient for hidden W[103][23]:	mock grad = 0.005913686561692666, computed grad = 0.005913686579171392
Iteration: 1193, Checking gradient for hidden b[0][35]:	mock grad = -0.011253977405978333, computed grad = -0.011253976462550240
Iteration: 1193, Checking gradient for output layer W[35][0]:	mock grad = 0.074056346918094373, computed grad = 0.074056346665115930
Iteration: 1194, Checking gradient for words E[660][25]:	mock grad = 0.007100518140790157, computed grad = 0.007100518163119383
Iteration: 1194, Checking gradient for hidden W[178][22]:	mock grad = -0.005334424145575500, computed grad = -0.005334424164532772
Iteration: 1194, Checking gradient for hidden b[0][31]:	mock grad = 0.031107057071516619, computed grad = 0.031107068761091670
Iteration: 1194, Checking gradient for output layer W[6][0]:	mock grad = 0.031699653667005556, computed grad = 0.031699653628320841
Iteration: 1195, Checking gradient for words E[49][45]:	mock grad = -0.004609153920864806, computed grad = -0.004609153910995405
Iteration: 1195, Checking gradient for hidden W[205][35]:	mock grad = 0.004346917770681191, computed grad = 0.004346917799459958
Iteration: 1195, Checking gradient for hidden b[0][36]:	mock grad = 0.000985692091115542, computed grad = 0.000985682400400244
Iteration: 1195, Checking gradient for output layer W[105][0]:	mock grad = -0.061632976669723227, computed grad = -0.061632976563855739
Iteration: 1196, Checking gradient for words E[770][45]:	mock grad = -0.003733106519632212, computed grad = -0.003733106529326265
Iteration: 1196, Checking gradient for hidden W[108][21]:	mock grad = 0.002974349947254540, computed grad = 0.002974349955299133
Iteration: 1196, Checking gradient for hidden b[0][40]:	mock grad = 0.089941052046738079, computed grad = 0.089941076633422615
Iteration: 1196, Checking gradient for output layer W[72][1]:	mock grad = 0.079801341523993985, computed grad = 0.079801341138597381
Iteration: 1197, Checking gradient for words E[31][20]:	mock grad = 0.031615594669548441, computed grad = 0.031615594835713468
Iteration: 1197, Checking gradient for hidden W[50][10]:	mock grad = 0.007444953771473806, computed grad = 0.007444953797275073
Iteration: 1197, Checking gradient for hidden b[0][33]:	mock grad = 0.113472978884032738, computed grad = 0.113473002479958604
Iteration: 1197, Checking gradient for output layer W[46][1]:	mock grad = -0.096496922763145143, computed grad = -0.096496922594648621
Iteration: 1198, Checking gradient for words E[757][46]:	mock grad = -0.004980129789056154, computed grad = -0.005288466058480976
Iteration: 1198, Checking gradient for hidden W[109][26]:	mock grad = 0.010002129316344366, computed grad = 0.010002129393275197
Iteration: 1198, Checking gradient for hidden b[0][36]:	mock grad = 0.004281753073931416, computed grad = 0.004281747202602680
Iteration: 1198, Checking gradient for output layer W[44][0]:	mock grad = 0.022495819959778007, computed grad = 0.022495819949566367
current: 100, Cost = 0.342664, Correct(%) = 1, time = 6.96931
Iteration: 1199, Checking gradient for words E[102][23]:	mock grad = -0.002678695223662375, computed grad = -0.002678695224381796
Iteration: 1199, Checking gradient for hidden W[103][3]:	mock grad = -0.003825784317401659, computed grad = -0.003825784355526534
Iteration: 1199, Checking gradient for hidden b[0][33]:	mock grad = -0.081704605014859544, computed grad = -0.081704614956821617
Iteration: 1199, Checking gradient for output layer W[51][1]:	mock grad = 0.118979087645326720, computed grad = 0.118979086651540777
current: 12, Correct(%) = 1, time = 7.02132
Dev start.
Dev finished. Total time taken is: 0.636692
dev:
Accuracy:	P=100/100=1
Test start.
Test finished. Total time taken is: 0.552492
test:
Accuracy:	P=64/100=0.64
##### Iteration 12
random: 0, 99
Iteration: 1200, Checking gradient for words E[19][39]:	mock grad = 0.016590547523426258, computed grad = 0.016590547496091637
Iteration: 1200, Checking gradient for hidden W[86][0]:	mock grad = 0.000340448025729079, computed grad = 0.000340447981199961
Iteration: 1200, Checking gradient for hidden b[0][14]:	mock grad = -0.035119189571497511, computed grad = -0.035119202119332643
Iteration: 1200, Checking gradient for output layer W[3][0]:	mock grad = 0.006078930923816839, computed grad = 0.006078930923463511
Iteration: 1201, Checking gradient for words E[62][15]:	mock grad = -0.010697191850206345, computed grad = -0.010697191809672107
Iteration: 1201, Checking gradient for hidden W[221][40]:	mock grad = 0.006273429489500604, computed grad = 0.006273429540092816
Iteration: 1201, Checking gradient for hidden b[0][1]:	mock grad = -0.096624066562411848, computed grad = -0.096624091393793493
Iteration: 1201, Checking gradient for output layer W[71][1]:	mock grad = 0.066514433925468630, computed grad = 0.066514433689682226
Iteration: 1202, Checking gradient for words E[11][21]:	mock grad = -0.006985871010778411, computed grad = -0.007861968652636876
Iteration: 1202, Checking gradient for hidden W[193][28]:	mock grad = 0.010399831463142606, computed grad = 0.010399831537454418
Iteration: 1202, Checking gradient for hidden b[0][45]:	mock grad = 0.007598458378665907, computed grad = 0.007598460240458261
Iteration: 1202, Checking gradient for output layer W[31][0]:	mock grad = -0.041639107548507859, computed grad = -0.041639107476098801
Iteration: 1203, Checking gradient for words E[11][32]:	mock grad = 0.011326168498276834, computed grad = 0.011326168538095907
Iteration: 1203, Checking gradient for hidden W[23][29]:	mock grad = 0.017198017253972075, computed grad = 0.017198017593758622
Iteration: 1203, Checking gradient for hidden b[0][12]:	mock grad = -0.071066062003771746, computed grad = -0.071066083221330067
Iteration: 1203, Checking gradient for output layer W[94][0]:	mock grad = 0.147073533761532538, computed grad = 0.147073532298263476
Iteration: 1204, Checking gradient for words E[102][40]:	mock grad = 0.010728747671884253, computed grad = 0.010728747727500772
Iteration: 1204, Checking gradient for hidden W[148][7]:	mock grad = 0.018804509735265151, computed grad = 0.018804509746764918
Iteration: 1204, Checking gradient for hidden b[0][5]:	mock grad = 0.009758388274744290, computed grad = 0.009758382290490074
Iteration: 1204, Checking gradient for output layer W[61][0]:	mock grad = 0.125326943982484451, computed grad = 0.125326942759704768
Iteration: 1205, Checking gradient for words E[114][43]:	mock grad = -0.011590521395626885, computed grad = -0.011590521438213134
Iteration: 1205, Checking gradient for hidden W[27][8]:	mock grad = -0.007021679713697937, computed grad = -0.007021679799510619
Iteration: 1205, Checking gradient for hidden b[0][28]:	mock grad = -0.018527665183676323, computed grad = -0.018527687184321337
Iteration: 1205, Checking gradient for output layer W[119][0]:	mock grad = -0.174348491945031103, computed grad = -0.174348488338778279
Iteration: 1206, Checking gradient for words E[267][16]:	mock grad = -0.006889161660167398, computed grad = -0.006889161698591652
Iteration: 1206, Checking gradient for hidden W[126][17]:	mock grad = 0.001864579029342561, computed grad = 0.001864579036639870
Iteration: 1206, Checking gradient for hidden b[0][16]:	mock grad = -0.014712779383918573, computed grad = -0.014712785954529335
Iteration: 1206, Checking gradient for output layer W[108][1]:	mock grad = 0.139626443044355009, computed grad = 0.139626439071650760
Iteration: 1207, Checking gradient for words E[69][14]:	mock grad = 0.019088789855681298, computed grad = 0.019088789942212775
Iteration: 1207, Checking gradient for hidden W[193][1]:	mock grad = -0.009522038634496299, computed grad = -0.009522038657596583
Iteration: 1207, Checking gradient for hidden b[0][12]:	mock grad = 0.086958859318336801, computed grad = 0.086958885707402678
Iteration: 1207, Checking gradient for output layer W[43][0]:	mock grad = 0.149529956909477679, computed grad = 0.149529956337876024
Iteration: 1208, Checking gradient for words E[285][32]:	mock grad = 0.032415289797138591, computed grad = 0.032415289809217269
Iteration: 1208, Checking gradient for hidden W[244][32]:	mock grad = 0.012509024619744569, computed grad = 0.012509024820585869
Iteration: 1208, Checking gradient for hidden b[0][40]:	mock grad = -0.079093213829511955, computed grad = -0.079093227452925741
Iteration: 1208, Checking gradient for output layer W[114][1]:	mock grad = 0.122988271191198928, computed grad = 0.122988269599715302
current: 10, Cost = 0.34353, Correct(%) = 1, time = 0.767613
Iteration: 1209, Checking gradient for words E[148][43]:	mock grad = 0.007217876239778986, computed grad = 0.007217876226464342
Iteration: 1209, Checking gradient for hidden W[192][1]:	mock grad = 0.011418820655084705, computed grad = 0.011418820665436626
Iteration: 1209, Checking gradient for hidden b[0][37]:	mock grad = 0.046928410486202221, computed grad = 0.046928417894977245
Iteration: 1209, Checking gradient for output layer W[116][1]:	mock grad = -0.140091620194460198, computed grad = -0.140091618585249461
Iteration: 1210, Checking gradient for words E[309][45]:	mock grad = 0.007701876995364065, computed grad = 0.007701877017090852
Iteration: 1210, Checking gradient for hidden W[227][21]:	mock grad = 0.004660076443674388, computed grad = 0.004660076471589917
Iteration: 1210, Checking gradient for hidden b[0][17]:	mock grad = -0.013636511611653246, computed grad = -0.013636514630465285
Iteration: 1210, Checking gradient for output layer W[53][1]:	mock grad = -0.104452820518186051, computed grad = -0.104452819132063321
Iteration: 1211, Checking gradient for words E[331][28]:	mock grad = 0.003064014143044913, computed grad = 0.003064014140466587
Iteration: 1211, Checking gradient for hidden W[118][22]:	mock grad = 0.007783817439654950, computed grad = 0.007783817493263308
Iteration: 1211, Checking gradient for hidden b[0][13]:	mock grad = -0.131814226096815812, computed grad = -0.131814263782656843
Iteration: 1211, Checking gradient for output layer W[71][0]:	mock grad = -0.133537879846212793, computed grad = -0.133537879062616410
Iteration: 1212, Checking gradient for words E[356][5]:	mock grad = -0.023528013956303218, computed grad = -0.023528013966212725
Iteration: 1212, Checking gradient for hidden W[226][17]:	mock grad = -0.001449131342728949, computed grad = -0.001449131341069906
Iteration: 1212, Checking gradient for hidden b[0][2]:	mock grad = -0.052612626390724504, computed grad = -0.052612643769976250
Iteration: 1212, Checking gradient for output layer W[128][1]:	mock grad = 0.039937793077815531, computed grad = 0.039937793014036924
Iteration: 1213, Checking gradient for words E[369][35]:	mock grad = 0.035895479149034015, computed grad = 0.035895479063667669
Iteration: 1213, Checking gradient for hidden W[60][0]:	mock grad = 0.002894199533642894, computed grad = 0.002894199564859780
Iteration: 1213, Checking gradient for hidden b[0][44]:	mock grad = -0.002122891580469410, computed grad = -0.002122900489426875
Iteration: 1213, Checking gradient for output layer W[146][0]:	mock grad = 0.049672701581793266, computed grad = 0.049672701457536002
Iteration: 1214, Checking gradient for words E[375][37]:	mock grad = -0.013730645485987569, computed grad = -0.013730645483865453
Iteration: 1214, Checking gradient for hidden W[249][7]:	mock grad = -0.007549507901161157, computed grad = -0.007549508103535234
Iteration: 1214, Checking gradient for hidden b[0][24]:	mock grad = 0.122141814878407029, computed grad = 0.122141846492972148
Iteration: 1214, Checking gradient for output layer W[73][0]:	mock grad = 0.007838753342814808, computed grad = 0.007838753342773865
Iteration: 1215, Checking gradient for words E[102][48]:	mock grad = -0.107427697784712173, computed grad = -0.107427697483969437
Iteration: 1215, Checking gradient for hidden W[105][36]:	mock grad = 0.005374338605396156, computed grad = 0.005374338629412737
Iteration: 1215, Checking gradient for hidden b[0][19]:	mock grad = -0.044557654190591212, computed grad = -0.044557664317219858
Iteration: 1215, Checking gradient for output layer W[41][1]:	mock grad = -0.055458812281722336, computed grad = -0.055458812174049785
Iteration: 1216, Checking gradient for words E[102][11]:	mock grad = 0.014451871672455097, computed grad = 0.014451871717022852
Iteration: 1216, Checking gradient for hidden W[116][0]:	mock grad = -0.000089823341375972, computed grad = -0.000089823328192238
Iteration: 1216, Checking gradient for hidden b[0][12]:	mock grad = -0.070378831394379837, computed grad = -0.070378853808399450
Iteration: 1216, Checking gradient for output layer W[33][0]:	mock grad = 0.014768403618914405, computed grad = 0.014768403617319879
Iteration: 1217, Checking gradient for words E[306][16]:	mock grad = 0.000811326836561155, computed grad = 0.000811326839354835
Iteration: 1217, Checking gradient for hidden W[76][19]:	mock grad = -0.000040249106922352, computed grad = -0.000040249091611484
Iteration: 1217, Checking gradient for hidden b[0][38]:	mock grad = -0.043223596651958962, computed grad = -0.043223603782207722
Iteration: 1217, Checking gradient for output layer W[27][1]:	mock grad = 0.021048887416885709, computed grad = 0.021048887402050704
Iteration: 1218, Checking gradient for words E[365][18]:	mock grad = 0.002097993367866646, computed grad = 0.002097993394619829
Iteration: 1218, Checking gradient for hidden W[217][44]:	mock grad = -0.000067415571763929, computed grad = -0.000067415571465095
Iteration: 1218, Checking gradient for hidden b[0][40]:	mock grad = -0.077833629508389235, computed grad = -0.077833647899388972
Iteration: 1218, Checking gradient for output layer W[141][1]:	mock grad = 0.107757966375437997, computed grad = 0.107757964950204316
current: 20, Cost = 0.440462, Correct(%) = 1, time = 1.42831
Iteration: 1219, Checking gradient for words E[119][32]:	mock grad = -0.003682296453255685, computed grad = -0.003682296422334603
Iteration: 1219, Checking gradient for hidden W[184][6]:	mock grad = 0.000604900190298574, computed grad = 0.000604900206166216
Iteration: 1219, Checking gradient for hidden b[0][35]:	mock grad = 0.011524504422905091, computed grad = 0.011524501832492359
Iteration: 1219, Checking gradient for output layer W[33][0]:	mock grad = -0.043672254779608677, computed grad = -0.043672254759384778
Iteration: 1220, Checking gradient for words E[124][25]:	mock grad = 0.052343101399232594, computed grad = 0.052479015321777400
Iteration: 1220, Checking gradient for hidden W[240][13]:	mock grad = 0.002645476739199237, computed grad = 0.002645476751223654
Iteration: 1220, Checking gradient for hidden b[0][45]:	mock grad = 0.008079875867234021, computed grad = 0.008079877498676138
Iteration: 1220, Checking gradient for output layer W[140][0]:	mock grad = -0.093942023647070050, computed grad = -0.093942023014107409
Iteration: 1221, Checking gradient for words E[6][38]:	mock grad = -0.032891035409376990, computed grad = -0.032891035313698816
Iteration: 1221, Checking gradient for hidden W[28][16]:	mock grad = 0.000417259332502473, computed grad = 0.000417259306801820
Iteration: 1221, Checking gradient for hidden b[0][39]:	mock grad = -0.099955422369707447, computed grad = -0.099955459356634885
Iteration: 1221, Checking gradient for output layer W[70][1]:	mock grad = -0.175640310680236222, computed grad = -0.175640309689532564
Iteration: 1222, Checking gradient for words E[351][41]:	mock grad = 0.025277263207251766, computed grad = 0.025277263054303882
Iteration: 1222, Checking gradient for hidden W[185][0]:	mock grad = -0.005397916603588904, computed grad = -0.005397916597230972
Iteration: 1222, Checking gradient for hidden b[0][44]:	mock grad = -0.003771222053167156, computed grad = -0.003771220106797750
Iteration: 1222, Checking gradient for output layer W[121][0]:	mock grad = -0.064447937157902091, computed grad = -0.064447936882210288
Iteration: 1223, Checking gradient for words E[166][42]:	mock grad = 0.010431459900483997, computed grad = 0.010431459931906552
Iteration: 1223, Checking gradient for hidden W[47][22]:	mock grad = 0.003864651325513613, computed grad = 0.003864651337147892
Iteration: 1223, Checking gradient for hidden b[0][18]:	mock grad = -0.053959791955715541, computed grad = -0.053959780542295992
Iteration: 1223, Checking gradient for output layer W[7][0]:	mock grad = 0.042036588628474592, computed grad = 0.042036588574925822
Iteration: 1224, Checking gradient for words E[205][28]:	mock grad = 0.006066801766985153, computed grad = 0.006066801726970022
Iteration: 1224, Checking gradient for hidden W[73][39]:	mock grad = -0.011485259878707410, computed grad = -0.011485259893757163
Iteration: 1224, Checking gradient for hidden b[0][14]:	mock grad = -0.054293052086873406, computed grad = -0.054293076212808829
Iteration: 1224, Checking gradient for output layer W[53][1]:	mock grad = 0.071110758595366308, computed grad = 0.071110758392964352
Iteration: 1225, Checking gradient for words E[316][42]:	mock grad = 0.001773299074636636, computed grad = 0.002559356146103916
Iteration: 1225, Checking gradient for hidden W[215][21]:	mock grad = 0.001177862111401140, computed grad = 0.001177862113487727
Iteration: 1225, Checking gradient for hidden b[0][10]:	mock grad = 0.083046190333974090, computed grad = 0.083046209647391567
Iteration: 1225, Checking gradient for output layer W[71][1]:	mock grad = 0.044551688246324828, computed grad = 0.044551688195102808
Iteration: 1226, Checking gradient for words E[237][46]:	mock grad = 0.019898453050032350, computed grad = 0.019898453045590334
Iteration: 1226, Checking gradient for hidden W[52][11]:	mock grad = -0.005016069533625434, computed grad = -0.005016069553552592
Iteration: 1226, Checking gradient for hidden b[0][23]:	mock grad = -0.095169851669657524, computed grad = -0.095169873727605991
Iteration: 1226, Checking gradient for output layer W[149][0]:	mock grad = -0.077242603905708362, computed grad = -0.077242603231875692
Iteration: 1227, Checking gradient for words E[68][16]:	mock grad = 0.056423450048415091, computed grad = 0.056423450216362039
Iteration: 1227, Checking gradient for hidden W[110][33]:	mock grad = -0.002096247485677827, computed grad = -0.002096247487275127
Iteration: 1227, Checking gradient for hidden b[0][31]:	mock grad = -0.046725772559291778, computed grad = -0.046725786760724947
Iteration: 1227, Checking gradient for output layer W[82][1]:	mock grad = 0.147286242943966972, computed grad = 0.147286242249221405
Iteration: 1228, Checking gradient for words E[207][6]:	mock grad = -0.001430773147043096, computed grad = -0.001430773170884925
Iteration: 1228, Checking gradient for hidden W[192][39]:	mock grad = 0.008028090891371020, computed grad = 0.008028090945087798
Iteration: 1228, Checking gradient for hidden b[0][29]:	mock grad = -0.050796440800943143, computed grad = -0.050796444301555096
Iteration: 1228, Checking gradient for output layer W[63][0]:	mock grad = -0.108180680037178734, computed grad = -0.108180678443979258
current: 30, Cost = 0.314729, Correct(%) = 1, time = 2.12232
Iteration: 1229, Checking gradient for words E[54][4]:	mock grad = -0.009253182989332753, computed grad = -0.009253182926720213
Iteration: 1229, Checking gradient for hidden W[98][18]:	mock grad = 0.008253517106315833, computed grad = 0.008253517449301050
Iteration: 1229, Checking gradient for hidden b[0][11]:	mock grad = 0.079373981820562989, computed grad = 0.079374002695546453
Iteration: 1229, Checking gradient for output layer W[142][0]:	mock grad = -0.123086020582058309, computed grad = -0.123086019150653495
Iteration: 1230, Checking gradient for words E[511][36]:	mock grad = -0.005863263894262394, computed grad = -0.005863263918012378
Iteration: 1230, Checking gradient for hidden W[191][39]:	mock grad = -0.010074354772016125, computed grad = -0.010074354928172840
Iteration: 1230, Checking gradient for hidden b[0][47]:	mock grad = -0.068969140011782004, computed grad = -0.068969156227999831
Iteration: 1230, Checking gradient for output layer W[100][0]:	mock grad = -0.042751520702322132, computed grad = -0.042751520635161383
Iteration: 1231, Checking gradient for words E[119][7]:	mock grad = -0.021148086878197692, computed grad = -0.021148086887008595
Iteration: 1231, Checking gradient for hidden W[170][30]:	mock grad = 0.006735564915844039, computed grad = 0.006735564955437344
Iteration: 1231, Checking gradient for hidden b[0][20]:	mock grad = 0.118150619737272811, computed grad = 0.118150654289596244
Iteration: 1231, Checking gradient for output layer W[108][1]:	mock grad = -0.140045444000119756, computed grad = -0.140045443274703585
Iteration: 1232, Checking gradient for words E[25][46]:	mock grad = -0.012805977677443625, computed grad = -0.012805977607492523
Iteration: 1232, Checking gradient for hidden W[149][36]:	mock grad = 0.008553045508924439, computed grad = 0.008553045584435171
Iteration: 1232, Checking gradient for hidden b[0][14]:	mock grad = -0.047391200001578593, computed grad = -0.047391220348480437
Iteration: 1232, Checking gradient for output layer W[46][1]:	mock grad = 0.056081195769663861, computed grad = 0.056081195623354452
Iteration: 1233, Checking gradient for words E[522][14]:	mock grad = 0.022689594809666147, computed grad = 0.022689594774879585
Iteration: 1233, Checking gradient for hidden W[170][4]:	mock grad = 0.004776749513130119, computed grad = 0.004776749530382443
Iteration: 1233, Checking gradient for hidden b[0][7]:	mock grad = -0.055990862512983464, computed grad = -0.055990867109968104
Iteration: 1233, Checking gradient for output layer W[30][0]:	mock grad = 0.004912372589760405, computed grad = 0.004912372589620298
Iteration: 1234, Checking gradient for words E[102][46]:	mock grad = -0.004379470695953236, computed grad = -0.004379470743195466
Iteration: 1234, Checking gradient for hidden W[100][45]:	mock grad = 0.002930882760571185, computed grad = 0.002930882805054276
Iteration: 1234, Checking gradient for hidden b[0][12]:	mock grad = 0.076322212014923174, computed grad = 0.076322236039380459
Iteration: 1234, Checking gradient for output layer W[89][1]:	mock grad = 0.173497962177693310, computed grad = 0.173497960350394415
Iteration: 1235, Checking gradient for words E[34][26]:	mock grad = 0.033749120564474655, computed grad = 0.033749120672074778
Iteration: 1235, Checking gradient for hidden W[242][35]:	mock grad = 0.000923128837870824, computed grad = 0.000923128837088426
Iteration: 1235, Checking gradient for hidden b[0][24]:	mock grad = -0.096708511043264167, computed grad = -0.096708521784273346
Iteration: 1235, Checking gradient for output layer W[5][0]:	mock grad = -0.019594927169103160, computed grad = -0.019594927166143857
Iteration: 1236, Checking gradient for words E[10][39]:	mock grad = 0.000765017059339845, computed grad = 0.000765016862500488
Iteration: 1236, Checking gradient for hidden W[24][0]:	mock grad = -0.026940281896786766, computed grad = -0.026940282142302860
Iteration: 1236, Checking gradient for hidden b[0][39]:	mock grad = 0.092447558500186044, computed grad = 0.092447587344569190
Iteration: 1236, Checking gradient for output layer W[36][0]:	mock grad = 0.047804915801918924, computed grad = 0.047804915776541078
Iteration: 1237, Checking gradient for words E[73][42]:	mock grad = 0.003256163177978166, computed grad = 0.003256163135272065
Iteration: 1237, Checking gradient for hidden W[203][39]:	mock grad = 0.000213646057789507, computed grad = 0.000213646061663643
Iteration: 1237, Checking gradient for hidden b[0][34]:	mock grad = 0.028248618752280130, computed grad = 0.028248628057540366
Iteration: 1237, Checking gradient for output layer W[10][0]:	mock grad = -0.018100629956080416, computed grad = -0.018100629952150820
Iteration: 1238, Checking gradient for words E[10][49]:	mock grad = 0.035198349449955257, computed grad = 0.035198349577545140
Iteration: 1238, Checking gradient for hidden W[54][32]:	mock grad = 0.009092782597919236, computed grad = 0.009092782656158163
Iteration: 1238, Checking gradient for hidden b[0][34]:	mock grad = -0.029075388032734351, computed grad = -0.029075391418290783
Iteration: 1238, Checking gradient for output layer W[16][0]:	mock grad = 0.016658711294692541, computed grad = 0.016658711293013169
current: 40, Cost = 0.257839, Correct(%) = 1, time = 2.55844
Iteration: 1239, Checking gradient for words E[102][7]:	mock grad = 0.000220703994607829, computed grad = 0.000220704033707260
Iteration: 1239, Checking gradient for hidden W[241][39]:	mock grad = 0.001206403881570672, computed grad = 0.001206403861105002
Iteration: 1239, Checking gradient for hidden b[0][21]:	mock grad = 0.038215189142876138, computed grad = 0.038215197157369173
Iteration: 1239, Checking gradient for output layer W[146][1]:	mock grad = -0.004213333125335783, computed grad = -0.004213333125268065
Iteration: 1240, Checking gradient for words E[42][10]:	mock grad = -0.013791727240752838, computed grad = -0.013791727231780865
Iteration: 1240, Checking gradient for hidden W[150][13]:	mock grad = -0.002845895368003282, computed grad = -0.002845895391208257
Iteration: 1240, Checking gradient for hidden b[0][2]:	mock grad = -0.038284462661491991, computed grad = -0.038284468954673793
Iteration: 1240, Checking gradient for output layer W[136][0]:	mock grad = -0.099961243175739067, computed grad = -0.099961241887707483
Iteration: 1241, Checking gradient for words E[559][14]:	mock grad = 0.024279295466028694, computed grad = 0.024279295414941379
Iteration: 1241, Checking gradient for hidden W[53][38]:	mock grad = 0.008158556756371338, computed grad = 0.008158556865874694
Iteration: 1241, Checking gradient for hidden b[0][36]:	mock grad = -0.010276467972747394, computed grad = -0.010276464441805579
Iteration: 1241, Checking gradient for output layer W[132][0]:	mock grad = -0.081213109296679464, computed grad = -0.081213109183961060
Iteration: 1242, Checking gradient for words E[6][30]:	mock grad = -0.003620048705021972, computed grad = -0.003620048676821544
Iteration: 1242, Checking gradient for hidden W[125][13]:	mock grad = 0.015556060707244690, computed grad = 0.015556060876244244
Iteration: 1242, Checking gradient for hidden b[0][16]:	mock grad = 0.015024990322520804, computed grad = 0.015024991352555665
Iteration: 1242, Checking gradient for output layer W[49][1]:	mock grad = -0.009672706576369094, computed grad = -0.009672706576061821
Iteration: 1243, Checking gradient for words E[78][34]:	mock grad = -0.016194780543293819, computed grad = -0.016194780549634236
Iteration: 1243, Checking gradient for hidden W[143][46]:	mock grad = 0.012824406558531454, computed grad = 0.012824406612422118
Iteration: 1243, Checking gradient for hidden b[0][21]:	mock grad = -0.059677765041787900, computed grad = -0.059677778045624880
Iteration: 1243, Checking gradient for output layer W[79][0]:	mock grad = -0.145551048278513839, computed grad = -0.145551047630076286
Iteration: 1244, Checking gradient for words E[500][27]:	mock grad = 0.002203920300980489, computed grad = 0.002203920304582915
Iteration: 1244, Checking gradient for hidden W[60][1]:	mock grad = -0.005595013265191628, computed grad = -0.005595013371774492
Iteration: 1244, Checking gradient for hidden b[0][20]:	mock grad = -0.067108789499947807, computed grad = -0.067108808587162269
Iteration: 1244, Checking gradient for output layer W[92][1]:	mock grad = -0.033365190375139742, computed grad = -0.033365190304894564
Iteration: 1245, Checking gradient for words E[222][42]:	mock grad = 0.023388850938527161, computed grad = 0.023388850934108214
Iteration: 1245, Checking gradient for hidden W[45][16]:	mock grad = -0.001956045765971748, computed grad = -0.001956045802037908
Iteration: 1245, Checking gradient for hidden b[0][18]:	mock grad = 0.081942545937491174, computed grad = 0.081942565832915898
Iteration: 1245, Checking gradient for output layer W[129][1]:	mock grad = -0.059222300274885953, computed grad = -0.059222300187351515
Iteration: 1246, Checking gradient for words E[405][43]:	mock grad = 0.006596833404814983, computed grad = 0.006596833411492859
Iteration: 1246, Checking gradient for hidden W[104][21]:	mock grad = -0.001053376551485297, computed grad = -0.001053376551825374
Iteration: 1246, Checking gradient for hidden b[0][43]:	mock grad = -0.008193573110715224, computed grad = -0.008193575316692769
Iteration: 1246, Checking gradient for output layer W[81][1]:	mock grad = 0.106027739226188800, computed grad = 0.106027737600006194
Iteration: 1247, Checking gradient for words E[102][33]:	mock grad = -0.006042427502628289, computed grad = -0.006042427528510814
Iteration: 1247, Checking gradient for hidden W[116][39]:	mock grad = 0.004463019025624515, computed grad = 0.004463019070245961
Iteration: 1247, Checking gradient for hidden b[0][42]:	mock grad = 0.054886402388043543, computed grad = 0.054886420216654790
Iteration: 1247, Checking gradient for output layer W[27][1]:	mock grad = 0.034816162893219760, computed grad = 0.034816162853513209
Iteration: 1248, Checking gradient for words E[166][18]:	mock grad = 0.010513895472491486, computed grad = 0.010513895500601287
Iteration: 1248, Checking gradient for hidden W[140][36]:	mock grad = 0.006430668667206652, computed grad = 0.006430668706938127
Iteration: 1248, Checking gradient for hidden b[0][37]:	mock grad = 0.043134024185154107, computed grad = 0.043134033378678494
Iteration: 1248, Checking gradient for output layer W[87][0]:	mock grad = -0.099696182089187291, computed grad = -0.099696180968866899
current: 50, Cost = 0.258556, Correct(%) = 1, time = 3.44766
Iteration: 1249, Checking gradient for words E[6][19]:	mock grad = 0.007121915570951209, computed grad = 0.007121915595212854
Iteration: 1249, Checking gradient for hidden W[101][35]:	mock grad = -0.000019337425521959, computed grad = 0.000579875781338627
Iteration: 1249, Checking gradient for hidden b[0][26]:	mock grad = -0.018677534767364001, computed grad = -0.018677534345177910
Iteration: 1249, Checking gradient for output layer W[82][1]:	mock grad = 0.139467234638507298, computed grad = 0.139467230977501161
Iteration: 1250, Checking gradient for words E[102][15]:	mock grad = 0.005525960453695022, computed grad = 0.005525960478551594
Iteration: 1250, Checking gradient for hidden W[117][49]:	mock grad = 0.009991479010457160, computed grad = 0.009991479213274588
Iteration: 1250, Checking gradient for hidden b[0][0]:	mock grad = -0.190805780863306795, computed grad = -0.190805837540035217
Iteration: 1250, Checking gradient for output layer W[72][1]:	mock grad = 0.109123243332009956, computed grad = 0.109123243008554152
Iteration: 1251, Checking gradient for words E[131][35]:	mock grad = 0.000986032712396723, computed grad = 0.000986032744094237
Iteration: 1251, Checking gradient for hidden W[4][46]:	mock grad = -0.022404182710628051, computed grad = -0.022404182944180284
Iteration: 1251, Checking gradient for hidden b[0][31]:	mock grad = 0.036517447909584222, computed grad = 0.036517460323743020
Iteration: 1251, Checking gradient for output layer W[99][0]:	mock grad = 0.163170987800026479, computed grad = 0.163170985249106837
Iteration: 1252, Checking gradient for words E[211][37]:	mock grad = 0.008552820529750260, computed grad = 0.008552820536062189
Iteration: 1252, Checking gradient for hidden W[101][20]:	mock grad = 0.010615497250621164, computed grad = 0.010615497327462886
Iteration: 1252, Checking gradient for hidden b[0][43]:	mock grad = 0.011054473048000002, computed grad = 0.011054475973786972
Iteration: 1252, Checking gradient for output layer W[138][0]:	mock grad = -0.107814452926568993, computed grad = -0.107814452321536830
Iteration: 1253, Checking gradient for words E[315][30]:	mock grad = -0.004172329213847359, computed grad = -0.004172329205701200
Iteration: 1253, Checking gradient for hidden W[118][20]:	mock grad = -0.000040795779615577, computed grad = -0.000040795810003371
Iteration: 1253, Checking gradient for hidden b[0][20]:	mock grad = 0.085459565090123535, computed grad = 0.085459583861455538
Iteration: 1253, Checking gradient for output layer W[45][0]:	mock grad = 0.001004648377350703, computed grad = 0.001004648377330441
Iteration: 1254, Checking gradient for words E[628][23]:	mock grad = 0.011733812089631535, computed grad = 0.011733812106119510
Iteration: 1254, Checking gradient for hidden W[189][8]:	mock grad = -0.016820014993285115, computed grad = -0.016820014987159983
Iteration: 1254, Checking gradient for hidden b[0][32]:	mock grad = -0.040480785957835996, computed grad = -0.040480799047269596
Iteration: 1254, Checking gradient for output layer W[102][0]:	mock grad = -0.148652178651842437, computed grad = -0.148652174026619316
Iteration: 1255, Checking gradient for words E[102][22]:	mock grad = -0.004326037385316006, computed grad = -0.004326037427411500
Iteration: 1255, Checking gradient for hidden W[188][37]:	mock grad = -0.002440711236895154, computed grad = -0.002440711283443854
Iteration: 1255, Checking gradient for hidden b[0][47]:	mock grad = 0.062492366784516751, computed grad = 0.062492384112889383
Iteration: 1255, Checking gradient for output layer W[132][1]:	mock grad = -0.019249330792642239, computed grad = -0.019249330784695914
Iteration: 1256, Checking gradient for words E[49][17]:	mock grad = -0.046616173629360214, computed grad = -0.046616173652729603
Iteration: 1256, Checking gradient for hidden W[102][42]:	mock grad = -0.010329529132074855, computed grad = -0.010329529207932658
Iteration: 1256, Checking gradient for hidden b[0][1]:	mock grad = 0.132033593122360404, computed grad = 0.132033644390791266
Iteration: 1256, Checking gradient for output layer W[26][1]:	mock grad = 0.035400470262098027, computed grad = 0.035400470254203745
Iteration: 1257, Checking gradient for words E[451][42]:	mock grad = 0.007936717833501028, computed grad = 0.007936717859469193
Iteration: 1257, Checking gradient for hidden W[156][17]:	mock grad = -0.002748705137528562, computed grad = -0.002748705162892288
Iteration: 1257, Checking gradient for hidden b[0][45]:	mock grad = -0.006244862061854928, computed grad = -0.006244861719037926
Iteration: 1257, Checking gradient for output layer W[12][0]:	mock grad = -0.042774271799661534, computed grad = -0.042774271739403409
Iteration: 1258, Checking gradient for words E[102][15]:	mock grad = 0.009355540145805641, computed grad = 0.009355540136135496
Iteration: 1258, Checking gradient for hidden W[220][7]:	mock grad = -0.003944344820677737, computed grad = -0.003944344814430860
Iteration: 1258, Checking gradient for hidden b[0][24]:	mock grad = 0.087550120223461869, computed grad = 0.087550143221981888
Iteration: 1258, Checking gradient for output layer W[82][1]:	mock grad = -0.174558256120005417, computed grad = -0.174558253405212083
current: 60, Cost = 0.335105, Correct(%) = 1, time = 4.14489
Iteration: 1259, Checking gradient for words E[102][46]:	mock grad = 0.021502823478874999, computed grad = 0.021502823467304911
Iteration: 1259, Checking gradient for hidden W[239][21]:	mock grad = -0.003987812594724582, computed grad = -0.003987812713959162
Iteration: 1259, Checking gradient for hidden b[0][2]:	mock grad = 0.051417625538008327, computed grad = 0.051417652998159799
Iteration: 1259, Checking gradient for output layer W[139][1]:	mock grad = -0.046758054414841599, computed grad = -0.046758054350098041
Iteration: 1260, Checking gradient for words E[654][49]:	mock grad = -0.001826214314215591, computed grad = -0.002657269569464388
Iteration: 1260, Checking gradient for hidden W[51][34]:	mock grad = 0.011954810968828333, computed grad = 0.011954811000523416
Iteration: 1260, Checking gradient for hidden b[0][33]:	mock grad = -0.096236153027590143, computed grad = -0.096236167821977692
Iteration: 1260, Checking gradient for output layer W[60][0]:	mock grad = -0.121635812186465797, computed grad = -0.121635811507899219
Iteration: 1261, Checking gradient for words E[107][46]:	mock grad = 0.005348489940049106, computed grad = 0.005348489965499751
Iteration: 1261, Checking gradient for hidden W[121][22]:	mock grad = 0.003636102764270621, computed grad = 0.003636102779107748
Iteration: 1261, Checking gradient for hidden b[0][29]:	mock grad = 0.081113036426189167, computed grad = 0.081113062835021413
Iteration: 1261, Checking gradient for output layer W[146][0]:	mock grad = -0.009797490319984892, computed grad = -0.009797490319549133
Iteration: 1262, Checking gradient for words E[102][13]:	mock grad = -0.049849334649854082, computed grad = -0.049849334577002503
Iteration: 1262, Checking gradient for hidden W[232][42]:	mock grad = -0.004882375816306350, computed grad = -0.004882375824185826
Iteration: 1262, Checking gradient for hidden b[0][19]:	mock grad = -0.048063966334166430, computed grad = -0.048063980112835906
Iteration: 1262, Checking gradient for output layer W[86][1]:	mock grad = 0.008962809568635599, computed grad = 0.008962809568209504
Iteration: 1263, Checking gradient for words E[69][24]:	mock grad = 0.011350519880826138, computed grad = 0.011414949240645006
Iteration: 1263, Checking gradient for hidden W[173][5]:	mock grad = 0.010834513670399115, computed grad = 0.006509810826448171
Iteration: 1263, Checking gradient for hidden b[0][24]:	mock grad = 0.090660518962754066, computed grad = 0.090660544973767548
Iteration: 1263, Checking gradient for output layer W[83][0]:	mock grad = 0.124602066029655401, computed grad = 0.124602064940582624
Iteration: 1264, Checking gradient for words E[112][29]:	mock grad = 0.001576314250684163, computed grad = 0.001576314193804447
Iteration: 1264, Checking gradient for hidden W[247][11]:	mock grad = 0.007154698205752430, computed grad = 0.007154698289414230
Iteration: 1264, Checking gradient for hidden b[0][42]:	mock grad = 0.067632633445752210, computed grad = 0.067632654452172161
Iteration: 1264, Checking gradient for output layer W[120][0]:	mock grad = -0.068860139979975976, computed grad = -0.068860139848523100
Iteration: 1265, Checking gradient for words E[358][37]:	mock grad = 0.004332381985161504, computed grad = 0.004332382035022373
Iteration: 1265, Checking gradient for hidden W[2][37]:	mock grad = -0.002267454148802139, computed grad = -0.002267454218179833
Iteration: 1265, Checking gradient for hidden b[0][7]:	mock grad = 0.055350264303796748, computed grad = 0.055350277606356330
Iteration: 1265, Checking gradient for output layer W[121][1]:	mock grad = -0.041050776482476348, computed grad = -0.041050776367584613
Iteration: 1266, Checking gradient for words E[776][6]:	mock grad = -0.018010955872505008, computed grad = -0.018010955899242849
Iteration: 1266, Checking gradient for hidden W[43][37]:	mock grad = 0.002943038429320533, computed grad = 0.002943038462413058
Iteration: 1266, Checking gradient for hidden b[0][14]:	mock grad = -0.042995934132983304, computed grad = -0.042995950159206847
Iteration: 1266, Checking gradient for output layer W[102][1]:	mock grad = -0.134434932547222719, computed grad = -0.134434930525793861
Iteration: 1267, Checking gradient for words E[25][31]:	mock grad = -0.016406647043276656, computed grad = -0.016406646845771471
Iteration: 1267, Checking gradient for hidden W[125][30]:	mock grad = 0.001700190431053183, computed grad = 0.001700190433703205
Iteration: 1267, Checking gradient for hidden b[0][33]:	mock grad = 0.087598678411321407, computed grad = 0.087598696968438361
Iteration: 1267, Checking gradient for output layer W[32][0]:	mock grad = 0.093907892442735008, computed grad = 0.093907891955441886
Iteration: 1268, Checking gradient for words E[664][26]:	mock grad = -0.012240818096087569, computed grad = -0.012240818104625876
Iteration: 1268, Checking gradient for hidden W[34][4]:	mock grad = -0.000344234741489480, computed grad = -0.000344234761010324
Iteration: 1268, Checking gradient for hidden b[0][43]:	mock grad = 0.008767861011310663, computed grad = 0.008767863273150756
Iteration: 1268, Checking gradient for output layer W[134][1]:	mock grad = 0.042734497823049278, computed grad = 0.042734497740772780
current: 70, Cost = 0.258587, Correct(%) = 1, time = 4.75713
Iteration: 1269, Checking gradient for words E[118][30]:	mock grad = -0.031274118549612373, computed grad = -0.031274118575526054
Iteration: 1269, Checking gradient for hidden W[146][10]:	mock grad = -0.001537007137536461, computed grad = -0.001537007127630044
Iteration: 1269, Checking gradient for hidden b[0][11]:	mock grad = 0.066541483598192563, computed grad = 0.066541500176736865
Iteration: 1269, Checking gradient for output layer W[53][1]:	mock grad = -0.063934556193967484, computed grad = -0.063934555841355253
Iteration: 1270, Checking gradient for words E[259][4]:	mock grad = -0.006924860871865279, computed grad = -0.006924860748979352
Iteration: 1270, Checking gradient for hidden W[217][4]:	mock grad = -0.001538719745758366, computed grad = -0.001538719753928206
Iteration: 1270, Checking gradient for hidden b[0][7]:	mock grad = -0.071338231205392288, computed grad = -0.071338235150781845
Iteration: 1270, Checking gradient for output layer W[85][0]:	mock grad = 0.147267035260634227, computed grad = 0.147267034010049508
Iteration: 1271, Checking gradient for words E[687][27]:	mock grad = -0.000349283062445194, computed grad = -0.000349283063843174
Iteration: 1271, Checking gradient for hidden W[41][8]:	mock grad = -0.015568730307946588, computed grad = -0.015568730458155440
Iteration: 1271, Checking gradient for hidden b[0][7]:	mock grad = -0.077973736374109537, computed grad = -0.077973747627184689
Iteration: 1271, Checking gradient for output layer W[113][0]:	mock grad = -0.149785689759929230, computed grad = -0.149785688251613369
Iteration: 1272, Checking gradient for words E[34][0]:	mock grad = -0.012431916047411784, computed grad = -0.012431916106698863
Iteration: 1272, Checking gradient for hidden W[218][44]:	mock grad = 0.015436689199094245, computed grad = 0.015436689247183574
Iteration: 1272, Checking gradient for hidden b[0][14]:	mock grad = 0.066056101231004583, computed grad = 0.066056127010688429
Iteration: 1272, Checking gradient for output layer W[7][1]:	mock grad = -0.043129600621333308, computed grad = -0.043129600588259723
Iteration: 1273, Checking gradient for words E[231][8]:	mock grad = 0.004776751189039530, computed grad = 0.004776751134437408
Iteration: 1273, Checking gradient for hidden W[95][43]:	mock grad = 0.000631412685303534, computed grad = 0.000631412687358389
Iteration: 1273, Checking gradient for hidden b[0][13]:	mock grad = -0.140691803783454805, computed grad = -0.140691848983294704
Iteration: 1273, Checking gradient for output layer W[105][0]:	mock grad = 0.013031520050971190, computed grad = 0.013031520050395380
Iteration: 1274, Checking gradient for words E[73][9]:	mock grad = -0.023157816767149786, computed grad = -0.023157816846818745
Iteration: 1274, Checking gradient for hidden W[119][27]:	mock grad = -0.015735889999785035, computed grad = -0.015735890353388408
Iteration: 1274, Checking gradient for hidden b[0][4]:	mock grad = -0.022330558976657588, computed grad = -0.022330568499575879
Iteration: 1274, Checking gradient for output layer W[93][0]:	mock grad = -0.039698036218144273, computed grad = -0.039698036198332072
Iteration: 1275, Checking gradient for words E[696][2]:	mock grad = -0.010372078695974896, computed grad = -0.010372078723252177
Iteration: 1275, Checking gradient for hidden W[212][17]:	mock grad = 0.000868236072287809, computed grad = 0.000868236072392789
Iteration: 1275, Checking gradient for hidden b[0][1]:	mock grad = 0.082902728463229991, computed grad = 0.082902757873788036
Iteration: 1275, Checking gradient for output layer W[32][0]:	mock grad = 0.073700811256072196, computed grad = 0.073700810896149674
Iteration: 1276, Checking gradient for words E[167][48]:	mock grad = 0.033595697819688297, computed grad = 0.033595697821122969
Iteration: 1276, Checking gradient for hidden W[8][45]:	mock grad = 0.001081161590144042, computed grad = 0.001081161593537586
Iteration: 1276, Checking gradient for hidden b[0][49]:	mock grad = 0.038533237764565298, computed grad = 0.038533247306001085
Iteration: 1276, Checking gradient for output layer W[29][0]:	mock grad = 0.022534038017607561, computed grad = 0.022534038010639384
Iteration: 1277, Checking gradient for words E[397][30]:	mock grad = -0.035745541792892022, computed grad = -0.035745541581553679
Iteration: 1277, Checking gradient for hidden W[237][32]:	mock grad = -0.000233241046532218, computed grad = -0.000233241046874118
Iteration: 1277, Checking gradient for hidden b[0][44]:	mock grad = 0.005402497994549949, computed grad = 0.005402498601085645
Iteration: 1277, Checking gradient for output layer W[126][1]:	mock grad = -0.053190893343879608, computed grad = -0.053190893152326502
Iteration: 1278, Checking gradient for words E[25][6]:	mock grad = 0.022649238867550325, computed grad = 0.022649238838795160
Iteration: 1278, Checking gradient for hidden W[240][30]:	mock grad = -0.002149264404061046, computed grad = -0.002149264419016578
Iteration: 1278, Checking gradient for hidden b[0][30]:	mock grad = 0.002676839216481097, computed grad = 0.002676839434155718
Iteration: 1278, Checking gradient for output layer W[73][0]:	mock grad = 0.088433268724624758, computed grad = 0.088433268278880248
current: 80, Cost = 0.286721, Correct(%) = 1, time = 5.24304
Iteration: 1279, Checking gradient for words E[396][19]:	mock grad = -0.009930477821107342, computed grad = -0.008779006523493196
Iteration: 1279, Checking gradient for hidden W[188][9]:	mock grad = -0.001930612348877148, computed grad = -0.001930612331017445
Iteration: 1279, Checking gradient for hidden b[0][36]:	mock grad = -0.009938913353546575, computed grad = -0.009938914185076338
Iteration: 1279, Checking gradient for output layer W[109][0]:	mock grad = 0.051985469663728967, computed grad = 0.051985469521904977
Iteration: 1280, Checking gradient for words E[458][1]:	mock grad = 0.012077036398661090, computed grad = 0.012077036411912467
Iteration: 1280, Checking gradient for hidden W[78][29]:	mock grad = 0.010253083334937907, computed grad = 0.010253083389403993
Iteration: 1280, Checking gradient for hidden b[0][34]:	mock grad = 0.024710798973676118, computed grad = 0.024710804250498868
Iteration: 1280, Checking gradient for output layer W[137][1]:	mock grad = 0.116526821160672922, computed grad = 0.116526820151445609
Iteration: 1281, Checking gradient for words E[715][49]:	mock grad = -0.000685874134015441, computed grad = -0.000685874142142574
Iteration: 1281, Checking gradient for hidden W[11][2]:	mock grad = 0.006626856515173918, computed grad = 0.006626856548598405
Iteration: 1281, Checking gradient for hidden b[0][7]:	mock grad = 0.063176324137853568, computed grad = 0.063176335775139161
Iteration: 1281, Checking gradient for output layer W[103][1]:	mock grad = -0.089494919829347097, computed grad = -0.089494919214844385
Iteration: 1282, Checking gradient for words E[250][35]:	mock grad = -0.008233417507719221, computed grad = -0.008233417514439833
Iteration: 1282, Checking gradient for hidden W[118][43]:	mock grad = 0.001195782166535375, computed grad = 0.001195782173701294
Iteration: 1282, Checking gradient for hidden b[0][8]:	mock grad = 0.031673838055856773, computed grad = 0.031673860328363126
Iteration: 1282, Checking gradient for output layer W[128][1]:	mock grad = -0.074204608221883905, computed grad = -0.074204607922379845
Iteration: 1283, Checking gradient for words E[34][22]:	mock grad = 0.000489111565138423, computed grad = 0.000489111546468865
Iteration: 1283, Checking gradient for hidden W[184][22]:	mock grad = -0.002242435633903472, computed grad = -0.002242435636891707
Iteration: 1283, Checking gradient for hidden b[0][0]:	mock grad = -0.161409402918422096, computed grad = -0.161409450259244081
Iteration: 1283, Checking gradient for output layer W[137][0]:	mock grad = 0.164248764009444770, computed grad = 0.164248761946005101
Iteration: 1284, Checking gradient for words E[293][11]:	mock grad = -0.006570238151454300, computed grad = -0.006570238193111234
Iteration: 1284, Checking gradient for hidden W[213][29]:	mock grad = 0.015044712803530258, computed grad = 0.015044713017635648
Iteration: 1284, Checking gradient for hidden b[0][1]:	mock grad = -0.086512029464885787, computed grad = -0.086512044290333268
Iteration: 1284, Checking gradient for output layer W[109][0]:	mock grad = 0.069088813690221240, computed grad = 0.069088813371517177
Iteration: 1285, Checking gradient for words E[571][37]:	mock grad = 0.009712845791043367, computed grad = 0.009712845818985648
Iteration: 1285, Checking gradient for hidden W[39][27]:	mock grad = 0.007410552831266237, computed grad = 0.007410552844753540
Iteration: 1285, Checking gradient for hidden b[0][44]:	mock grad = 0.013551618676777011, computed grad = 0.013551631222801862
Iteration: 1285, Checking gradient for output layer W[95][0]:	mock grad = 0.103730522768324951, computed grad = 0.103730521672899270
Iteration: 1286, Checking gradient for words E[48][37]:	mock grad = -0.026188076056532505, computed grad = -0.026188076027211064
Iteration: 1286, Checking gradient for hidden W[239][34]:	mock grad = -0.008719080137803426, computed grad = -0.008719080174396430
Iteration: 1286, Checking gradient for hidden b[0][5]:	mock grad = -0.008762687044389095, computed grad = -0.008762682056028818
Iteration: 1286, Checking gradient for output layer W[119][1]:	mock grad = -0.083270602789176085, computed grad = -0.083270602384353562
Iteration: 1287, Checking gradient for words E[219][33]:	mock grad = 0.022222771758062532, computed grad = 0.022222771819735650
Iteration: 1287, Checking gradient for hidden W[22][22]:	mock grad = 0.007446787525344778, computed grad = 0.007446787578411326
Iteration: 1287, Checking gradient for hidden b[0][3]:	mock grad = -0.036649485175621610, computed grad = -0.036649496045456725
Iteration: 1287, Checking gradient for output layer W[90][0]:	mock grad = -0.035274554649217249, computed grad = -0.035274554619933507
Iteration: 1288, Checking gradient for words E[742][33]:	mock grad = -0.020926628365641742, computed grad = -0.020926628403170534
Iteration: 1288, Checking gradient for hidden W[146][44]:	mock grad = -0.027277391513230720, computed grad = -0.027277391227941615
Iteration: 1288, Checking gradient for hidden b[0][4]:	mock grad = -0.019695304906974798, computed grad = -0.019695314332834241
Iteration: 1288, Checking gradient for output layer W[65][0]:	mock grad = 0.134477210803229941, computed grad = 0.134477209021546557
current: 90, Cost = 0.328844, Correct(%) = 1, time = 6.19211
Iteration: 1289, Checking gradient for words E[619][3]:	mock grad = 0.016608448598520731, computed grad = 0.016608448592722751
Iteration: 1289, Checking gradient for hidden W[195][35]:	mock grad = 0.000573160012812268, computed grad = 0.000573160051980480
Iteration: 1289, Checking gradient for hidden b[0][14]:	mock grad = 0.050421110520948442, computed grad = 0.050421125707150287
Iteration: 1289, Checking gradient for output layer W[133][0]:	mock grad = -0.096673760260179709, computed grad = -0.096673759653680372
Iteration: 1290, Checking gradient for words E[97][34]:	mock grad = 0.007515654324052878, computed grad = 0.007515654330750207
Iteration: 1290, Checking gradient for hidden W[49][5]:	mock grad = 0.003076196344292681, computed grad = 0.003076196246767907
Iteration: 1290, Checking gradient for hidden b[0][11]:	mock grad = -0.074724912422136081, computed grad = -0.074724934939707097
Iteration: 1290, Checking gradient for output layer W[101][1]:	mock grad = -0.141196631813750306, computed grad = -0.141196629083764652
Iteration: 1291, Checking gradient for words E[97][37]:	mock grad = 0.027132472991797663, computed grad = 0.025601650342330407
Iteration: 1291, Checking gradient for hidden W[64][3]:	mock grad = -0.007324377714917896, computed grad = -0.007324377752083872
Iteration: 1291, Checking gradient for hidden b[0][40]:	mock grad = 0.093844767166395560, computed grad = 0.093844790765138117
Iteration: 1291, Checking gradient for output layer W[2][0]:	mock grad = 0.043755137136142208, computed grad = 0.043755137089640211
Iteration: 1292, Checking gradient for words E[486][44]:	mock grad = 0.000380687927792334, computed grad = 0.000380687920778800
Iteration: 1292, Checking gradient for hidden W[210][31]:	mock grad = 0.007906321986217435, computed grad = 0.007906321961676009
Iteration: 1292, Checking gradient for hidden b[0][16]:	mock grad = 0.013671354827188287, computed grad = 0.013671359411908461
Iteration: 1292, Checking gradient for output layer W[44][0]:	mock grad = -0.002342059458509960, computed grad = -0.002342059458490953
Iteration: 1293, Checking gradient for words E[287][27]:	mock grad = 0.003178925436247981, computed grad = 0.003178925428306027
Iteration: 1293, Checking gradient for hidden W[236][7]:	mock grad = 0.006448542725356887, computed grad = 0.006448542728108848
Iteration: 1293, Checking gradient for hidden b[0][18]:	mock grad = -0.056342894304922675, computed grad = -0.056342887024108654
Iteration: 1293, Checking gradient for output layer W[46][0]:	mock grad = 0.057769822404019067, computed grad = 0.057769822273606421
Iteration: 1294, Checking gradient for words E[444][15]:	mock grad = 0.007027703307987965, computed grad = 0.007027703317321036
Iteration: 1294, Checking gradient for hidden W[78][5]:	mock grad = -0.000490429216931654, computed grad = -0.000490429230256804
Iteration: 1294, Checking gradient for hidden b[0][21]:	mock grad = 0.036091245482566814, computed grad = 0.036091251373574690
Iteration: 1294, Checking gradient for output layer W[80][1]:	mock grad = -0.106567889296416141, computed grad = -0.106567887678737511
Iteration: 1295, Checking gradient for words E[354][32]:	mock grad = -0.002478189617849402, computed grad = -0.003287371946129316
Iteration: 1295, Checking gradient for hidden W[117][24]:	mock grad = -0.003176553785205760, computed grad = -0.003176553769369533
Iteration: 1295, Checking gradient for hidden b[0][19]:	mock grad = 0.036522551510764778, computed grad = 0.036522547305150144
Iteration: 1295, Checking gradient for output layer W[2][0]:	mock grad = -0.029293269665414678, computed grad = -0.029293269652978654
Iteration: 1296, Checking gradient for words E[197][1]:	mock grad = -0.001044198240818250, computed grad = -0.001044198260128184
Iteration: 1296, Checking gradient for hidden W[49][8]:	mock grad = -0.002715503205158454, computed grad = -0.002715503193403424
Iteration: 1296, Checking gradient for hidden b[0][27]:	mock grad = 0.029066805268707085, computed grad = 0.029066821171675153
Iteration: 1296, Checking gradient for output layer W[94][0]:	mock grad = -0.107695468748891798, computed grad = -0.107695467690181026
Iteration: 1297, Checking gradient for words E[473][28]:	mock grad = 0.035978057996127744, computed grad = 0.035978058093771845
Iteration: 1297, Checking gradient for hidden W[107][45]:	mock grad = -0.001253256431132943, computed grad = -0.001253256471332629
Iteration: 1297, Checking gradient for hidden b[0][6]:	mock grad = -0.043571731037805872, computed grad = -0.043571744129643861
Iteration: 1297, Checking gradient for output layer W[85][1]:	mock grad = -0.225186811707078771, computed grad = -0.225186809285111839
Iteration: 1298, Checking gradient for words E[773][11]:	mock grad = 0.014271326287951247, computed grad = 0.013753749903312311
Iteration: 1298, Checking gradient for hidden W[159][41]:	mock grad = -0.007511937681364822, computed grad = -0.007511937783143142
Iteration: 1298, Checking gradient for hidden b[0][30]:	mock grad = 0.000879574792334026, computed grad = 0.000879573358394602
Iteration: 1298, Checking gradient for output layer W[76][1]:	mock grad = -0.006120001238341111, computed grad = -0.006120001238119598
current: 100, Cost = 0.332267, Correct(%) = 1, time = 6.97308
Iteration: 1299, Checking gradient for words E[756][42]:	mock grad = 0.000105017464779866, computed grad = 0.000105017460029918
Iteration: 1299, Checking gradient for hidden W[208][42]:	mock grad = -0.012731365135770778, computed grad = -0.012731365264450921
Iteration: 1299, Checking gradient for hidden b[0][48]:	mock grad = 0.068789723309065076, computed grad = 0.068789718707334607
Iteration: 1299, Checking gradient for output layer W[101][0]:	mock grad = 0.120655672141811721, computed grad = 0.120655670999969308
current: 13, Correct(%) = 1, time = 7.02546
Dev start.
Dev finished. Total time taken is: 0.635389
dev:
Accuracy:	P=100/100=1
Test start.
Test finished. Total time taken is: 0.544972
test:
Accuracy:	P=63/100=0.63
##### Iteration 13
random: 0, 99
Iteration: 1300, Checking gradient for words E[49][48]:	mock grad = -0.006599563078010795, computed grad = -0.006599563112876613
Iteration: 1300, Checking gradient for hidden W[92][12]:	mock grad = 0.001529657964183251, computed grad = 0.001529657972075812
Iteration: 1300, Checking gradient for hidden b[0][6]:	mock grad = 0.026536145082703411, computed grad = 0.026536154500860387
Iteration: 1300, Checking gradient for output layer W[33][0]:	mock grad = -0.035223699135583653, computed grad = -0.035223699059199663
Iteration: 1301, Checking gradient for words E[74][26]:	mock grad = 0.007599200419733387, computed grad = 0.007599200433814823
Iteration: 1301, Checking gradient for hidden W[176][3]:	mock grad = 0.005375533788015696, computed grad = 0.005375533803591459
Iteration: 1301, Checking gradient for hidden b[0][46]:	mock grad = 0.107011021080155588, computed grad = 0.107011035245084388
Iteration: 1301, Checking gradient for output layer W[132][0]:	mock grad = 0.035303779573725436, computed grad = 0.035303779535690270
Iteration: 1302, Checking gradient for words E[30][42]:	mock grad = 0.015272548376549233, computed grad = 0.015272548397205647
Iteration: 1302, Checking gradient for hidden W[137][11]:	mock grad = 0.001523029397737519, computed grad = 0.001523029390588541
Iteration: 1302, Checking gradient for hidden b[0][11]:	mock grad = 0.070997135074635231, computed grad = 0.070997152792503848
Iteration: 1302, Checking gradient for output layer W[37][1]:	mock grad = 0.020324040882702121, computed grad = 0.020324040873756565
Iteration: 1303, Checking gradient for words E[60][45]:	mock grad = -0.009617572006404007, computed grad = -0.009617572067433791
Iteration: 1303, Checking gradient for hidden W[177][40]:	mock grad = -0.002767123069097321, computed grad = -0.002767123055977110
Iteration: 1303, Checking gradient for hidden b[0][49]:	mock grad = -0.035331696016938086, computed grad = -0.035331696678503749
Iteration: 1303, Checking gradient for output layer W[141][0]:	mock grad = -0.160380042755536856, computed grad = -0.160380040659666212
Iteration: 1304, Checking gradient for words E[148][27]:	mock grad = 0.014488082305419425, computed grad = 0.014488082348822647
Iteration: 1304, Checking gradient for hidden W[49][26]:	mock grad = 0.008260314377228628, computed grad = 0.008260314476274452
Iteration: 1304, Checking gradient for hidden b[0][36]:	mock grad = 0.005701830651738682, computed grad = 0.005701825368480655
Iteration: 1304, Checking gradient for output layer W[101][1]:	mock grad = 0.102414062904343517, computed grad = 0.102414062180900226
Iteration: 1305, Checking gradient for words E[34][7]:	mock grad = -0.000301151648091080, computed grad = -0.000301151629691907
Iteration: 1305, Checking gradient for hidden W[123][38]:	mock grad = 0.008310981121695260, computed grad = 0.008310981149489511
Iteration: 1305, Checking gradient for hidden b[0][12]:	mock grad = -0.063247102039282188, computed grad = -0.063247122977624207
Iteration: 1305, Checking gradient for output layer W[104][0]:	mock grad = -0.106719908044200507, computed grad = -0.106719907160134006
Iteration: 1306, Checking gradient for words E[267][42]:	mock grad = 0.018030007086333733, computed grad = 0.018030007132575739
Iteration: 1306, Checking gradient for hidden W[153][43]:	mock grad = 0.000827036698278216, computed grad = 0.000827036699859310
Iteration: 1306, Checking gradient for hidden b[0][44]:	mock grad = 0.013643378416972052, computed grad = 0.013643390378891729
Iteration: 1306, Checking gradient for output layer W[81][1]:	mock grad = -0.040627418634128776, computed grad = -0.040627418529630831
Iteration: 1307, Checking gradient for words E[278][42]:	mock grad = 0.031725012430428023, computed grad = 0.029823363873059835
Iteration: 1307, Checking gradient for hidden W[184][34]:	mock grad = -0.002332409511696110, computed grad = -0.002332409509484550
Iteration: 1307, Checking gradient for hidden b[0][45]:	mock grad = -0.012680474759385607, computed grad = -0.012680478787428874
Iteration: 1307, Checking gradient for output layer W[109][0]:	mock grad = 0.028496471159461523, computed grad = 0.028496471155118688
Iteration: 1308, Checking gradient for words E[287][9]:	mock grad = -0.004437769801524105, computed grad = -0.004437769785966653
Iteration: 1308, Checking gradient for hidden W[211][4]:	mock grad = -0.003844883886633399, computed grad = -0.003844883905027502
Iteration: 1308, Checking gradient for hidden b[0][41]:	mock grad = 0.028308301324719842, computed grad = 0.028308307231094954
Iteration: 1308, Checking gradient for output layer W[67][1]:	mock grad = -0.037019788162617751, computed grad = -0.037019788115680567
current: 10, Cost = 0.334151, Correct(%) = 1, time = 0.76977
Iteration: 1309, Checking gradient for words E[306][25]:	mock grad = -0.012504440347643753, computed grad = -0.012641549654420331
Iteration: 1309, Checking gradient for hidden W[228][49]:	mock grad = 0.005800592060511978, computed grad = 0.005800592134158208
Iteration: 1309, Checking gradient for hidden b[0][9]:	mock grad = -0.045761356088386274, computed grad = -0.045761355040268722
Iteration: 1309, Checking gradient for output layer W[53][0]:	mock grad = -0.065163483830926738, computed grad = -0.065163483654168436
Iteration: 1310, Checking gradient for words E[54][14]:	mock grad = -0.008515782015022788, computed grad = -0.008515782049362042
Iteration: 1310, Checking gradient for hidden W[54][49]:	mock grad = -0.004412379762924745, computed grad = -0.004412379758965028
Iteration: 1310, Checking gradient for hidden b[0][7]:	mock grad = -0.052234730869404888, computed grad = -0.052234734564418038
Iteration: 1310, Checking gradient for output layer W[53][1]:	mock grad = -0.102750087141834090, computed grad = -0.102750085719237078
Iteration: 1311, Checking gradient for words E[216][0]:	mock grad = 0.037606223791369908, computed grad = 0.037606223828257276
Iteration: 1311, Checking gradient for hidden W[240][28]:	mock grad = -0.016700597102681236, computed grad = -0.016700597084713633
Iteration: 1311, Checking gradient for hidden b[0][22]:	mock grad = 0.049920438735667760, computed grad = 0.049920452293476476
Iteration: 1311, Checking gradient for output layer W[23][0]:	mock grad = 0.095390167529685321, computed grad = 0.095390167222338076
Iteration: 1312, Checking gradient for words E[350][6]:	mock grad = -0.008336225841343303, computed grad = -0.008336225859004589
Iteration: 1312, Checking gradient for hidden W[97][43]:	mock grad = 0.002077173849285296, computed grad = 0.002077173880425778
Iteration: 1312, Checking gradient for hidden b[0][20]:	mock grad = -0.081775392719807272, computed grad = -0.081775415915003688
Iteration: 1312, Checking gradient for output layer W[1][0]:	mock grad = -0.016821050447424213, computed grad = -0.016821050442216833
Iteration: 1313, Checking gradient for words E[364][20]:	mock grad = -0.002233636933662853, computed grad = -0.002233636907146916
Iteration: 1313, Checking gradient for hidden W[121][48]:	mock grad = 0.002892105959412250, computed grad = 0.002892105985131375
Iteration: 1313, Checking gradient for hidden b[0][49]:	mock grad = 0.035418770470696526, computed grad = 0.035418782471408894
Iteration: 1313, Checking gradient for output layer W[135][1]:	mock grad = -0.000962104237695049, computed grad = -0.000962104237672209
Iteration: 1314, Checking gradient for words E[103][35]:	mock grad = -0.042955925776910764, computed grad = -0.042955925837358050
Iteration: 1314, Checking gradient for hidden W[23][17]:	mock grad = 0.003113352068651043, computed grad = 0.003454173726122055
Iteration: 1314, Checking gradient for hidden b[0][43]:	mock grad = 0.014684673870224429, computed grad = 0.014684677989675599
Iteration: 1314, Checking gradient for output layer W[46][0]:	mock grad = 0.105409576003023808, computed grad = 0.105409575858562463
Iteration: 1315, Checking gradient for words E[383][22]:	mock grad = -0.000354200942465077, computed grad = -0.000354200898940394
Iteration: 1315, Checking gradient for hidden W[75][19]:	mock grad = 0.001370049364984860, computed grad = 0.001370049365095739
Iteration: 1315, Checking gradient for hidden b[0][33]:	mock grad = -0.095025658249259237, computed grad = -0.095025684627457876
Iteration: 1315, Checking gradient for output layer W[53][1]:	mock grad = 0.081290240213555132, computed grad = 0.081290239842900403
Iteration: 1316, Checking gradient for words E[776][16]:	mock grad = -0.034660795814989154, computed grad = -0.034088941075952790
Iteration: 1316, Checking gradient for hidden W[205][26]:	mock grad = -0.010946415882923111, computed grad = -0.010946416006368823
Iteration: 1316, Checking gradient for hidden b[0][20]:	mock grad = -0.095751429264545607, computed grad = -0.095751455335463531
Iteration: 1316, Checking gradient for output layer W[111][1]:	mock grad = 0.070804142636576861, computed grad = 0.070804142440297896
Iteration: 1317, Checking gradient for words E[406][21]:	mock grad = -0.003088247602520133, computed grad = -0.003088247609262634
Iteration: 1317, Checking gradient for hidden W[157][34]:	mock grad = -0.008598538726992122, computed grad = -0.008598538821659762
Iteration: 1317, Checking gradient for hidden b[0][14]:	mock grad = 0.045304592512465658, computed grad = 0.045304607473034608
Iteration: 1317, Checking gradient for output layer W[99][1]:	mock grad = -0.109561243478467629, computed grad = -0.109561241215245353
Iteration: 1318, Checking gradient for words E[428][7]:	mock grad = -0.012806983573060426, computed grad = -0.012806983575771423
Iteration: 1318, Checking gradient for hidden W[79][2]:	mock grad = 0.022058449841377836, computed grad = 0.022058449780481066
Iteration: 1318, Checking gradient for hidden b[0][6]:	mock grad = -0.028390133682804075, computed grad = -0.028390142681026603
Iteration: 1318, Checking gradient for output layer W[110][1]:	mock grad = 0.094612827018725199, computed grad = 0.094612825973632506
current: 20, Cost = 0.434345, Correct(%) = 1, time = 1.4281
Iteration: 1319, Checking gradient for words E[119][19]:	mock grad = 0.020392657722356766, computed grad = 0.020392657715482557
Iteration: 1319, Checking gradient for hidden W[173][21]:	mock grad = 0.001167589621986487, computed grad = 0.001167589623380578
Iteration: 1319, Checking gradient for hidden b[0][29]:	mock grad = -0.090754280248156460, computed grad = -0.090754303735951808
Iteration: 1319, Checking gradient for output layer W[40][1]:	mock grad = -0.045010327285904816, computed grad = -0.045010327262548583
Iteration: 1320, Checking gradient for words E[443][8]:	mock grad = 0.005482995069777674, computed grad = 0.004732403485582629
Iteration: 1320, Checking gradient for hidden W[3][18]:	mock grad = -0.012415371901330907, computed grad = -0.012415372116085554
Iteration: 1320, Checking gradient for hidden b[0][29]:	mock grad = 0.072015501369843138, computed grad = 0.072015526403340399
Iteration: 1320, Checking gradient for output layer W[132][0]:	mock grad = 0.015913913769510790, computed grad = 0.015913913766227368
Iteration: 1321, Checking gradient for words E[102][30]:	mock grad = 0.028309194960357553, computed grad = 0.028309195051898408
Iteration: 1321, Checking gradient for hidden W[115][15]:	mock grad = -0.000986685586096270, computed grad = -0.000986685587099648
Iteration: 1321, Checking gradient for hidden b[0][24]:	mock grad = 0.117065047509135045, computed grad = 0.117065081307749952
Iteration: 1321, Checking gradient for output layer W[81][1]:	mock grad = -0.015168329451309415, computed grad = -0.015168329450675771
Iteration: 1322, Checking gradient for words E[377][29]:	mock grad = -0.010151602405528148, computed grad = -0.010151602451207421
Iteration: 1322, Checking gradient for hidden W[62][49]:	mock grad = 0.011404148929666791, computed grad = 0.011404148966587421
Iteration: 1322, Checking gradient for hidden b[0][8]:	mock grad = -0.025016534764277232, computed grad = -0.025016545489507328
Iteration: 1322, Checking gradient for output layer W[79][1]:	mock grad = -0.012912024517613974, computed grad = -0.012912024515196581
Iteration: 1323, Checking gradient for words E[421][47]:	mock grad = 0.018099344193001077, computed grad = 0.018099344240575167
Iteration: 1323, Checking gradient for hidden W[40][40]:	mock grad = -0.005829561344150580, computed grad = -0.005829561376513877
Iteration: 1323, Checking gradient for hidden b[0][49]:	mock grad = -0.030167736108560606, computed grad = -0.030167735209798902
Iteration: 1323, Checking gradient for output layer W[119][0]:	mock grad = -0.150943063021352630, computed grad = -0.150943060336902884
Iteration: 1324, Checking gradient for words E[42][14]:	mock grad = 0.010170284794885909, computed grad = 0.010170284773064236
Iteration: 1324, Checking gradient for hidden W[3][42]:	mock grad = -0.016123276104507145, computed grad = -0.016123276324700012
Iteration: 1324, Checking gradient for hidden b[0][10]:	mock grad = 0.086970282212273098, computed grad = 0.086970304944893759
Iteration: 1324, Checking gradient for output layer W[118][0]:	mock grad = -0.000627847325046504, computed grad = -0.000627847325052863
Iteration: 1325, Checking gradient for words E[460][17]:	mock grad = -0.028365269137464733, computed grad = -0.028365269168262416
Iteration: 1325, Checking gradient for hidden W[111][5]:	mock grad = 0.009591602552955214, computed grad = 0.009591602834276305
Iteration: 1325, Checking gradient for hidden b[0][4]:	mock grad = 0.020394350287250562, computed grad = 0.020394358891944912
Iteration: 1325, Checking gradient for output layer W[58][0]:	mock grad = 0.003911939890716143, computed grad = 0.003911939890602997
Iteration: 1326, Checking gradient for words E[472][22]:	mock grad = 0.001293474679364692, computed grad = 0.001293474684582856
Iteration: 1326, Checking gradient for hidden W[217][33]:	mock grad = 0.006248404488734538, computed grad = 0.006248404509460547
Iteration: 1326, Checking gradient for hidden b[0][5]:	mock grad = 0.001307308854781031, computed grad = 0.001307298418019665
Iteration: 1326, Checking gradient for output layer W[128][0]:	mock grad = -0.042739119610876730, computed grad = -0.042739119485493623
Iteration: 1327, Checking gradient for words E[69][45]:	mock grad = -0.026557248277936374, computed grad = -0.026557248232039619
Iteration: 1327, Checking gradient for hidden W[73][46]:	mock grad = 0.030963860712468483, computed grad = 0.030963861071947537
Iteration: 1327, Checking gradient for hidden b[0][22]:	mock grad = 0.054416724126260529, computed grad = 0.054416738809660040
Iteration: 1327, Checking gradient for output layer W[97][0]:	mock grad = -0.123935790706852789, computed grad = -0.123935790256047562
Iteration: 1328, Checking gradient for words E[354][34]:	mock grad = 0.002143875363291858, computed grad = 0.002143875362870502
Iteration: 1328, Checking gradient for hidden W[224][16]:	mock grad = -0.000640060810380705, computed grad = -0.000640060814037425
Iteration: 1328, Checking gradient for hidden b[0][22]:	mock grad = 0.027190277239880212, computed grad = 0.027190279491835570
Iteration: 1328, Checking gradient for output layer W[27][1]:	mock grad = -0.033984163553962832, computed grad = -0.033984163501199628
current: 30, Cost = 0.308329, Correct(%) = 1, time = 2.11916
Iteration: 1329, Checking gradient for words E[49][1]:	mock grad = 0.017598616981001625, computed grad = 0.017598616962771894
Iteration: 1329, Checking gradient for hidden W[115][7]:	mock grad = -0.001222596082844918, computed grad = -0.001222596082728750
Iteration: 1329, Checking gradient for hidden b[0][26]:	mock grad = 0.025731013133550862, computed grad = 0.025731018184605031
Iteration: 1329, Checking gradient for output layer W[38][1]:	mock grad = -0.003499124412748023, computed grad = -0.003499124412792886
Iteration: 1330, Checking gradient for words E[511][17]:	mock grad = 0.042130662706252320, computed grad = 0.042130662619459594
Iteration: 1330, Checking gradient for hidden W[176][17]:	mock grad = 0.004117388081814610, computed grad = 0.004117388097351403
Iteration: 1330, Checking gradient for hidden b[0][37]:	mock grad = 0.040009891454279023, computed grad = 0.040009896000691408
Iteration: 1330, Checking gradient for output layer W[110][1]:	mock grad = -0.130443877010166798, computed grad = -0.130443874941269938
Iteration: 1331, Checking gradient for words E[514][25]:	mock grad = 0.002682921691199436, computed grad = 0.001194108676707896
Iteration: 1331, Checking gradient for hidden W[136][20]:	mock grad = -0.010337778006297427, computed grad = -0.010337778116018881
Iteration: 1331, Checking gradient for hidden b[0][31]:	mock grad = -0.044713735619816841, computed grad = -0.044713748504330297
Iteration: 1331, Checking gradient for output layer W[75][0]:	mock grad = -0.138033441323293093, computed grad = -0.138033440556810133
Iteration: 1332, Checking gradient for words E[194][3]:	mock grad = -0.008756644373136835, computed grad = -0.008756644363746336
Iteration: 1332, Checking gradient for hidden W[43][19]:	mock grad = 0.007571574273640413, computed grad = 0.007571574305768884
Iteration: 1332, Checking gradient for hidden b[0][44]:	mock grad = -0.004649100508402615, computed grad = -0.004649112597426203
Iteration: 1332, Checking gradient for output layer W[130][1]:	mock grad = -0.071915170104436266, computed grad = -0.071915169775047971
Iteration: 1333, Checking gradient for words E[143][31]:	mock grad = 0.006756687194492406, computed grad = 0.006756687190236535
Iteration: 1333, Checking gradient for hidden W[248][5]:	mock grad = 0.001539577547948756, computed grad = 0.001539577563234857
Iteration: 1333, Checking gradient for hidden b[0][26]:	mock grad = 0.022876569526014690, computed grad = 0.022876574088065635
Iteration: 1333, Checking gradient for output layer W[135][0]:	mock grad = 0.002960138736129991, computed grad = 0.002960138736082240
Iteration: 1334, Checking gradient for words E[282][23]:	mock grad = 0.016340343235138333, computed grad = 0.016340343222871839
Iteration: 1334, Checking gradient for hidden W[84][37]:	mock grad = 0.003170471379460027, computed grad = 0.003170471426504081
Iteration: 1334, Checking gradient for hidden b[0][25]:	mock grad = 0.001829725722629627, computed grad = 0.001829719172413728
Iteration: 1334, Checking gradient for output layer W[15][1]:	mock grad = 0.120229621134754217, computed grad = 0.120229620470040183
Iteration: 1335, Checking gradient for words E[383][13]:	mock grad = -0.014806159962382770, computed grad = -0.014806160005074984
Iteration: 1335, Checking gradient for hidden W[101][43]:	mock grad = 0.000114564047054566, computed grad = 0.000114564047259874
Iteration: 1335, Checking gradient for hidden b[0][47]:	mock grad = -0.084326007350976617, computed grad = -0.084326029181320789
Iteration: 1335, Checking gradient for output layer W[141][1]:	mock grad = -0.101478776054542141, computed grad = -0.101478775609475019
Iteration: 1336, Checking gradient for words E[531][19]:	mock grad = 0.058021636607286098, computed grad = 0.058021636605739405
Iteration: 1336, Checking gradient for hidden W[243][46]:	mock grad = -0.003690013507695600, computed grad = -0.003690013527399052
Iteration: 1336, Checking gradient for hidden b[0][49]:	mock grad = 0.052647865229293878, computed grad = 0.052647881664237381
Iteration: 1336, Checking gradient for output layer W[94][0]:	mock grad = -0.144107576959595640, computed grad = -0.144107576220283445
Iteration: 1337, Checking gradient for words E[48][21]:	mock grad = -0.035281009087534398, computed grad = -0.035281009055554466
Iteration: 1337, Checking gradient for hidden W[101][29]:	mock grad = 0.007777442178280580, computed grad = 0.007777442200945903
Iteration: 1337, Checking gradient for hidden b[0][28]:	mock grad = -0.006802853703635758, computed grad = -0.006802864044825311
Iteration: 1337, Checking gradient for output layer W[130][0]:	mock grad = -0.118838166288853797, computed grad = -0.118838165094005369
Iteration: 1338, Checking gradient for words E[10][41]:	mock grad = -0.008180569904175927, computed grad = -0.008158281837144744
Iteration: 1338, Checking gradient for hidden W[206][29]:	mock grad = 0.011960340677935610, computed grad = 0.011960340737863843
Iteration: 1338, Checking gradient for hidden b[0][39]:	mock grad = 0.086143440369390589, computed grad = 0.086143468723511488
Iteration: 1338, Checking gradient for output layer W[84][1]:	mock grad = 0.120036993517813606, computed grad = 0.120036992829444633
current: 40, Cost = 0.250093, Correct(%) = 1, time = 2.55636
Iteration: 1339, Checking gradient for words E[25][2]:	mock grad = 0.001969529047685104, computed grad = 0.001969529029364517
Iteration: 1339, Checking gradient for hidden W[85][6]:	mock grad = -0.001754419586885714, computed grad = -0.001754419602007382
Iteration: 1339, Checking gradient for hidden b[0][29]:	mock grad = 0.060584371948890769, computed grad = 0.060584393610101511
Iteration: 1339, Checking gradient for output layer W[28][1]:	mock grad = -0.029169535412792991, computed grad = -0.029169535376189642
Iteration: 1340, Checking gradient for words E[172][19]:	mock grad = -0.011037369399502506, computed grad = -0.011037369376329683
Iteration: 1340, Checking gradient for hidden W[246][8]:	mock grad = 0.003484724165864206, computed grad = 0.003484724277240133
Iteration: 1340, Checking gradient for hidden b[0][41]:	mock grad = 0.025304716420038176, computed grad = 0.025304721757626353
Iteration: 1340, Checking gradient for output layer W[118][0]:	mock grad = -0.063640158864897156, computed grad = -0.063640158509132350
Iteration: 1341, Checking gradient for words E[24][47]:	mock grad = -0.035463261449292238, computed grad = -0.035463261471751772
Iteration: 1341, Checking gradient for hidden W[46][32]:	mock grad = 0.002178423237159111, computed grad = 0.002178423240025128
Iteration: 1341, Checking gradient for hidden b[0][39]:	mock grad = 0.094862136017004328, computed grad = 0.094862164528279169
Iteration: 1341, Checking gradient for output layer W[26][1]:	mock grad = -0.059090789634036467, computed grad = -0.059090789585916799
Iteration: 1342, Checking gradient for words E[6][44]:	mock grad = 0.016288796613556089, computed grad = 0.019152167086694340
Iteration: 1342, Checking gradient for hidden W[248][46]:	mock grad = -0.000658725140156857, computed grad = -0.000658725144215703
Iteration: 1342, Checking gradient for hidden b[0][10]:	mock grad = 0.090961220654134367, computed grad = 0.090961238594181537
Iteration: 1342, Checking gradient for output layer W[47][1]:	mock grad = 0.049621451958553608, computed grad = 0.049621451917274441
Iteration: 1343, Checking gradient for words E[48][38]:	mock grad = -0.066479224756416100, computed grad = -0.066479225371585515
Iteration: 1343, Checking gradient for hidden W[111][19]:	mock grad = -0.021338621810335079, computed grad = -0.021338622206862792
Iteration: 1343, Checking gradient for hidden b[0][6]:	mock grad = 0.043910711334582686, computed grad = 0.043910726001459399
Iteration: 1343, Checking gradient for output layer W[118][0]:	mock grad = -0.006022534640659494, computed grad = -0.006022534640608534
Iteration: 1344, Checking gradient for words E[10][40]:	mock grad = 0.001286704434902419, computed grad = 0.001286704444928609
Iteration: 1344, Checking gradient for hidden W[167][24]:	mock grad = -0.003058896898805452, computed grad = -0.003058896906308349
Iteration: 1344, Checking gradient for hidden b[0][48]:	mock grad = -0.051082856115436392, computed grad = -0.051082865951636955
Iteration: 1344, Checking gradient for output layer W[101][1]:	mock grad = 0.072633980420386268, computed grad = 0.072633979624955325
Iteration: 1345, Checking gradient for words E[194][20]:	mock grad = -0.007330663709476104, computed grad = -0.007330663757561177
Iteration: 1345, Checking gradient for hidden W[57][5]:	mock grad = 0.011069661071394510, computed grad = 0.011069661060932594
Iteration: 1345, Checking gradient for hidden b[0][39]:	mock grad = 0.079901364526735463, computed grad = 0.079901387776936131
Iteration: 1345, Checking gradient for output layer W[119][0]:	mock grad = 0.091538225890364089, computed grad = 0.091538225531774264
Iteration: 1346, Checking gradient for words E[593][25]:	mock grad = 0.000584176423096272, computed grad = 0.000584176460372255
Iteration: 1346, Checking gradient for hidden W[198][24]:	mock grad = 0.004096196780212225, computed grad = 0.004096196776470410
Iteration: 1346, Checking gradient for hidden b[0][37]:	mock grad = 0.040319386524489964, computed grad = 0.040319395871057408
Iteration: 1346, Checking gradient for output layer W[144][0]:	mock grad = 0.053852836266632398, computed grad = 0.053852836035985577
Iteration: 1347, Checking gradient for words E[88][7]:	mock grad = 0.002258058152820874, computed grad = 0.002258058173553771
Iteration: 1347, Checking gradient for hidden W[152][6]:	mock grad = -0.001244357599572421, computed grad = -0.001244357594679920
Iteration: 1347, Checking gradient for hidden b[0][48]:	mock grad = -0.071136905220481328, computed grad = -0.071136922691780027
Iteration: 1347, Checking gradient for output layer W[82][0]:	mock grad = 0.153008733942611164, computed grad = 0.153008730303617363
Iteration: 1348, Checking gradient for words E[80][48]:	mock grad = 0.009638694837721395, computed grad = 0.009638694891054050
Iteration: 1348, Checking gradient for hidden W[35][40]:	mock grad = -0.006665700762520377, computed grad = -0.006665700817693754
Iteration: 1348, Checking gradient for hidden b[0][20]:	mock grad = 0.072529309263630326, computed grad = 0.072529322554958642
Iteration: 1348, Checking gradient for output layer W[58][0]:	mock grad = -0.037427065013528882, computed grad = -0.037427064950370036
current: 50, Cost = 0.250437, Correct(%) = 1, time = 3.4437
Iteration: 1349, Checking gradient for words E[585][20]:	mock grad = 0.011951430677392683, computed grad = 0.011951430694541989
Iteration: 1349, Checking gradient for hidden W[159][2]:	mock grad = -0.014594795919781189, computed grad = -0.014594796024296103
Iteration: 1349, Checking gradient for hidden b[0][3]:	mock grad = -0.029548644240678934, computed grad = -0.029548653951537850
Iteration: 1349, Checking gradient for output layer W[49][0]:	mock grad = -0.033843324160204302, computed grad = -0.033843324103137791
Iteration: 1350, Checking gradient for words E[617][10]:	mock grad = 0.005150155414052060, computed grad = 0.005150155436569135
Iteration: 1350, Checking gradient for hidden W[244][24]:	mock grad = 0.003506088576604727, computed grad = 0.003506088564394083
Iteration: 1350, Checking gradient for hidden b[0][49]:	mock grad = 0.038520530450586454, computed grad = 0.038520533247587141
Iteration: 1350, Checking gradient for output layer W[65][1]:	mock grad = 0.232180079659916050, computed grad = 0.232180076210359743
Iteration: 1351, Checking gradient for words E[620][30]:	mock grad = 0.002002052971933344, computed grad = 0.002002052993564298
Iteration: 1351, Checking gradient for hidden W[103][31]:	mock grad = 0.001957419234838165, computed grad = 0.001957419248532236
Iteration: 1351, Checking gradient for hidden b[0][24]:	mock grad = 0.082417146467422819, computed grad = 0.082417167022462406
Iteration: 1351, Checking gradient for output layer W[55][0]:	mock grad = 0.109903573933062271, computed grad = 0.109903573085572040
Iteration: 1352, Checking gradient for words E[119][30]:	mock grad = -0.020023173410294026, computed grad = -0.020023173479018889
Iteration: 1352, Checking gradient for hidden W[58][12]:	mock grad = -0.004160084614401871, computed grad = -0.004160084655783448
Iteration: 1352, Checking gradient for hidden b[0][38]:	mock grad = -0.059503812397831979, computed grad = -0.059503822230245293
Iteration: 1352, Checking gradient for output layer W[106][1]:	mock grad = 0.004653679210236961, computed grad = 0.004653679210190991
Iteration: 1353, Checking gradient for words E[242][23]:	mock grad = 0.011456059420072995, computed grad = 0.011456059437100517
Iteration: 1353, Checking gradient for hidden W[104][21]:	mock grad = 0.000226902665489970, computed grad = 0.000226902667284242
Iteration: 1353, Checking gradient for hidden b[0][5]:	mock grad = -0.006971722798232438, computed grad = -0.006971716614201068
Iteration: 1353, Checking gradient for output layer W[98][1]:	mock grad = -0.005852023951979923, computed grad = -0.005852023951730297
Iteration: 1354, Checking gradient for words E[632][45]:	mock grad = 0.005591728787002204, computed grad = 0.004350132277337077
Iteration: 1354, Checking gradient for hidden W[70][30]:	mock grad = -0.005087124875624571, computed grad = -0.005087124906075311
Iteration: 1354, Checking gradient for hidden b[0][24]:	mock grad = 0.067312988974610466, computed grad = 0.067313008893929355
Iteration: 1354, Checking gradient for output layer W[140][1]:	mock grad = 0.097247064600278210, computed grad = 0.097247063170793138
Iteration: 1355, Checking gradient for words E[635][5]:	mock grad = -0.046137377402805102, computed grad = -0.046137376945592848
Iteration: 1355, Checking gradient for hidden W[81][42]:	mock grad = -0.007853869859075768, computed grad = -0.007853869928013823
Iteration: 1355, Checking gradient for hidden b[0][38]:	mock grad = -0.046261127549934722, computed grad = -0.046261135151744529
Iteration: 1355, Checking gradient for output layer W[103][0]:	mock grad = -0.095097030867630217, computed grad = -0.095097029832597421
Iteration: 1356, Checking gradient for words E[435][6]:	mock grad = 0.088826415213710819, computed grad = 0.088826415300331946
Iteration: 1356, Checking gradient for hidden W[126][5]:	mock grad = -0.003587588550041376, computed grad = -0.003587588615805847
Iteration: 1356, Checking gradient for hidden b[0][27]:	mock grad = -0.033905376984250113, computed grad = -0.033905384259821550
Iteration: 1356, Checking gradient for output layer W[24][0]:	mock grad = 0.064746862042469600, computed grad = 0.064746861990097354
Iteration: 1357, Checking gradient for words E[356][17]:	mock grad = -0.008236529361188571, computed grad = -0.008236529375177713
Iteration: 1357, Checking gradient for hidden W[162][35]:	mock grad = -0.000555860444673151, computed grad = -0.000555860481587572
Iteration: 1357, Checking gradient for hidden b[0][4]:	mock grad = 0.015851555976997833, computed grad = 0.015851559850819423
Iteration: 1357, Checking gradient for output layer W[52][1]:	mock grad = 0.017264303905639711, computed grad = 0.017264303901270289
Iteration: 1358, Checking gradient for words E[77][38]:	mock grad = -0.000407594522189303, computed grad = -0.000407594503449809
Iteration: 1358, Checking gradient for hidden W[87][46]:	mock grad = -0.025512944544309191, computed grad = -0.025512944747728403
Iteration: 1358, Checking gradient for hidden b[0][34]:	mock grad = 0.030647846090608999, computed grad = 0.030647856931922068
Iteration: 1358, Checking gradient for output layer W[3][0]:	mock grad = -0.021083098363544606, computed grad = -0.021083098358355555
current: 60, Cost = 0.3254, Correct(%) = 1, time = 4.13761
Iteration: 1359, Checking gradient for words E[203][0]:	mock grad = -0.001679218036232388, computed grad = -0.001679218010853692
Iteration: 1359, Checking gradient for hidden W[221][3]:	mock grad = -0.003272636398127249, computed grad = -0.003272636425750045
Iteration: 1359, Checking gradient for hidden b[0][43]:	mock grad = -0.009921546895746980, computed grad = -0.009921549429348784
Iteration: 1359, Checking gradient for output layer W[18][1]:	mock grad = 0.077700756510035163, computed grad = 0.077700756184713449
Iteration: 1360, Checking gradient for words E[124][37]:	mock grad = -0.003016763952867763, computed grad = -0.003016763991299990
Iteration: 1360, Checking gradient for hidden W[75][46]:	mock grad = 0.032243791646374698, computed grad = 0.032243792055799754
Iteration: 1360, Checking gradient for hidden b[0][47]:	mock grad = -0.071066435189393040, computed grad = -0.071066443198605597
Iteration: 1360, Checking gradient for output layer W[31][1]:	mock grad = -0.060262128703914009, computed grad = -0.060262128615955854
Iteration: 1361, Checking gradient for words E[99][45]:	mock grad = -0.010508515319584966, computed grad = -0.010508515335584088
Iteration: 1361, Checking gradient for hidden W[27][23]:	mock grad = -0.005671425116926532, computed grad = -0.005671425153382365
Iteration: 1361, Checking gradient for hidden b[0][32]:	mock grad = -0.056314830650078251, computed grad = -0.056314849687774379
Iteration: 1361, Checking gradient for output layer W[117][0]:	mock grad = -0.097645111459043754, computed grad = -0.097645111043014599
Iteration: 1362, Checking gradient for words E[326][36]:	mock grad = 0.018486297783920635, computed grad = 0.018486297862311995
Iteration: 1362, Checking gradient for hidden W[184][18]:	mock grad = 0.005330378037959882, computed grad = 0.005330378065682503
Iteration: 1362, Checking gradient for hidden b[0][32]:	mock grad = 0.050555916115108124, computed grad = 0.050555933028724007
Iteration: 1362, Checking gradient for output layer W[24][1]:	mock grad = 0.055761920162655798, computed grad = 0.055761920048159185
Iteration: 1363, Checking gradient for words E[208][28]:	mock grad = -0.012228518857182458, computed grad = -0.012228518877602377
Iteration: 1363, Checking gradient for hidden W[140][3]:	mock grad = -0.002261932526753663, computed grad = -0.002261932517577182
Iteration: 1363, Checking gradient for hidden b[0][44]:	mock grad = 0.003813393734369841, computed grad = 0.003813402468220799
Iteration: 1363, Checking gradient for output layer W[2][0]:	mock grad = -0.051703842046918469, computed grad = -0.051703841960942507
Iteration: 1364, Checking gradient for words E[626][19]:	mock grad = -0.013170872480899876, computed grad = -0.012455481070273422
Iteration: 1364, Checking gradient for hidden W[130][4]:	mock grad = 0.001105437772264128, computed grad = 0.001105437777687631
Iteration: 1364, Checking gradient for hidden b[0][24]:	mock grad = 0.093305435175661255, computed grad = 0.093305460164334719
Iteration: 1364, Checking gradient for output layer W[142][0]:	mock grad = -0.149585215627939805, computed grad = -0.149585214172186648
Iteration: 1365, Checking gradient for words E[272][42]:	mock grad = 0.006515690337813296, computed grad = 0.006515690354070262
Iteration: 1365, Checking gradient for hidden W[106][46]:	mock grad = 0.012849160035938167, computed grad = 0.012849160149719875
Iteration: 1365, Checking gradient for hidden b[0][20]:	mock grad = 0.061752204931445065, computed grad = 0.061752214646901761
Iteration: 1365, Checking gradient for output layer W[121][1]:	mock grad = -0.039380698712510043, computed grad = -0.039380698602376342
Iteration: 1366, Checking gradient for words E[69][29]:	mock grad = -0.014051768238515505, computed grad = -0.014051768306676636
Iteration: 1366, Checking gradient for hidden W[89][35]:	mock grad = 0.006632887222607353, computed grad = 0.006632887284898857
Iteration: 1366, Checking gradient for hidden b[0][27]:	mock grad = 0.038615498059835218, computed grad = 0.038615523150223194
Iteration: 1366, Checking gradient for output layer W[134][0]:	mock grad = 0.127685240708941272, computed grad = 0.127685238817377172
Iteration: 1367, Checking gradient for words E[589][38]:	mock grad = 0.028803785618952693, computed grad = 0.028601437945337517
Iteration: 1367, Checking gradient for hidden W[37][36]:	mock grad = -0.006076311786090693, computed grad = -0.006076311752647750
Iteration: 1367, Checking gradient for hidden b[0][27]:	mock grad = -0.037918333765596390, computed grad = -0.037918351366172658
Iteration: 1367, Checking gradient for output layer W[133][0]:	mock grad = -0.075361265560125457, computed grad = -0.075361265284419263
Iteration: 1368, Checking gradient for words E[335][38]:	mock grad = -0.007408866172120554, computed grad = -0.007408866189033495
Iteration: 1368, Checking gradient for hidden W[44][35]:	mock grad = -0.000454186646042798, computed grad = -0.000454186644182940
Iteration: 1368, Checking gradient for hidden b[0][39]:	mock grad = -0.061217168924737608, computed grad = -0.061217191632980332
Iteration: 1368, Checking gradient for output layer W[38][1]:	mock grad = -0.025255782903632618, computed grad = -0.025255782885126438
current: 70, Cost = 0.252221, Correct(%) = 1, time = 4.74906
Iteration: 1369, Checking gradient for words E[320][49]:	mock grad = 0.004023713918555050, computed grad = 0.004023713924451371
Iteration: 1369, Checking gradient for hidden W[19][6]:	mock grad = -0.000004633006395194, computed grad = -0.000004633022777397
Iteration: 1369, Checking gradient for hidden b[0][6]:	mock grad = -0.027971852367802974, computed grad = -0.027971862168536910
Iteration: 1369, Checking gradient for output layer W[100][0]:	mock grad = -0.091732038188740983, computed grad = -0.091732037073937067
Iteration: 1370, Checking gradient for words E[682][19]:	mock grad = 0.001229551452930355, computed grad = 0.001229551463673923
Iteration: 1370, Checking gradient for hidden W[39][7]:	mock grad = -0.012819254710111361, computed grad = -0.012819254815353689
Iteration: 1370, Checking gradient for hidden b[0][13]:	mock grad = 0.112972002107886160, computed grad = 0.112972024588530104
Iteration: 1370, Checking gradient for output layer W[58][0]:	mock grad = 0.063323394728809212, computed grad = 0.063323394621216722
Iteration: 1371, Checking gradient for words E[69][27]:	mock grad = 0.003266689474729079, computed grad = 0.003266689502682572
Iteration: 1371, Checking gradient for hidden W[195][40]:	mock grad = 0.004732196337953187, computed grad = 0.004732196334057090
Iteration: 1371, Checking gradient for hidden b[0][23]:	mock grad = -0.137084450438695882, computed grad = -0.137084488862931175
Iteration: 1371, Checking gradient for output layer W[138][1]:	mock grad = 0.050277174835466898, computed grad = 0.050277174773524676
Iteration: 1372, Checking gradient for words E[25][3]:	mock grad = 0.007227050494357234, computed grad = 0.007227050531538829
Iteration: 1372, Checking gradient for hidden W[232][20]:	mock grad = -0.003739092647003783, computed grad = -0.003739092635921202
Iteration: 1372, Checking gradient for hidden b[0][29]:	mock grad = 0.083607043482425425, computed grad = 0.083607073055822212
Iteration: 1372, Checking gradient for output layer W[79][0]:	mock grad = 0.038041923872189631, computed grad = 0.038041923847967118
Iteration: 1373, Checking gradient for words E[231][7]:	mock grad = 0.035376837076550594, computed grad = 0.035376836821481884
Iteration: 1373, Checking gradient for hidden W[24][44]:	mock grad = 0.026380898734745806, computed grad = 0.026380898983091379
Iteration: 1373, Checking gradient for hidden b[0][8]:	mock grad = 0.044684461459804092, computed grad = 0.044684493490714391
Iteration: 1373, Checking gradient for output layer W[130][0]:	mock grad = 0.047114050836821741, computed grad = 0.047114050805991341
Iteration: 1374, Checking gradient for words E[304][8]:	mock grad = -0.008066916731996887, computed grad = -0.008066916695029507
Iteration: 1374, Checking gradient for hidden W[189][11]:	mock grad = 0.002722207167155100, computed grad = 0.002722207209507166
Iteration: 1374, Checking gradient for hidden b[0][29]:	mock grad = 0.086955688088058425, computed grad = 0.086955718096015960
Iteration: 1374, Checking gradient for output layer W[19][1]:	mock grad = 0.047894008491339291, computed grad = 0.047894008452828075
Iteration: 1375, Checking gradient for words E[16][20]:	mock grad = -0.015991947523247285, computed grad = -0.015991947544950848
Iteration: 1375, Checking gradient for hidden W[173][10]:	mock grad = 0.004606903417941899, computed grad = 0.004606903403151129
Iteration: 1375, Checking gradient for hidden b[0][13]:	mock grad = 0.091835686930274996, computed grad = 0.091835705821054103
Iteration: 1375, Checking gradient for output layer W[76][0]:	mock grad = 0.029072257865386808, computed grad = 0.029072257841264978
Iteration: 1376, Checking gradient for words E[119][21]:	mock grad = 0.029590105669219380, computed grad = 0.029590105749462067
Iteration: 1376, Checking gradient for hidden W[244][14]:	mock grad = -0.002843231043603289, computed grad = -0.002843231127215471
Iteration: 1376, Checking gradient for hidden b[0][10]:	mock grad = 0.075588522976011152, computed grad = 0.075588537643426310
Iteration: 1376, Checking gradient for output layer W[96][1]:	mock grad = 0.168304273809649629, computed grad = 0.168304270613519075
Iteration: 1377, Checking gradient for words E[94][27]:	mock grad = 0.008377742049953119, computed grad = 0.006412019043021700
Iteration: 1377, Checking gradient for hidden W[53][23]:	mock grad = -0.001903650550860725, computed grad = -0.001903650551023362
Iteration: 1377, Checking gradient for hidden b[0][20]:	mock grad = 0.072891305572225784, computed grad = 0.072891321050711305
Iteration: 1377, Checking gradient for output layer W[4][0]:	mock grad = 0.001175371862865981, computed grad = 0.001175371862849219
Iteration: 1378, Checking gradient for words E[69][43]:	mock grad = -0.016803945819404698, computed grad = -0.016803945823669342
Iteration: 1378, Checking gradient for hidden W[146][9]:	mock grad = 0.001772909630243236, computed grad = 0.001772909630211644
Iteration: 1378, Checking gradient for hidden b[0][47]:	mock grad = 0.069836887181706775, computed grad = 0.069836903750913643
Iteration: 1378, Checking gradient for output layer W[7][0]:	mock grad = 0.064398430874246237, computed grad = 0.064398430692036324
current: 80, Cost = 0.278065, Correct(%) = 1, time = 5.23412
Iteration: 1379, Checking gradient for words E[156][38]:	mock grad = -0.018179471508561473, computed grad = -0.018179471542204059
Iteration: 1379, Checking gradient for hidden W[167][45]:	mock grad = -0.001207183339457796, computed grad = -0.001207183343320495
Iteration: 1379, Checking gradient for hidden b[0][4]:	mock grad = 0.017019426884756772, computed grad = 0.017019433592200036
Iteration: 1379, Checking gradient for output layer W[72][0]:	mock grad = -0.014096005186731153, computed grad = -0.014096005183635052
Iteration: 1380, Checking gradient for words E[107][4]:	mock grad = 0.012813286595442230, computed grad = 0.012813286603622255
Iteration: 1380, Checking gradient for hidden W[60][7]:	mock grad = 0.010073128035359424, computed grad = 0.010073128064067941
Iteration: 1380, Checking gradient for hidden b[0][46]:	mock grad = -0.122815330720810190, computed grad = -0.122815355315397681
Iteration: 1380, Checking gradient for output layer W[79][1]:	mock grad = -0.026492708673941934, computed grad = -0.026492708661163593
Iteration: 1381, Checking gradient for words E[57][10]:	mock grad = 0.003618921461556734, computed grad = 0.003618921464347716
Iteration: 1381, Checking gradient for hidden W[194][21]:	mock grad = -0.003855422072457371, computed grad = -0.003855422083533635
Iteration: 1381, Checking gradient for hidden b[0][4]:	mock grad = 0.014257988107058139, computed grad = 0.014257991063941827
Iteration: 1381, Checking gradient for output layer W[105][1]:	mock grad = -0.060664812905397714, computed grad = -0.060664812696741205
Iteration: 1382, Checking gradient for words E[250][9]:	mock grad = -0.004046585047268181, computed grad = -0.004046585050071548
Iteration: 1382, Checking gradient for hidden W[180][40]:	mock grad = 0.009622002533932195, computed grad = 0.009622002682886320
Iteration: 1382, Checking gradient for hidden b[0][2]:	mock grad = 0.058651158336947562, computed grad = 0.058651193691405701
Iteration: 1382, Checking gradient for output layer W[20][1]:	mock grad = 0.002784728212634757, computed grad = 0.002784728212701906
Iteration: 1383, Checking gradient for words E[174][13]:	mock grad = 0.009152240967452707, computed grad = 0.009152240982419282
Iteration: 1383, Checking gradient for hidden W[88][33]:	mock grad = -0.013271966562061754, computed grad = -0.013271966592781695
Iteration: 1383, Checking gradient for hidden b[0][35]:	mock grad = 0.011517850683623321, computed grad = 0.011517850113843880
Iteration: 1383, Checking gradient for output layer W[139][1]:	mock grad = -0.106166992887818390, computed grad = -0.106166992275582073
Iteration: 1384, Checking gradient for words E[200][25]:	mock grad = 0.004571021439275791, computed grad = 0.004571021473262181
Iteration: 1384, Checking gradient for hidden W[200][37]:	mock grad = -0.005440186975819161, computed grad = -0.005440186983710412
Iteration: 1384, Checking gradient for hidden b[0][15]:	mock grad = -0.073832830991327203, computed grad = -0.073832847769255097
Iteration: 1384, Checking gradient for output layer W[16][0]:	mock grad = 0.021958724299359078, computed grad = 0.021958724287896032
Iteration: 1385, Checking gradient for words E[571][27]:	mock grad = -0.004426586397598964, computed grad = -0.004426586406913054
Iteration: 1385, Checking gradient for hidden W[174][48]:	mock grad = -0.002979717897033662, computed grad = -0.002979717850232795
Iteration: 1385, Checking gradient for hidden b[0][32]:	mock grad = -0.044139324529429702, computed grad = -0.044139339325223834
Iteration: 1385, Checking gradient for output layer W[22][0]:	mock grad = -0.024301552300098894, computed grad = -0.024301552284610367
Iteration: 1386, Checking gradient for words E[366][0]:	mock grad = 0.014993623521586397, computed grad = 0.014993623451665543
Iteration: 1386, Checking gradient for hidden W[97][9]:	mock grad = -0.013382025951580445, computed grad = -0.013382025988656079
Iteration: 1386, Checking gradient for hidden b[0][38]:	mock grad = 0.059012833319188607, computed grad = 0.059012848543416346
Iteration: 1386, Checking gradient for output layer W[111][1]:	mock grad = -0.027879088325133417, computed grad = -0.027879088308676366
Iteration: 1387, Checking gradient for words E[203][20]:	mock grad = 0.005021997790621846, computed grad = 0.005969343028490400
Iteration: 1387, Checking gradient for hidden W[40][31]:	mock grad = -0.002625539605116689, computed grad = -0.002625539592935005
Iteration: 1387, Checking gradient for hidden b[0][24]:	mock grad = -0.069654889065740022, computed grad = -0.069654886849308914
Iteration: 1387, Checking gradient for output layer W[105][0]:	mock grad = 0.014058379461306947, computed grad = 0.014058379459304856
Iteration: 1388, Checking gradient for words E[75][2]:	mock grad = -0.015303476441214103, computed grad = -0.014996052770536776
Iteration: 1388, Checking gradient for hidden W[78][19]:	mock grad = 0.006630744143060197, computed grad = 0.006630744136072918
Iteration: 1388, Checking gradient for hidden b[0][18]:	mock grad = -0.049106054138603472, computed grad = -0.049106042575038922
Iteration: 1388, Checking gradient for output layer W[91][1]:	mock grad = -0.005713649751604155, computed grad = -0.005713649751417868
current: 90, Cost = 0.319918, Correct(%) = 1, time = 6.17993
Iteration: 1389, Checking gradient for words E[200][38]:	mock grad = -0.023689409375804438, computed grad = -0.023689409368087059
Iteration: 1389, Checking gradient for hidden W[8][17]:	mock grad = -0.004311497272452280, computed grad = -0.004311497415899515
Iteration: 1389, Checking gradient for hidden b[0][14]:	mock grad = 0.049414088923727117, computed grad = 0.049414103826779562
Iteration: 1389, Checking gradient for output layer W[69][1]:	mock grad = -0.069492607929988459, computed grad = -0.069492607684771113
Iteration: 1390, Checking gradient for words E[719][38]:	mock grad = 0.010334781390675118, computed grad = 0.010334781411558688
Iteration: 1390, Checking gradient for hidden W[191][9]:	mock grad = 0.007299322862031055, computed grad = 0.007299322828538940
Iteration: 1390, Checking gradient for hidden b[0][11]:	mock grad = -0.072884099538650382, computed grad = -0.072884121490055992
Iteration: 1390, Checking gradient for output layer W[95][0]:	mock grad = -0.098561844065647719, computed grad = -0.098561843056849660
Iteration: 1391, Checking gradient for words E[199][18]:	mock grad = 0.030439193617642912, computed grad = 0.030439193552174872
Iteration: 1391, Checking gradient for hidden W[209][29]:	mock grad = 0.013294250886197734, computed grad = 0.013294250962749967
Iteration: 1391, Checking gradient for hidden b[0][9]:	mock grad = -0.046291126121977832, computed grad = -0.046291124902539588
Iteration: 1391, Checking gradient for output layer W[54][0]:	mock grad = -0.115547789233422238, computed grad = -0.115547788315327610
Iteration: 1392, Checking gradient for words E[109][48]:	mock grad = 0.003187823301192827, computed grad = 0.003187823329210251
Iteration: 1392, Checking gradient for hidden W[56][38]:	mock grad = -0.003476265669336009, computed grad = -0.003476265690002365
Iteration: 1392, Checking gradient for hidden b[0][30]:	mock grad = 0.000907528583560246, computed grad = 0.000907531657420702
Iteration: 1392, Checking gradient for output layer W[24][0]:	mock grad = -0.058408708936391607, computed grad = -0.058408708633564038
Iteration: 1393, Checking gradient for words E[102][27]:	mock grad = 0.005759845520647566, computed grad = 0.005759845527976774
Iteration: 1393, Checking gradient for hidden W[187][29]:	mock grad = 0.020227883338774788, computed grad = 0.020227883801286112
Iteration: 1393, Checking gradient for hidden b[0][32]:	mock grad = -0.049078142249969758, computed grad = -0.049078158664425907
Iteration: 1393, Checking gradient for output layer W[140][1]:	mock grad = 0.102275394352063786, computed grad = 0.102275393568434167
Iteration: 1394, Checking gradient for words E[327][38]:	mock grad = 0.008106862619206012, computed grad = 0.008106862611135159
Iteration: 1394, Checking gradient for hidden W[116][20]:	mock grad = 0.001834488458224426, computed grad = 0.001834488463452958
Iteration: 1394, Checking gradient for hidden b[0][6]:	mock grad = -0.027102319689192855, computed grad = -0.027102328499366595
Iteration: 1394, Checking gradient for output layer W[7][1]:	mock grad = -0.030222787218259217, computed grad = -0.030222787177983424
Iteration: 1395, Checking gradient for words E[102][49]:	mock grad = 0.008166799567055349, computed grad = 0.008166799613705630
Iteration: 1395, Checking gradient for hidden W[81][30]:	mock grad = 0.004435442713585092, computed grad = 0.004435442748876708
Iteration: 1395, Checking gradient for hidden b[0][28]:	mock grad = -0.017625338241566979, computed grad = -0.017625359170419051
Iteration: 1395, Checking gradient for output layer W[52][0]:	mock grad = 0.133576702815468851, computed grad = 0.133576701539622450
Iteration: 1396, Checking gradient for words E[25][30]:	mock grad = 0.005755128580003932, computed grad = 0.005755128571016673
Iteration: 1396, Checking gradient for hidden W[71][8]:	mock grad = 0.009625944725838442, computed grad = 0.009625944890855028
Iteration: 1396, Checking gradient for hidden b[0][31]:	mock grad = -0.022581860561227129, computed grad = -0.022581857731614835
Iteration: 1396, Checking gradient for output layer W[40][0]:	mock grad = 0.026743610338858526, computed grad = 0.026743610321171854
Iteration: 1397, Checking gradient for words E[102][38]:	mock grad = 0.028379333337102963, computed grad = 0.028379333368384198
Iteration: 1397, Checking gradient for hidden W[10][45]:	mock grad = -0.000086013728028966, computed grad = -0.000086013728887485
Iteration: 1397, Checking gradient for hidden b[0][24]:	mock grad = 0.105541570964623466, computed grad = 0.105541597141833601
Iteration: 1397, Checking gradient for output layer W[44][0]:	mock grad = 0.042603142763331103, computed grad = 0.042603142744859511
Iteration: 1398, Checking gradient for words E[775][42]:	mock grad = 0.007826117766673857, computed grad = 0.007826117724185091
Iteration: 1398, Checking gradient for hidden W[198][30]:	mock grad = 0.000679555624660821, computed grad = 0.000679555641205797
Iteration: 1398, Checking gradient for hidden b[0][23]:	mock grad = -0.108672129395886996, computed grad = -0.108672158891193971
Iteration: 1398, Checking gradient for output layer W[142][1]:	mock grad = 0.112303635282862135, computed grad = 0.112303633765205010
current: 100, Cost = 0.322641, Correct(%) = 1, time = 6.96493
Iteration: 1399, Checking gradient for words E[39][11]:	mock grad = 0.025839995262511017, computed grad = 0.025839995367463699
Iteration: 1399, Checking gradient for hidden W[160][45]:	mock grad = -0.002398361176464414, computed grad = -0.002398361189089971
Iteration: 1399, Checking gradient for hidden b[0][29]:	mock grad = -0.058520127170808012, computed grad = -0.058520130124805704
Iteration: 1399, Checking gradient for output layer W[122][0]:	mock grad = 0.121356936390593084, computed grad = 0.121356935118374573
current: 14, Correct(%) = 1, time = 7.01618
Dev start.
Dev finished. Total time taken is: 0.636183
dev:
Accuracy:	P=100/100=1
Test start.
Test finished. Total time taken is: 0.547121
test:
Accuracy:	P=62/100=0.62
##### Iteration 14
random: 0, 99
Iteration: 1400, Checking gradient for words E[21][39]:	mock grad = -0.008573936467373700, computed grad = -0.008573936490622051
Iteration: 1400, Checking gradient for hidden W[183][22]:	mock grad = 0.001950747354859117, computed grad = 0.001950747364311253
Iteration: 1400, Checking gradient for hidden b[0][35]:	mock grad = 0.008630260597233819, computed grad = 0.008630260704371253
Iteration: 1400, Checking gradient for output layer W[141][0]:	mock grad = 0.083690372072731112, computed grad = 0.083690370969118549
Iteration: 1401, Checking gradient for words E[86][40]:	mock grad = -0.003997369712654963, computed grad = -0.003997369715344487
Iteration: 1401, Checking gradient for hidden W[118][10]:	mock grad = -0.003925280438299428, computed grad = -0.003925280433519537
Iteration: 1401, Checking gradient for hidden b[0][31]:	mock grad = -0.032213174672779399, computed grad = -0.032213182449762706
Iteration: 1401, Checking gradient for output layer W[116][1]:	mock grad = -0.101270243988399455, computed grad = -0.101270243027563725
Iteration: 1402, Checking gradient for words E[48][22]:	mock grad = 0.026567535565263922, computed grad = 0.029860748342300576
Iteration: 1402, Checking gradient for hidden W[151][41]:	mock grad = -0.005347063079225167, computed grad = -0.005347063113035936
Iteration: 1402, Checking gradient for hidden b[0][24]:	mock grad = 0.073445286033396728, computed grad = 0.073445307548587621
Iteration: 1402, Checking gradient for output layer W[133][1]:	mock grad = 0.078647574730639214, computed grad = 0.078647574168241682
Iteration: 1403, Checking gradient for words E[97][18]:	mock grad = 0.018619775774747493, computed grad = 0.018619775789415718
Iteration: 1403, Checking gradient for hidden W[215][36]:	mock grad = 0.000591200865374120, computed grad = 0.000591200863172863
Iteration: 1403, Checking gradient for hidden b[0][13]:	mock grad = 0.113163319809178597, computed grad = 0.113163345156410716
Iteration: 1403, Checking gradient for output layer W[123][0]:	mock grad = -0.119118659227729351, computed grad = -0.119118658276886308
Iteration: 1404, Checking gradient for words E[157][2]:	mock grad = -0.014930342992686141, computed grad = -0.014930343007783198
Iteration: 1404, Checking gradient for hidden W[220][39]:	mock grad = 0.004186227388347508, computed grad = 0.004186227415123578
Iteration: 1404, Checking gradient for hidden b[0][34]:	mock grad = 0.025188134338438584, computed grad = 0.025188140686841980
Iteration: 1404, Checking gradient for output layer W[32][1]:	mock grad = -0.084757830223769481, computed grad = -0.084757829777035679
Iteration: 1405, Checking gradient for words E[241][37]:	mock grad = 0.003099658923838566, computed grad = 0.003099658924377816
Iteration: 1405, Checking gradient for hidden W[240][18]:	mock grad = -0.013652790788271085, computed grad = -0.013652790822182587
Iteration: 1405, Checking gradient for hidden b[0][4]:	mock grad = -0.015777091775831531, computed grad = -0.015777097126734495
Iteration: 1405, Checking gradient for output layer W[76][1]:	mock grad = -0.039022927153492537, computed grad = -0.039022927107469935
Iteration: 1406, Checking gradient for words E[119][0]:	mock grad = 0.032814737614975109, computed grad = 0.032814737791453656
Iteration: 1406, Checking gradient for hidden W[176][49]:	mock grad = -0.002338078978592062, computed grad = -0.002338078983352165
Iteration: 1406, Checking gradient for hidden b[0][8]:	mock grad = -0.031497783599163109, computed grad = -0.031497800582639825
Iteration: 1406, Checking gradient for output layer W[51][1]:	mock grad = -0.036627507701975093, computed grad = -0.036627507619475440
Iteration: 1407, Checking gradient for words E[69][12]:	mock grad = -0.054471408036577129, computed grad = -0.054471408134289107
Iteration: 1407, Checking gradient for hidden W[140][28]:	mock grad = 0.027100864985113438, computed grad = 0.027100865195271929
Iteration: 1407, Checking gradient for hidden b[0][27]:	mock grad = 0.037947242021496574, computed grad = 0.037947259966038184
Iteration: 1407, Checking gradient for output layer W[10][0]:	mock grad = 0.056205197558584219, computed grad = 0.056205197522560729
Iteration: 1408, Checking gradient for words E[288][45]:	mock grad = -0.009169668514247276, computed grad = -0.009169668528233062
Iteration: 1408, Checking gradient for hidden W[142][1]:	mock grad = -0.008996379960934942, computed grad = -0.008996380128366866
Iteration: 1408, Checking gradient for hidden b[0][28]:	mock grad = -0.001942856528713843, computed grad = -0.001942860891855303
Iteration: 1408, Checking gradient for output layer W[103][1]:	mock grad = 0.106087578857994025, computed grad = 0.106087577670038652
current: 10, Cost = 0.327206, Correct(%) = 1, time = 0.771756
Iteration: 1409, Checking gradient for words E[199][43]:	mock grad = 0.016945816570412831, computed grad = 0.016945816665365068
Iteration: 1409, Checking gradient for hidden W[12][15]:	mock grad = -0.007703219230215108, computed grad = -0.007703219206601353
Iteration: 1409, Checking gradient for hidden b[0][39]:	mock grad = 0.070075390297574813, computed grad = 0.070075406685683767
Iteration: 1409, Checking gradient for output layer W[75][1]:	mock grad = 0.127580349112893510, computed grad = 0.127580347697130697
Iteration: 1410, Checking gradient for words E[123][26]:	mock grad = 0.006876176923381605, computed grad = 0.007667682590123168
Iteration: 1410, Checking gradient for hidden W[203][36]:	mock grad = -0.000445956746697451, computed grad = -0.000445956768499747
Iteration: 1410, Checking gradient for hidden b[0][43]:	mock grad = 0.007916445551037210, computed grad = 0.007916447722238725
Iteration: 1410, Checking gradient for output layer W[109][1]:	mock grad = 0.110850054368727413, computed grad = 0.110850052475656768
Iteration: 1411, Checking gradient for words E[329][4]:	mock grad = 0.024494234868221998, computed grad = 0.024494234815908660
Iteration: 1411, Checking gradient for hidden W[147][38]:	mock grad = 0.013801114020495220, computed grad = 0.013801114297007464
Iteration: 1411, Checking gradient for hidden b[0][23]:	mock grad = 0.143797092537506499, computed grad = 0.143797121192513144
Iteration: 1411, Checking gradient for output layer W[124][0]:	mock grad = 0.044160956282596198, computed grad = 0.044160956249678522
Iteration: 1412, Checking gradient for words E[63][24]:	mock grad = 0.006871664273155886, computed grad = 0.006871664284129973
Iteration: 1412, Checking gradient for hidden W[20][36]:	mock grad = -0.003300884947154969, computed grad = -0.003300884978295223
Iteration: 1412, Checking gradient for hidden b[0][37]:	mock grad = -0.038111896559001046, computed grad = -0.038111906687315462
Iteration: 1412, Checking gradient for output layer W[44][0]:	mock grad = 0.029851432830046987, computed grad = 0.029851432799268260
Iteration: 1413, Checking gradient for words E[367][3]:	mock grad = 0.005761071111276550, computed grad = 0.005761071135047537
Iteration: 1413, Checking gradient for hidden W[96][44]:	mock grad = 0.012568937525464507, computed grad = 0.012568937613392690
Iteration: 1413, Checking gradient for hidden b[0][45]:	mock grad = -0.006842649747318408, computed grad = -0.006842650836643626
Iteration: 1413, Checking gradient for output layer W[115][1]:	mock grad = -0.025229679164129326, computed grad = -0.025229679144269403
Iteration: 1414, Checking gradient for words E[34][4]:	mock grad = 0.039577677332425232, computed grad = 0.039577677815903795
Iteration: 1414, Checking gradient for hidden W[118][45]:	mock grad = 0.006626220562522267, computed grad = 0.006626220643082736
Iteration: 1414, Checking gradient for hidden b[0][24]:	mock grad = 0.118875110384386584, computed grad = 0.118875141495529885
Iteration: 1414, Checking gradient for output layer W[50][0]:	mock grad = 0.144753377205320000, computed grad = 0.144753376794256011
Iteration: 1415, Checking gradient for words E[378][47]:	mock grad = 0.015747514126168705, computed grad = 0.015747514101113917
Iteration: 1415, Checking gradient for hidden W[30][9]:	mock grad = -0.008289375866915583, computed grad = -0.008289375875833881
Iteration: 1415, Checking gradient for hidden b[0][47]:	mock grad = -0.068163096650142307, computed grad = -0.068163109324848981
Iteration: 1415, Checking gradient for output layer W[50][1]:	mock grad = 0.123570256849114823, computed grad = 0.123570255443948201
Iteration: 1416, Checking gradient for words E[97][5]:	mock grad = 0.010186446929794091, computed grad = 0.010186446940754929
Iteration: 1416, Checking gradient for hidden W[246][32]:	mock grad = 0.000146144396362402, computed grad = 0.000146144391168620
Iteration: 1416, Checking gradient for hidden b[0][49]:	mock grad = -0.023676591024424098, computed grad = -0.023676584667698956
Iteration: 1416, Checking gradient for output layer W[43][1]:	mock grad = 0.098558570599205364, computed grad = 0.098558570033938420
Iteration: 1417, Checking gradient for words E[354][38]:	mock grad = 0.003173493058297727, computed grad = -0.000104247474415157
Iteration: 1417, Checking gradient for hidden W[185][0]:	mock grad = 0.003256471109741588, computed grad = 0.003256471156022783
Iteration: 1417, Checking gradient for hidden b[0][31]:	mock grad = 0.027638696101436877, computed grad = 0.027638706948381216
Iteration: 1417, Checking gradient for output layer W[145][0]:	mock grad = -0.067159557874951759, computed grad = -0.067159557309702467
Iteration: 1418, Checking gradient for words E[305][25]:	mock grad = -0.003908479961317601, computed grad = -0.003908480003789282
Iteration: 1418, Checking gradient for hidden W[96][31]:	mock grad = -0.003105115340823472, computed grad = -0.003105115334298464
Iteration: 1418, Checking gradient for hidden b[0][44]:	mock grad = 0.006801564936709248, computed grad = 0.006801573191229799
Iteration: 1418, Checking gradient for output layer W[46][1]:	mock grad = -0.066434327403830373, computed grad = -0.066434327019965322
current: 20, Cost = 0.428515, Correct(%) = 1, time = 1.4348
Iteration: 1419, Checking gradient for words E[42][14]:	mock grad = -0.041078806441663396, computed grad = -0.041078806571365228
Iteration: 1419, Checking gradient for hidden W[95][36]:	mock grad = -0.010067165925303767, computed grad = -0.010067165999320652
Iteration: 1419, Checking gradient for hidden b[0][12]:	mock grad = 0.078972191797727076, computed grad = 0.078972216909028073
Iteration: 1419, Checking gradient for output layer W[13][0]:	mock grad = 0.062252480721169956, computed grad = 0.062252480655848742
Iteration: 1420, Checking gradient for words E[48][41]:	mock grad = -0.032382272979636362, computed grad = -0.032382273040698989
Iteration: 1420, Checking gradient for hidden W[59][22]:	mock grad = 0.004475012511689913, computed grad = 0.004475012534232783
Iteration: 1420, Checking gradient for hidden b[0][20]:	mock grad = -0.088157574140518280, computed grad = -0.088157601504982253
Iteration: 1420, Checking gradient for output layer W[74][0]:	mock grad = 0.077477961869715228, computed grad = 0.077477961464002237
Iteration: 1421, Checking gradient for words E[102][24]:	mock grad = 0.066029116227439566, computed grad = 0.066029116412911815
Iteration: 1421, Checking gradient for hidden W[84][31]:	mock grad = -0.011265506656255031, computed grad = -0.011265506732142256
Iteration: 1421, Checking gradient for hidden b[0][4]:	mock grad = -0.024973893412882919, computed grad = -0.024973904305818320
Iteration: 1421, Checking gradient for output layer W[32][1]:	mock grad = -0.154364525013717113, computed grad = -0.154364524286861787
Iteration: 1422, Checking gradient for words E[351][25]:	mock grad = 0.004346912276215198, computed grad = 0.004346912273220571
Iteration: 1422, Checking gradient for hidden W[116][49]:	mock grad = -0.005854198686372492, computed grad = -0.005854198684391672
Iteration: 1422, Checking gradient for hidden b[0][19]:	mock grad = 0.031187197821525015, computed grad = 0.031187196643199112
Iteration: 1422, Checking gradient for output layer W[36][0]:	mock grad = -0.025666208398833490, computed grad = -0.025666208378154032
Iteration: 1423, Checking gradient for words E[102][4]:	mock grad = 0.030025313173476231, computed grad = 0.030025313204989260
Iteration: 1423, Checking gradient for hidden W[134][12]:	mock grad = 0.008141546117279574, computed grad = 0.008141546201332403
Iteration: 1423, Checking gradient for hidden b[0][40]:	mock grad = -0.086517694677129553, computed grad = -0.086517718405883043
Iteration: 1423, Checking gradient for output layer W[74][1]:	mock grad = -0.114928315083168142, computed grad = -0.114928313815849739
Iteration: 1424, Checking gradient for words E[462][39]:	mock grad = 0.031475846204054303, computed grad = 0.031475846156330353
Iteration: 1424, Checking gradient for hidden W[129][18]:	mock grad = 0.004079458443356776, computed grad = 0.004079458450787524
Iteration: 1424, Checking gradient for hidden b[0][7]:	mock grad = 0.076082536660859734, computed grad = 0.076082557000076143
Iteration: 1424, Checking gradient for output layer W[10][0]:	mock grad = 0.017396396938318404, computed grad = 0.017396396934800037
Iteration: 1425, Checking gradient for words E[306][14]:	mock grad = -0.032609961410939858, computed grad = -0.032609961447124518
Iteration: 1425, Checking gradient for hidden W[248][17]:	mock grad = 0.002436346331347039, computed grad = 0.002436346365036425
Iteration: 1425, Checking gradient for hidden b[0][4]:	mock grad = 0.020099799826395204, computed grad = 0.020099808320275997
Iteration: 1425, Checking gradient for output layer W[137][1]:	mock grad = -0.108716636757211216, computed grad = -0.108716635908957929
Iteration: 1426, Checking gradient for words E[276][4]:	mock grad = 0.006523814068307154, computed grad = 0.006523814080565113
Iteration: 1426, Checking gradient for hidden W[115][32]:	mock grad = 0.002758446975587026, computed grad = 0.002758446974040163
Iteration: 1426, Checking gradient for hidden b[0][28]:	mock grad = -0.006013678761532248, computed grad = -0.006013687398247453
Iteration: 1426, Checking gradient for output layer W[101][0]:	mock grad = -0.053170374898867112, computed grad = -0.053170374639053899
Iteration: 1427, Checking gradient for words E[485][21]:	mock grad = 0.013214584795528950, computed grad = 0.011162200529993449
Iteration: 1427, Checking gradient for hidden W[225][7]:	mock grad = -0.017701118563279161, computed grad = -0.017701118734454912
Iteration: 1427, Checking gradient for hidden b[0][43]:	mock grad = -0.013216231355722696, computed grad = -0.013216235235169470
Iteration: 1427, Checking gradient for output layer W[142][1]:	mock grad = -0.147029880544768954, computed grad = -0.147029879725364465
Iteration: 1428, Checking gradient for words E[393][40]:	mock grad = 0.006104219180930182, computed grad = 0.006104219199224123
Iteration: 1428, Checking gradient for hidden W[232][30]:	mock grad = -0.006528114510712113, computed grad = -0.006528114553889795
Iteration: 1428, Checking gradient for hidden b[0][7]:	mock grad = 0.050428917023614961, computed grad = 0.050428923022098296
Iteration: 1428, Checking gradient for output layer W[33][0]:	mock grad = -0.029324618803450164, computed grad = -0.029324618766323474
current: 30, Cost = 0.30155, Correct(%) = 1, time = 2.12608
Iteration: 1429, Checking gradient for words E[48][44]:	mock grad = 0.022564410817854652, computed grad = 0.022564410839912449
Iteration: 1429, Checking gradient for hidden W[234][39]:	mock grad = 0.010807148137642608, computed grad = 0.010807148415405962
Iteration: 1429, Checking gradient for hidden b[0][40]:	mock grad = -0.082905878609484951, computed grad = -0.082905897308275550
Iteration: 1429, Checking gradient for output layer W[43][0]:	mock grad = -0.095344569832123138, computed grad = -0.095344569076337343
Iteration: 1430, Checking gradient for words E[102][34]:	mock grad = -0.000969319631227705, computed grad = -0.000969319671522937
Iteration: 1430, Checking gradient for hidden W[73][24]:	mock grad = 0.004516197735254801, computed grad = 0.004516197716077791
Iteration: 1430, Checking gradient for hidden b[0][29]:	mock grad = -0.065858017033265659, computed grad = -0.065858035388995040
Iteration: 1430, Checking gradient for output layer W[45][0]:	mock grad = 0.022172537622461297, computed grad = 0.022172537611495471
Iteration: 1431, Checking gradient for words E[48][45]:	mock grad = 0.039290197356905976, computed grad = 0.039290197395986104
Iteration: 1431, Checking gradient for hidden W[222][47]:	mock grad = 0.008893281281618926, computed grad = 0.008893281307574922
Iteration: 1431, Checking gradient for hidden b[0][12]:	mock grad = 0.079968774550853716, computed grad = 0.079968801375662799
Iteration: 1431, Checking gradient for output layer W[117][0]:	mock grad = 0.129327367835996299, computed grad = 0.129327367147639899
Iteration: 1432, Checking gradient for words E[102][9]:	mock grad = 0.044846039702478580, computed grad = 0.044846039520275847
Iteration: 1432, Checking gradient for hidden W[59][45]:	mock grad = -0.000619444998933849, computed grad = -0.000619444989779779
Iteration: 1432, Checking gradient for hidden b[0][22]:	mock grad = 0.037527821481392332, computed grad = 0.037527830312028304
Iteration: 1432, Checking gradient for output layer W[78][1]:	mock grad = 0.129350592364324557, computed grad = 0.129350590327772824
Iteration: 1433, Checking gradient for words E[167][32]:	mock grad = -0.001252309342753000, computed grad = -0.001252309340215956
Iteration: 1433, Checking gradient for hidden W[39][3]:	mock grad = -0.004813300920780694, computed grad = -0.004813300927963281
Iteration: 1433, Checking gradient for hidden b[0][1]:	mock grad = 0.077244718236918475, computed grad = 0.077244747212942999
Iteration: 1433, Checking gradient for output layer W[37][1]:	mock grad = 0.032563553143932777, computed grad = 0.032563553100015435
Iteration: 1434, Checking gradient for words E[6][36]:	mock grad = -0.009412728706276186, computed grad = -0.009412728688363714
Iteration: 1434, Checking gradient for hidden W[77][23]:	mock grad = -0.016691402117907650, computed grad = -0.016691402407368779
Iteration: 1434, Checking gradient for hidden b[0][38]:	mock grad = 0.065911317641970113, computed grad = 0.065911332322216021
Iteration: 1434, Checking gradient for output layer W[119][1]:	mock grad = -0.056572951076278022, computed grad = -0.056572951001421817
Iteration: 1435, Checking gradient for words E[383][2]:	mock grad = 0.030696983390615662, computed grad = 0.031522796818775782
Iteration: 1435, Checking gradient for hidden W[174][12]:	mock grad = 0.010023539189774056, computed grad = 0.010023539285586167
Iteration: 1435, Checking gradient for hidden b[0][25]:	mock grad = 0.011979115183108346, computed grad = 0.011979119204896492
Iteration: 1435, Checking gradient for output layer W[149][1]:	mock grad = -0.010424681474996422, computed grad = -0.010424681474505286
Iteration: 1436, Checking gradient for words E[531][40]:	mock grad = 0.003315836733325250, computed grad = 0.003426201101350696
Iteration: 1436, Checking gradient for hidden W[43][25]:	mock grad = -0.004191538108405668, computed grad = -0.004191538233653294
Iteration: 1436, Checking gradient for hidden b[0][20]:	mock grad = 0.119801353645004882, computed grad = 0.119801389363945993
Iteration: 1436, Checking gradient for output layer W[32][0]:	mock grad = -0.111370621314943463, computed grad = -0.111370620948969962
Iteration: 1437, Checking gradient for words E[42][17]:	mock grad = -0.007545977382006352, computed grad = -0.007545977374920501
Iteration: 1437, Checking gradient for hidden W[66][0]:	mock grad = -0.005054404319526640, computed grad = -0.005054404348169286
Iteration: 1437, Checking gradient for hidden b[0][29]:	mock grad = 0.072040054082483929, computed grad = 0.072040079602335230
Iteration: 1437, Checking gradient for output layer W[57][1]:	mock grad = -0.155070429342596405, computed grad = -0.155070426497164959
Iteration: 1438, Checking gradient for words E[197][34]:	mock grad = 0.008432370192573702, computed grad = 0.008432370171037804
Iteration: 1438, Checking gradient for hidden W[18][14]:	mock grad = -0.002582381907340059, computed grad = -0.002582381854781165
Iteration: 1438, Checking gradient for hidden b[0][45]:	mock grad = -0.009208924362347215, computed grad = -0.009208925964843165
Iteration: 1438, Checking gradient for output layer W[5][0]:	mock grad = -0.045634844015179921, computed grad = -0.045634843974049592
current: 40, Cost = 0.242369, Correct(%) = 1, time = 2.56132
Iteration: 1439, Checking gradient for words E[65][4]:	mock grad = 0.021021389653291811, computed grad = 0.021021389697593162
Iteration: 1439, Checking gradient for hidden W[171][44]:	mock grad = 0.003386625705384394, computed grad = 0.003386625710629816
Iteration: 1439, Checking gradient for hidden b[0][11]:	mock grad = 0.063207643748841580, computed grad = 0.063207660375088182
Iteration: 1439, Checking gradient for output layer W[82][1]:	mock grad = -0.126255551189002890, computed grad = -0.126255547952804054
Iteration: 1440, Checking gradient for words E[776][19]:	mock grad = -0.016056510534503965, computed grad = -0.017225036880459184
Iteration: 1440, Checking gradient for hidden W[158][23]:	mock grad = -0.002725244626050616, computed grad = -0.002725244641585852
Iteration: 1440, Checking gradient for hidden b[0][32]:	mock grad = -0.040047864536887912, computed grad = -0.040047878040552654
Iteration: 1440, Checking gradient for output layer W[54][1]:	mock grad = -0.082333667059786286, computed grad = -0.082333666224843324
Iteration: 1441, Checking gradient for words E[559][48]:	mock grad = 0.049256774982714413, computed grad = 0.049256775018010679
Iteration: 1441, Checking gradient for hidden W[29][46]:	mock grad = -0.028020580036591802, computed grad = -0.028020580699812798
Iteration: 1441, Checking gradient for hidden b[0][4]:	mock grad = 0.019515540013087529, computed grad = 0.019515544160038777
Iteration: 1441, Checking gradient for output layer W[12][1]:	mock grad = 0.065555951315798833, computed grad = 0.065555951244695029
Iteration: 1442, Checking gradient for words E[6][35]:	mock grad = 0.057058819997984900, computed grad = 0.057058820112412097
Iteration: 1442, Checking gradient for hidden W[158][42]:	mock grad = 0.000440283591363055, computed grad = 0.000440283594802813
Iteration: 1442, Checking gradient for hidden b[0][19]:	mock grad = -0.053226520988242099, computed grad = -0.053226534685338424
Iteration: 1442, Checking gradient for output layer W[55][0]:	mock grad = -0.092157711369889039, computed grad = -0.092157711081644195
Iteration: 1443, Checking gradient for words E[156][2]:	mock grad = 0.013167966463722980, computed grad = 0.012535636547746740
Iteration: 1443, Checking gradient for hidden W[193][12]:	mock grad = -0.003869878279799677, computed grad = -0.003869878315528390
Iteration: 1443, Checking gradient for hidden b[0][42]:	mock grad = -0.066539264742676574, computed grad = -0.066539273752620767
Iteration: 1443, Checking gradient for output layer W[129][0]:	mock grad = 0.123073344303448184, computed grad = 0.123073343832191415
Iteration: 1444, Checking gradient for words E[566][3]:	mock grad = -0.002294991879980834, computed grad = -0.002294991871622833
Iteration: 1444, Checking gradient for hidden W[240][32]:	mock grad = -0.009730640128255419, computed grad = -0.009730640332930583
Iteration: 1444, Checking gradient for hidden b[0][42]:	mock grad = 0.039299548614424307, computed grad = 0.039299560215438172
Iteration: 1444, Checking gradient for output layer W[122][0]:	mock grad = -0.083164165536664392, computed grad = -0.083164164232845811
Iteration: 1445, Checking gradient for words E[222][38]:	mock grad = 0.059995136247714687, computed grad = 0.059995136168645283
Iteration: 1445, Checking gradient for hidden W[60][14]:	mock grad = 0.002718345368440955, computed grad = 0.002718345366096930
Iteration: 1445, Checking gradient for hidden b[0][41]:	mock grad = -0.035792255014321928, computed grad = -0.035792264992998556
Iteration: 1445, Checking gradient for output layer W[65][0]:	mock grad = -0.161223727788317728, computed grad = -0.161223725656950173
Iteration: 1446, Checking gradient for words E[48][40]:	mock grad = 0.038967113271171083, computed grad = 0.039847542742461405
Iteration: 1446, Checking gradient for hidden W[176][49]:	mock grad = 0.003045231710907537, computed grad = 0.003770986955868507
Iteration: 1446, Checking gradient for hidden b[0][39]:	mock grad = 0.054476221126295576, computed grad = 0.054476232069915448
Iteration: 1446, Checking gradient for output layer W[121][1]:	mock grad = -0.104277243497971628, computed grad = -0.104277241705217030
Iteration: 1447, Checking gradient for words E[148][27]:	mock grad = 0.004909802880898040, computed grad = 0.004909802902251599
Iteration: 1447, Checking gradient for hidden W[63][26]:	mock grad = -0.003040020431971380, computed grad = -0.003040020468175375
Iteration: 1447, Checking gradient for hidden b[0][18]:	mock grad = -0.044624338680326892, computed grad = -0.044624328210961836
Iteration: 1447, Checking gradient for output layer W[54][0]:	mock grad = 0.080801937309193939, computed grad = 0.080801936733682869
Iteration: 1448, Checking gradient for words E[156][5]:	mock grad = 0.003466978466037940, computed grad = 0.003170302512053694
Iteration: 1448, Checking gradient for hidden W[67][3]:	mock grad = 0.008571729324780009, computed grad = 0.008571729364467954
Iteration: 1448, Checking gradient for hidden b[0][16]:	mock grad = 0.013826905455704974, computed grad = 0.013826909855588170
Iteration: 1448, Checking gradient for output layer W[104][1]:	mock grad = -0.073563994335529292, computed grad = -0.073563993822436793
current: 50, Cost = 0.241773, Correct(%) = 1, time = 3.4531
Iteration: 1449, Checking gradient for words E[614][32]:	mock grad = 0.003250091816800471, computed grad = 0.003250091828087476
Iteration: 1449, Checking gradient for hidden W[51][19]:	mock grad = -0.001438435830641849, computed grad = -0.001438435830498189
Iteration: 1449, Checking gradient for hidden b[0][15]:	mock grad = -0.065226953933822385, computed grad = -0.065226968020043449
Iteration: 1449, Checking gradient for output layer W[136][1]:	mock grad = -0.098770657037325349, computed grad = -0.098770655477710315
Iteration: 1450, Checking gradient for words E[102][14]:	mock grad = -0.005675790131498548, computed grad = -0.005675790175609544
Iteration: 1450, Checking gradient for hidden W[10][46]:	mock grad = -0.017001221622292562, computed grad = -0.017001221687027307
Iteration: 1450, Checking gradient for hidden b[0][33]:	mock grad = -0.098526770727414004, computed grad = -0.098526784535391737
Iteration: 1450, Checking gradient for output layer W[45][1]:	mock grad = -0.015653429627587867, computed grad = -0.015653429626444785
Iteration: 1451, Checking gradient for words E[131][4]:	mock grad = 0.005089614419362398, computed grad = 0.005089614427350568
Iteration: 1451, Checking gradient for hidden W[47][18]:	mock grad = 0.002582794554867318, computed grad = 0.002582794543643319
Iteration: 1451, Checking gradient for hidden b[0][19]:	mock grad = 0.027388353411511579, computed grad = 0.027388345661907849
Iteration: 1451, Checking gradient for output layer W[27][1]:	mock grad = 0.041242570969102221, computed grad = 0.041242570920522768
Iteration: 1452, Checking gradient for words E[48][39]:	mock grad = -0.033621350824103446, computed grad = -0.032152364859756179
Iteration: 1452, Checking gradient for hidden W[171][15]:	mock grad = -0.003601633975891527, computed grad = -0.003601633984524947
Iteration: 1452, Checking gradient for hidden b[0][19]:	mock grad = 0.030345199898196284, computed grad = 0.030345192878296810
Iteration: 1452, Checking gradient for output layer W[21][1]:	mock grad = 0.041585905978053228, computed grad = 0.041585905935692051
Iteration: 1453, Checking gradient for words E[627][45]:	mock grad = -0.015188396243870361, computed grad = -0.015188396232678147
Iteration: 1453, Checking gradient for hidden W[137][41]:	mock grad = -0.008807278307876309, computed grad = -0.008807278331146548
Iteration: 1453, Checking gradient for hidden b[0][20]:	mock grad = 0.081430475848326411, computed grad = 0.081430493403224558
Iteration: 1453, Checking gradient for output layer W[136][0]:	mock grad = 0.107166589338625329, computed grad = 0.107166588184535908
Iteration: 1454, Checking gradient for words E[629][22]:	mock grad = 0.000366175217841880, computed grad = 0.000366175222102436
Iteration: 1454, Checking gradient for hidden W[20][36]:	mock grad = 0.008035402059056174, computed grad = 0.008035402108581266
Iteration: 1454, Checking gradient for hidden b[0][34]:	mock grad = 0.016100332084600932, computed grad = 0.016100333445556482
Iteration: 1454, Checking gradient for output layer W[5][1]:	mock grad = -0.031615770452741998, computed grad = -0.031615770398958146
Iteration: 1455, Checking gradient for words E[156][16]:	mock grad = 0.000191956344319788, computed grad = 0.000191956351370647
Iteration: 1455, Checking gradient for hidden W[244][29]:	mock grad = 0.011092055509698273, computed grad = 0.011092055625924893
Iteration: 1455, Checking gradient for hidden b[0][25]:	mock grad = -0.006941461682369310, computed grad = -0.006941462428310272
Iteration: 1455, Checking gradient for output layer W[36][1]:	mock grad = 0.003515794944258799, computed grad = 0.003515794944152573
Iteration: 1456, Checking gradient for words E[48][35]:	mock grad = 0.009319868204626935, computed grad = 0.009319868216797445
Iteration: 1456, Checking gradient for hidden W[227][37]:	mock grad = 0.003239104189783903, computed grad = 0.003239104152699509
Iteration: 1456, Checking gradient for hidden b[0][39]:	mock grad = -0.095849306499112830, computed grad = -0.095849340476172648
Iteration: 1456, Checking gradient for output layer W[125][1]:	mock grad = -0.027771190525716083, computed grad = -0.027771190521297704
Iteration: 1457, Checking gradient for words E[282][31]:	mock grad = -0.009976710045406234, computed grad = -0.009976710063813123
Iteration: 1457, Checking gradient for hidden W[62][11]:	mock grad = 0.001182512889164045, computed grad = 0.001182512897396718
Iteration: 1457, Checking gradient for hidden b[0][34]:	mock grad = -0.012869459287190299, computed grad = -0.012869452920771157
Iteration: 1457, Checking gradient for output layer W[92][1]:	mock grad = 0.071281729440070274, computed grad = 0.071281729123550464
Iteration: 1458, Checking gradient for words E[530][13]:	mock grad = 0.008502592486608895, computed grad = 0.008502592571510082
Iteration: 1458, Checking gradient for hidden W[203][20]:	mock grad = -0.003782189071932507, computed grad = -0.003782189060238121
Iteration: 1458, Checking gradient for hidden b[0][35]:	mock grad = -0.010639978244991743, computed grad = -0.010639976464836026
Iteration: 1458, Checking gradient for output layer W[94][1]:	mock grad = -0.122933294487703737, computed grad = -0.122933293397357757
current: 60, Cost = 0.315829, Correct(%) = 1, time = 4.14627
Iteration: 1459, Checking gradient for words E[225][7]:	mock grad = -0.017484403098622092, computed grad = -0.017484403132113024
Iteration: 1459, Checking gradient for hidden W[165][10]:	mock grad = 0.009696891418053966, computed grad = 0.009696891472866216
Iteration: 1459, Checking gradient for hidden b[0][5]:	mock grad = -0.001072822166076071, computed grad = -0.001072811366722633
Iteration: 1459, Checking gradient for output layer W[106][0]:	mock grad = 0.093802614330951162, computed grad = 0.093802613704019905
Iteration: 1460, Checking gradient for words E[215][25]:	mock grad = -0.010375181109745224, computed grad = -0.010375181126727062
Iteration: 1460, Checking gradient for hidden W[19][1]:	mock grad = -0.020496465793023999, computed grad = -0.020496465909587463
Iteration: 1460, Checking gradient for hidden b[0][29]:	mock grad = -0.076060721429904721, computed grad = -0.076060735135193083
Iteration: 1460, Checking gradient for output layer W[25][1]:	mock grad = 0.083371044490965218, computed grad = 0.083371044240515571
Iteration: 1461, Checking gradient for words E[99][44]:	mock grad = 0.001630260227675207, computed grad = 0.001630260258675858
Iteration: 1461, Checking gradient for hidden W[65][0]:	mock grad = -0.012635083269568748, computed grad = -0.012635083340321710
Iteration: 1461, Checking gradient for hidden b[0][23]:	mock grad = -0.137467519242845393, computed grad = -0.137467562668367432
Iteration: 1461, Checking gradient for output layer W[90][1]:	mock grad = -0.086904632195877740, computed grad = -0.086904631872096794
Iteration: 1462, Checking gradient for words E[350][37]:	mock grad = -0.000890423779653648, computed grad = -0.000890423692153008
Iteration: 1462, Checking gradient for hidden W[80][21]:	mock grad = 0.000385121811607991, computed grad = 0.000385121816226266
Iteration: 1462, Checking gradient for hidden b[0][17]:	mock grad = 0.015337147738714307, computed grad = 0.015337149946902226
Iteration: 1462, Checking gradient for output layer W[37][1]:	mock grad = -0.052955899264794937, computed grad = -0.052955899157842795
Iteration: 1463, Checking gradient for words E[208][43]:	mock grad = 0.006852793542067026, computed grad = 0.006852793500575128
Iteration: 1463, Checking gradient for hidden W[46][20]:	mock grad = 0.006737228160663689, computed grad = 0.006737228208568504
Iteration: 1463, Checking gradient for hidden b[0][16]:	mock grad = -0.016173412263925657, computed grad = -0.016173417692729470
Iteration: 1463, Checking gradient for output layer W[87][1]:	mock grad = -0.021784302460303717, computed grad = -0.021784302453212695
Iteration: 1464, Checking gradient for words E[460][18]:	mock grad = -0.019574963471546969, computed grad = -0.019574963508997886
Iteration: 1464, Checking gradient for hidden W[160][43]:	mock grad = -0.001150858700005308, computed grad = -0.001150858706440766
Iteration: 1464, Checking gradient for hidden b[0][23]:	mock grad = -0.135640376132672991, computed grad = -0.135640412619070644
Iteration: 1464, Checking gradient for output layer W[21][1]:	mock grad = 0.009154608715755685, computed grad = 0.009154608715414262
Iteration: 1465, Checking gradient for words E[102][34]:	mock grad = -0.002579587390874605, computed grad = -0.002579587401697473
Iteration: 1465, Checking gradient for hidden W[172][37]:	mock grad = -0.006638468388844854, computed grad = -0.006638468417006973
Iteration: 1465, Checking gradient for hidden b[0][5]:	mock grad = -0.002471113234536748, computed grad = -0.002471106132490939
Iteration: 1465, Checking gradient for output layer W[78][1]:	mock grad = 0.123138620930718368, computed grad = 0.123138617253560345
Iteration: 1466, Checking gradient for words E[180][37]:	mock grad = -0.003771032166088872, computed grad = -0.003771032157494019
Iteration: 1466, Checking gradient for hidden W[65][8]:	mock grad = 0.001922554805383614, computed grad = 0.001922554693483387
Iteration: 1466, Checking gradient for hidden b[0][27]:	mock grad = 0.037802391464503149, computed grad = 0.037802416050697278
Iteration: 1466, Checking gradient for output layer W[15][0]:	mock grad = -0.082007636627551284, computed grad = -0.082007636089911906
Iteration: 1467, Checking gradient for words E[671][22]:	mock grad = -0.000134419469860481, computed grad = -0.000134419466798610
Iteration: 1467, Checking gradient for hidden W[150][36]:	mock grad = 0.004004725360134964, computed grad = 0.004004725363972288
Iteration: 1467, Checking gradient for hidden b[0][17]:	mock grad = -0.015358199736581479, computed grad = -0.015358202790571511
Iteration: 1467, Checking gradient for output layer W[137][1]:	mock grad = 0.089128066403987294, computed grad = 0.089128065906966786
Iteration: 1468, Checking gradient for words E[258][42]:	mock grad = -0.008801434350613757, computed grad = -0.008801434362689819
Iteration: 1468, Checking gradient for hidden W[172][21]:	mock grad = 0.001293289369097561, computed grad = 0.001293289369870715
Iteration: 1468, Checking gradient for hidden b[0][32]:	mock grad = -0.041592248713973046, computed grad = -0.041592261656931383
Iteration: 1468, Checking gradient for output layer W[69][0]:	mock grad = 0.033044224549028112, computed grad = 0.033044224503913061
current: 70, Cost = 0.244972, Correct(%) = 1, time = 4.76622
Iteration: 1469, Checking gradient for words E[119][21]:	mock grad = 0.013314025525873463, computed grad = 0.013314025623913451
Iteration: 1469, Checking gradient for hidden W[177][38]:	mock grad = 0.003608139778732422, computed grad = 0.003608139886141958
Iteration: 1469, Checking gradient for hidden b[0][31]:	mock grad = 0.029023431684468970, computed grad = 0.029023442806334482
Iteration: 1469, Checking gradient for output layer W[68][1]:	mock grad = -0.153592115601772217, computed grad = -0.153592109939969856
Iteration: 1470, Checking gradient for words E[627][35]:	mock grad = 0.011191028948309700, computed grad = 0.011191028942595091
Iteration: 1470, Checking gradient for hidden W[55][30]:	mock grad = 0.006601101119885211, computed grad = 0.006601101232266340
Iteration: 1470, Checking gradient for hidden b[0][8]:	mock grad = -0.036634974781152296, computed grad = -0.036634994772981315
Iteration: 1470, Checking gradient for output layer W[107][0]:	mock grad = -0.099222137559945578, computed grad = -0.099222137111157765
Iteration: 1471, Checking gradient for words E[686][25]:	mock grad = -0.001136783100069705, computed grad = -0.001136783100713178
Iteration: 1471, Checking gradient for hidden W[223][37]:	mock grad = -0.011960781294340084, computed grad = -0.011960781374418493
Iteration: 1471, Checking gradient for hidden b[0][5]:	mock grad = 0.015656337506719797, computed grad = 0.015656336809523726
Iteration: 1471, Checking gradient for output layer W[15][1]:	mock grad = -0.084833291626507101, computed grad = -0.084833291304620070
Iteration: 1472, Checking gradient for words E[169][46]:	mock grad = 0.016853161376090275, computed grad = 0.021718924897091280
Iteration: 1472, Checking gradient for hidden W[55][15]:	mock grad = -0.003643805796871291, computed grad = -0.003643805813701480
Iteration: 1472, Checking gradient for hidden b[0][15]:	mock grad = 0.089670292459903544, computed grad = 0.089670312654669182
Iteration: 1472, Checking gradient for output layer W[1][0]:	mock grad = -0.022248714060313368, computed grad = -0.022248714055167842
Iteration: 1473, Checking gradient for words E[691][18]:	mock grad = -0.012818134305508933, computed grad = -0.012818134319276137
Iteration: 1473, Checking gradient for hidden W[29][19]:	mock grad = -0.014554492045598666, computed grad = -0.014554492174701108
Iteration: 1473, Checking gradient for hidden b[0][41]:	mock grad = -0.041525683230181354, computed grad = -0.041525695164402743
Iteration: 1473, Checking gradient for output layer W[148][0]:	mock grad = 0.130935029076489906, computed grad = 0.130935028359212807
Iteration: 1474, Checking gradient for words E[71][1]:	mock grad = -0.005186524357686517, computed grad = -0.005186524376669631
Iteration: 1474, Checking gradient for hidden W[78][10]:	mock grad = 0.001960285716001708, computed grad = 0.001960285732460142
Iteration: 1474, Checking gradient for hidden b[0][47]:	mock grad = 0.083427206889336025, computed grad = 0.083427228243664900
Iteration: 1474, Checking gradient for output layer W[1][1]:	mock grad = -0.003177879461802036, computed grad = -0.003177879461758949
Iteration: 1475, Checking gradient for words E[694][49]:	mock grad = -0.006875530910166461, computed grad = -0.006875530836258691
Iteration: 1475, Checking gradient for hidden W[203][9]:	mock grad = -0.017562637588869334, computed grad = -0.017562638012842812
Iteration: 1475, Checking gradient for hidden b[0][32]:	mock grad = -0.043414942710040538, computed grad = -0.043414955560610896
Iteration: 1475, Checking gradient for output layer W[16][0]:	mock grad = -0.023307439465325430, computed grad = -0.023307439451607195
Iteration: 1476, Checking gradient for words E[349][19]:	mock grad = -0.000448644541822807, computed grad = -0.000448644510386391
Iteration: 1476, Checking gradient for hidden W[154][14]:	mock grad = -0.022438198409829857, computed grad = -0.022438198861482715
Iteration: 1476, Checking gradient for hidden b[0][39]:	mock grad = 0.073594433149359872, computed grad = 0.073594452994683521
Iteration: 1476, Checking gradient for output layer W[68][0]:	mock grad = -0.121338103731855051, computed grad = -0.121338102439679352
Iteration: 1477, Checking gradient for words E[698][33]:	mock grad = -0.029375043105683840, computed grad = -0.029375043011706396
Iteration: 1477, Checking gradient for hidden W[119][46]:	mock grad = 0.012128663249150184, computed grad = 0.012128663341718514
Iteration: 1477, Checking gradient for hidden b[0][22]:	mock grad = 0.031521987084920378, computed grad = 0.031521993678410770
Iteration: 1477, Checking gradient for output layer W[97][0]:	mock grad = -0.063549827106795798, computed grad = -0.063549826732426151
Iteration: 1478, Checking gradient for words E[90][28]:	mock grad = 0.028253174295278516, computed grad = 0.028399323684858958
Iteration: 1478, Checking gradient for hidden W[26][28]:	mock grad = 0.003876151107445880, computed grad = 0.003876151146843818
Iteration: 1478, Checking gradient for hidden b[0][44]:	mock grad = 0.005622392007131882, computed grad = 0.005622402928915241
Iteration: 1478, Checking gradient for output layer W[119][1]:	mock grad = 0.164642395195480074, computed grad = 0.164642391936192983
current: 80, Cost = 0.270326, Correct(%) = 1, time = 5.25269
Iteration: 1479, Checking gradient for words E[704][30]:	mock grad = -0.016058492648934886, computed grad = -0.016058492703620524
Iteration: 1479, Checking gradient for hidden W[224][36]:	mock grad = 0.005689616697224231, computed grad = 0.005689616720562744
Iteration: 1479, Checking gradient for hidden b[0][24]:	mock grad = -0.066906430972790698, computed grad = -0.066906433280870800
Iteration: 1479, Checking gradient for output layer W[114][0]:	mock grad = 0.048161777135347084, computed grad = 0.048161777002083385
Iteration: 1480, Checking gradient for words E[306][13]:	mock grad = 0.028986069513403656, computed grad = 0.028986069518407143
Iteration: 1480, Checking gradient for hidden W[67][40]:	mock grad = -0.004369581524610311, computed grad = -0.004369581519700654
Iteration: 1480, Checking gradient for hidden b[0][3]:	mock grad = 0.033830792420974642, computed grad = 0.033830796460449911
Iteration: 1480, Checking gradient for output layer W[26][0]:	mock grad = -0.041329470559348103, computed grad = -0.041329470507575655
Iteration: 1481, Checking gradient for words E[34][8]:	mock grad = 0.004722702251491739, computed grad = 0.004722702283011051
Iteration: 1481, Checking gradient for hidden W[23][15]:	mock grad = 0.012011982739673632, computed grad = 0.012011982922258357
Iteration: 1481, Checking gradient for hidden b[0][25]:	mock grad = 0.003866918413697462, computed grad = 0.003866915631984475
Iteration: 1481, Checking gradient for output layer W[127][0]:	mock grad = 0.102917765982896148, computed grad = 0.102917764868863490
Iteration: 1482, Checking gradient for words E[438][35]:	mock grad = 0.002425958894880509, computed grad = 0.002425958908891711
Iteration: 1482, Checking gradient for hidden W[53][36]:	mock grad = 0.005388875486955769, computed grad = 0.005388875529351600
Iteration: 1482, Checking gradient for hidden b[0][40]:	mock grad = 0.083093616404822379, computed grad = 0.083093637462222383
Iteration: 1482, Checking gradient for output layer W[134][0]:	mock grad = 0.105177793070510006, computed grad = 0.105177792018498487
Iteration: 1483, Checking gradient for words E[217][10]:	mock grad = -0.010047400819668262, computed grad = -0.010047400808729765
Iteration: 1483, Checking gradient for hidden W[191][13]:	mock grad = -0.005974610447850859, computed grad = -0.005974610452396012
Iteration: 1483, Checking gradient for hidden b[0][26]:	mock grad = -0.028636327640718662, computed grad = -0.028636332958018190
Iteration: 1483, Checking gradient for output layer W[54][1]:	mock grad = 0.147870199351141096, computed grad = 0.147870197542664644
Iteration: 1484, Checking gradient for words E[34][29]:	mock grad = 0.007335433174188166, computed grad = 0.007335433197719638
Iteration: 1484, Checking gradient for hidden W[4][45]:	mock grad = -0.001921604599552929, computed grad = -0.001921604605021551
Iteration: 1484, Checking gradient for hidden b[0][14]:	mock grad = -0.041589521291962583, computed grad = -0.041589539907724453
Iteration: 1484, Checking gradient for output layer W[72][0]:	mock grad = -0.080673643235218417, computed grad = -0.080673642615768038
Iteration: 1485, Checking gradient for words E[152][31]:	mock grad = 0.005052102745289888, computed grad = 0.005052102774363639
Iteration: 1485, Checking gradient for hidden W[98][0]:	mock grad = 0.008983813982677002, computed grad = 0.008983814027520643
Iteration: 1485, Checking gradient for hidden b[0][44]:	mock grad = 0.012542794093239884, computed grad = 0.012542805475451687
Iteration: 1485, Checking gradient for output layer W[75][0]:	mock grad = 0.130620302401129873, computed grad = 0.130620299768150705
Iteration: 1486, Checking gradient for words E[740][12]:	mock grad = 0.005542951752451897, computed grad = 0.005542951741986727
Iteration: 1486, Checking gradient for hidden W[78][9]:	mock grad = -0.002113640360623670, computed grad = -0.002113640368128316
Iteration: 1486, Checking gradient for hidden b[0][39]:	mock grad = 0.072327490564794150, computed grad = 0.072327512228061400
Iteration: 1486, Checking gradient for output layer W[99][1]:	mock grad = 0.106865819853974031, computed grad = 0.106865818841688331
Iteration: 1487, Checking gradient for words E[34][7]:	mock grad = 0.013436302558289537, computed grad = 0.013436302529848050
Iteration: 1487, Checking gradient for hidden W[22][3]:	mock grad = -0.004852693840345923, computed grad = -0.004852693905437108
Iteration: 1487, Checking gradient for hidden b[0][19]:	mock grad = -0.033874126067506838, computed grad = -0.033874128350059124
Iteration: 1487, Checking gradient for output layer W[55][0]:	mock grad = -0.132435163125249611, computed grad = -0.132435161318004729
Iteration: 1488, Checking gradient for words E[102][21]:	mock grad = 0.006530475702204441, computed grad = 0.006530475678037530
Iteration: 1488, Checking gradient for hidden W[32][20]:	mock grad = -0.000854901270469499, computed grad = -0.000854901237936758
Iteration: 1488, Checking gradient for hidden b[0][26]:	mock grad = 0.023283416954456015, computed grad = 0.023283420705225547
Iteration: 1488, Checking gradient for output layer W[110][1]:	mock grad = 0.080036712425263223, computed grad = 0.080036711975929653
current: 90, Cost = 0.310829, Correct(%) = 1, time = 6.19696
Iteration: 1489, Checking gradient for words E[79][46]:	mock grad = -0.020678934804757576, computed grad = -0.020678934856895800
Iteration: 1489, Checking gradient for hidden W[222][4]:	mock grad = 0.007257014479039592, computed grad = 0.007257014612786641
Iteration: 1489, Checking gradient for hidden b[0][4]:	mock grad = -0.017097305922558759, computed grad = -0.017097312845917992
Iteration: 1489, Checking gradient for output layer W[89][0]:	mock grad = 0.120274966942529593, computed grad = 0.120274965556010444
Iteration: 1490, Checking gradient for words E[573][40]:	mock grad = 0.007837876848798553, computed grad = 0.007837876831142874
Iteration: 1490, Checking gradient for hidden W[214][42]:	mock grad = -0.005707178617281317, computed grad = -0.005707178632342897
Iteration: 1490, Checking gradient for hidden b[0][1]:	mock grad = -0.086841353815897415, computed grad = -0.086841371774510595
Iteration: 1490, Checking gradient for output layer W[74][0]:	mock grad = -0.181866201978186925, computed grad = -0.181866195095280958
Iteration: 1491, Checking gradient for words E[750][17]:	mock grad = 0.030138199179380321, computed grad = 0.030138199215660483
Iteration: 1491, Checking gradient for hidden W[127][44]:	mock grad = -0.001984581166569743, computed grad = -0.001984581168251717
Iteration: 1491, Checking gradient for hidden b[0][42]:	mock grad = -0.057429303188777370, computed grad = -0.057429315618287201
Iteration: 1491, Checking gradient for output layer W[114][0]:	mock grad = 0.116975256257895177, computed grad = 0.116975255233057587
Iteration: 1492, Checking gradient for words E[102][35]:	mock grad = 0.015912905604070771, computed grad = 0.015912905583742878
Iteration: 1492, Checking gradient for hidden W[218][40]:	mock grad = -0.000243707075228694, computed grad = -0.000243707104991086
Iteration: 1492, Checking gradient for hidden b[0][12]:	mock grad = 0.048146574476520820, computed grad = 0.048146588426889042
Iteration: 1492, Checking gradient for output layer W[117][1]:	mock grad = -0.125755700310647978, computed grad = -0.125755696986703219
Iteration: 1493, Checking gradient for words E[9][0]:	mock grad = 0.007706164906390311, computed grad = 0.007706164916588468
Iteration: 1493, Checking gradient for hidden W[116][17]:	mock grad = -0.006625681534838668, computed grad = -0.006625681635808448
Iteration: 1493, Checking gradient for hidden b[0][39]:	mock grad = -0.073345504513017135, computed grad = -0.073345534444644575
Iteration: 1493, Checking gradient for output layer W[148][0]:	mock grad = -0.136144506727831072, computed grad = -0.136144504731625593
Iteration: 1494, Checking gradient for words E[148][23]:	mock grad = 0.002338702055224018, computed grad = 0.002338702053899305
Iteration: 1494, Checking gradient for hidden W[183][18]:	mock grad = -0.007175668081046971, computed grad = -0.007175668097816437
Iteration: 1494, Checking gradient for hidden b[0][32]:	mock grad = -0.038861424963676505, computed grad = -0.038861438040098735
Iteration: 1494, Checking gradient for output layer W[20][0]:	mock grad = 0.006507027913826846, computed grad = 0.006507027913380092
Iteration: 1495, Checking gradient for words E[124][10]:	mock grad = -0.008918381756956029, computed grad = -0.008918381840334113
Iteration: 1495, Checking gradient for hidden W[171][4]:	mock grad = -0.003188461117548114, computed grad = -0.003188461123540689
Iteration: 1495, Checking gradient for hidden b[0][4]:	mock grad = -0.021932969365728860, computed grad = -0.021932980319674163
Iteration: 1495, Checking gradient for output layer W[99][0]:	mock grad = 0.202043753371822232, computed grad = 0.202043748591228534
Iteration: 1496, Checking gradient for words E[616][42]:	mock grad = -0.033103731972183859, computed grad = -0.033111351012525124
Iteration: 1496, Checking gradient for hidden W[100][28]:	mock grad = -0.007738963330394810, computed grad = -0.007738963381304392
Iteration: 1496, Checking gradient for hidden b[0][25]:	mock grad = 0.005466668047071632, computed grad = 0.005466666956101721
Iteration: 1496, Checking gradient for output layer W[145][0]:	mock grad = 0.130734524287634546, computed grad = 0.130734522009456677
Iteration: 1497, Checking gradient for words E[48][48]:	mock grad = 0.037451647526193232, computed grad = 0.037451647519477688
Iteration: 1497, Checking gradient for hidden W[26][26]:	mock grad = 0.002947213976672858, computed grad = 0.002947214028615633
Iteration: 1497, Checking gradient for hidden b[0][11]:	mock grad = 0.102513667889603255, computed grad = 0.102513694286408164
Iteration: 1497, Checking gradient for output layer W[28][0]:	mock grad = 0.018551606906364038, computed grad = 0.018551606904691882
Iteration: 1498, Checking gradient for words E[475][29]:	mock grad = 0.011265338877908260, computed grad = 0.011265338875688400
Iteration: 1498, Checking gradient for hidden W[172][5]:	mock grad = -0.011846159367639997, computed grad = -0.011846159447384878
Iteration: 1498, Checking gradient for hidden b[0][9]:	mock grad = 0.042906182157659289, computed grad = 0.042906190909376446
Iteration: 1498, Checking gradient for output layer W[142][1]:	mock grad = 0.109634920706452865, computed grad = 0.109634919172205525
current: 100, Cost = 0.314085, Correct(%) = 1, time = 6.98269
Iteration: 1499, Checking gradient for words E[49][19]:	mock grad = -0.010008688882651162, computed grad = -0.010008688896402620
Iteration: 1499, Checking gradient for hidden W[192][35]:	mock grad = -0.000501425112403586, computed grad = -0.000501425110882012
Iteration: 1499, Checking gradient for hidden b[0][20]:	mock grad = 0.085152377349517749, computed grad = 0.085152395125416086
Iteration: 1499, Checking gradient for output layer W[42][0]:	mock grad = 0.031057211507590221, computed grad = 0.031057211484536277
current: 15, Correct(%) = 1, time = 7.03406
Dev start.
Dev finished. Total time taken is: 0.642754
dev:
Accuracy:	P=100/100=1
Test start.
Test finished. Total time taken is: 0.548705
test:
Accuracy:	P=64/100=0.64
##### Iteration 15
random: 0, 99
Iteration: 1500, Checking gradient for words E[48][27]:	mock grad = 0.014345927451211993, computed grad = 0.014345927455773601
Iteration: 1500, Checking gradient for hidden W[209][3]:	mock grad = -0.009918403700348577, computed grad = -0.009918403787042531
Iteration: 1500, Checking gradient for hidden b[0][2]:	mock grad = 0.037909556826212776, computed grad = 0.037909578006105123
Iteration: 1500, Checking gradient for output layer W[11][1]:	mock grad = 0.032097878751274900, computed grad = 0.032097878684397105
Iteration: 1501, Checking gradient for words E[66][43]:	mock grad = 0.006164263187224961, computed grad = 0.006164263190883333
Iteration: 1501, Checking gradient for hidden W[113][38]:	mock grad = 0.008690156203278177, computed grad = 0.008690156272184954
Iteration: 1501, Checking gradient for hidden b[0][42]:	mock grad = -0.040799179530925933, computed grad = -0.040799178508221047
Iteration: 1501, Checking gradient for output layer W[60][1]:	mock grad = 0.094067123049046408, computed grad = 0.094067122227849770
Iteration: 1502, Checking gradient for words E[30][47]:	mock grad = -0.007372998384963525, computed grad = -0.007372998424801248
Iteration: 1502, Checking gradient for hidden W[158][32]:	mock grad = -0.006213955496714441, computed grad = -0.006213955535294437
Iteration: 1502, Checking gradient for hidden b[0][11]:	mock grad = 0.068086319435833875, computed grad = 0.068086336354798216
Iteration: 1502, Checking gradient for output layer W[92][1]:	mock grad = -0.020095118515417454, computed grad = -0.020095118505386676
Iteration: 1503, Checking gradient for words E[25][17]:	mock grad = 0.003973181344235632, computed grad = 0.003973181341628806
Iteration: 1503, Checking gradient for hidden W[43][28]:	mock grad = -0.011044375120672356, computed grad = -0.011044375215068654
Iteration: 1503, Checking gradient for hidden b[0][7]:	mock grad = -0.065131764375891965, computed grad = -0.065131769112429641
Iteration: 1503, Checking gradient for output layer W[58][0]:	mock grad = 0.022325528654554283, computed grad = 0.022325528647738319
Iteration: 1504, Checking gradient for words E[156][4]:	mock grad = 0.000830142234148346, computed grad = 0.002407125884188597
Iteration: 1504, Checking gradient for hidden W[55][27]:	mock grad = 0.001476075561113177, computed grad = 0.001476075557194294
Iteration: 1504, Checking gradient for hidden b[0][26]:	mock grad = 0.027051683877343802, computed grad = 0.027051690507933771
Iteration: 1504, Checking gradient for output layer W[108][0]:	mock grad = -0.150164434998711904, computed grad = -0.150164432311983759
Iteration: 1505, Checking gradient for words E[232][42]:	mock grad = -0.009740405269575980, computed grad = -0.009740405309430544
Iteration: 1505, Checking gradient for hidden W[86][47]:	mock grad = -0.003717692145577267, computed grad = -0.003717692138984255
Iteration: 1505, Checking gradient for hidden b[0][24]:	mock grad = 0.069307642421773119, computed grad = 0.069307655194610227
Iteration: 1505, Checking gradient for output layer W[64][0]:	mock grad = 0.164150979821919840, computed grad = 0.164150976200768128
Iteration: 1506, Checking gradient for words E[277][40]:	mock grad = 0.013024180233786598, computed grad = 0.014628117558150822
Iteration: 1506, Checking gradient for hidden W[224][7]:	mock grad = 0.002051077175671545, computed grad = 0.002051077149714423
Iteration: 1506, Checking gradient for hidden b[0][33]:	mock grad = 0.064517753434120695, computed grad = 0.064517768905676981
Iteration: 1506, Checking gradient for output layer W[3][1]:	mock grad = 0.028005747976289630, computed grad = 0.028005747936917656
Iteration: 1507, Checking gradient for words E[278][12]:	mock grad = 0.000667908903684289, computed grad = 0.000667908989413605
Iteration: 1507, Checking gradient for hidden W[229][42]:	mock grad = -0.028269746998960121, computed grad = -0.028269747646287788
Iteration: 1507, Checking gradient for hidden b[0][28]:	mock grad = 0.016302534012635483, computed grad = 0.016302553126838364
Iteration: 1507, Checking gradient for output layer W[55][1]:	mock grad = 0.217318363482210675, computed grad = 0.217318361230016022
Iteration: 1508, Checking gradient for words E[25][16]:	mock grad = -0.000380313741527027, computed grad = -0.000380313712110672
Iteration: 1508, Checking gradient for hidden W[35][25]:	mock grad = -0.006443586021837033, computed grad = -0.006443586084814746
Iteration: 1508, Checking gradient for hidden b[0][10]:	mock grad = -0.068766883933230050, computed grad = -0.068766905594525959
Iteration: 1508, Checking gradient for output layer W[6][0]:	mock grad = 0.032313232868119712, computed grad = 0.032313232832428325
current: 10, Cost = 0.320551, Correct(%) = 1, time = 0.773393
Iteration: 1509, Checking gradient for words E[305][28]:	mock grad = -0.029461151487331083, computed grad = -0.029461151448996359
Iteration: 1509, Checking gradient for hidden W[44][37]:	mock grad = -0.005466336553322515, computed grad = -0.005466336574818671
Iteration: 1509, Checking gradient for hidden b[0][46]:	mock grad = 0.124470635359419379, computed grad = 0.124470666461020743
Iteration: 1509, Checking gradient for output layer W[45][1]:	mock grad = -0.014666495641579846, computed grad = -0.014666495639282660
Iteration: 1510, Checking gradient for words E[54][33]:	mock grad = -0.029230187488182846, computed grad = -0.029434259983442748
Iteration: 1510, Checking gradient for hidden W[153][14]:	mock grad = 0.006019424962394604, computed grad = 0.006019424983385193
Iteration: 1510, Checking gradient for hidden b[0][38]:	mock grad = -0.046659025969286727, computed grad = -0.046659035480002317
Iteration: 1510, Checking gradient for output layer W[104][0]:	mock grad = -0.028273654268523574, computed grad = -0.028273654235078546
Iteration: 1511, Checking gradient for words E[134][23]:	mock grad = -0.007630297352484439, computed grad = -0.007630297397885651
Iteration: 1511, Checking gradient for hidden W[186][40]:	mock grad = 0.000106006952743654, computed grad = 0.000106006949038429
Iteration: 1511, Checking gradient for hidden b[0][11]:	mock grad = -0.095755550028109315, computed grad = -0.095755577835326663
Iteration: 1511, Checking gradient for output layer W[92][1]:	mock grad = 0.035155828483179885, computed grad = 0.035155828465539260
Iteration: 1512, Checking gradient for words E[350][16]:	mock grad = 0.004995672128277340, computed grad = 0.004995672135240531
Iteration: 1512, Checking gradient for hidden W[3][35]:	mock grad = -0.004200306113560925, computed grad = -0.004200306187825756
Iteration: 1512, Checking gradient for hidden b[0][9]:	mock grad = 0.039149818880507281, computed grad = 0.039149824750887886
Iteration: 1512, Checking gradient for output layer W[101][1]:	mock grad = 0.082136191857068441, computed grad = 0.082136191165403757
Iteration: 1513, Checking gradient for words E[158][17]:	mock grad = 0.020503507013158728, computed grad = 0.020503507018611054
Iteration: 1513, Checking gradient for hidden W[246][8]:	mock grad = 0.000954051920659671, computed grad = 0.000954051986468790
Iteration: 1513, Checking gradient for hidden b[0][46]:	mock grad = 0.101565879117582192, computed grad = 0.101565900906991496
Iteration: 1513, Checking gradient for output layer W[79][1]:	mock grad = 0.072005393657648131, computed grad = 0.072005393155242084
Iteration: 1514, Checking gradient for words E[69][7]:	mock grad = -0.029930163767510898, computed grad = -0.029930163785792746
Iteration: 1514, Checking gradient for hidden W[28][24]:	mock grad = 0.022925846774296543, computed grad = 0.022925846969122012
Iteration: 1514, Checking gradient for hidden b[0][32]:	mock grad = -0.070485569663158998, computed grad = -0.070485593250100509
Iteration: 1514, Checking gradient for output layer W[82][1]:	mock grad = -0.258866204168400893, computed grad = -0.258866201583506594
Iteration: 1515, Checking gradient for words E[102][41]:	mock grad = 0.017448467410446922, computed grad = 0.017448467425102053
Iteration: 1515, Checking gradient for hidden W[134][2]:	mock grad = -0.000282514592941929, computed grad = -0.000282514575913425
Iteration: 1515, Checking gradient for hidden b[0][44]:	mock grad = 0.002172434406783053, computed grad = 0.002172430382298624
Iteration: 1515, Checking gradient for output layer W[103][1]:	mock grad = -0.070131937837614178, computed grad = -0.070131937560710456
Iteration: 1516, Checking gradient for words E[166][22]:	mock grad = -0.008705707657430395, computed grad = -0.008705707663075950
Iteration: 1516, Checking gradient for hidden W[156][34]:	mock grad = 0.007250009353282350, computed grad = 0.007250009384441462
Iteration: 1516, Checking gradient for hidden b[0][39]:	mock grad = -0.078064600865096478, computed grad = -0.078064633192016980
Iteration: 1516, Checking gradient for output layer W[32][1]:	mock grad = -0.094521352746929077, computed grad = -0.094521352208776513
Iteration: 1517, Checking gradient for words E[286][7]:	mock grad = -0.000058032056618473, computed grad = -0.000058032053810143
Iteration: 1517, Checking gradient for hidden W[30][47]:	mock grad = 0.001446457447804028, computed grad = 0.001446457451843255
Iteration: 1517, Checking gradient for hidden b[0][7]:	mock grad = -0.041372421374905044, computed grad = -0.041372421820081502
Iteration: 1517, Checking gradient for output layer W[149][0]:	mock grad = -0.114406901083580204, computed grad = -0.114406898068026480
Iteration: 1518, Checking gradient for words E[298][12]:	mock grad = -0.001138635401770616, computed grad = -0.001138635422074119
Iteration: 1518, Checking gradient for hidden W[232][6]:	mock grad = -0.001954655488384249, computed grad = -0.001954655503484655
Iteration: 1518, Checking gradient for hidden b[0][38]:	mock grad = -0.041569434849370035, computed grad = -0.041569438867069701
Iteration: 1518, Checking gradient for output layer W[14][1]:	mock grad = 0.004137325319869545, computed grad = 0.004137325319792484
current: 20, Cost = 0.423644, Correct(%) = 1, time = 1.4349
Iteration: 1519, Checking gradient for words E[403][38]:	mock grad = -0.002229421295241174, computed grad = -0.002229421225110099
Iteration: 1519, Checking gradient for hidden W[99][47]:	mock grad = 0.001011811723516942, computed grad = 0.001011811690862894
Iteration: 1519, Checking gradient for hidden b[0][7]:	mock grad = 0.084602096414587713, computed grad = 0.084602113623798172
Iteration: 1519, Checking gradient for output layer W[7][1]:	mock grad = 0.033866810039917539, computed grad = 0.033866810028952955
Iteration: 1520, Checking gradient for words E[442][24]:	mock grad = 0.019887437051002710, computed grad = 0.019887437098500618
Iteration: 1520, Checking gradient for hidden W[189][4]:	mock grad = -0.003529851691608243, computed grad = -0.003529851829764359
Iteration: 1520, Checking gradient for hidden b[0][10]:	mock grad = -0.075110270655404099, computed grad = -0.075110297323843611
Iteration: 1520, Checking gradient for output layer W[19][0]:	mock grad = -0.010268642497729408, computed grad = -0.010268642496753045
Iteration: 1521, Checking gradient for words E[776][5]:	mock grad = -0.071982017597366221, computed grad = -0.071982017376742286
Iteration: 1521, Checking gradient for hidden W[194][2]:	mock grad = 0.029724535296843380, computed grad = 0.029724536085420472
Iteration: 1521, Checking gradient for hidden b[0][4]:	mock grad = -0.024866495192704674, computed grad = -0.024866506096026260
Iteration: 1521, Checking gradient for output layer W[65][0]:	mock grad = 0.185416446279645930, computed grad = 0.185416444968016153
Iteration: 1522, Checking gradient for words E[103][31]:	mock grad = -0.003300601675221282, computed grad = -0.001962367875263271
Iteration: 1522, Checking gradient for hidden W[74][30]:	mock grad = 0.010275295559497577, computed grad = 0.010275295613941233
Iteration: 1522, Checking gradient for hidden b[0][43]:	mock grad = 0.008177279222976175, computed grad = 0.008177281230787547
Iteration: 1522, Checking gradient for output layer W[2][0]:	mock grad = -0.046746998208196944, computed grad = -0.046746998074784503
Iteration: 1523, Checking gradient for words E[48][6]:	mock grad = 0.019342064891764155, computed grad = 0.019342064921546987
Iteration: 1523, Checking gradient for hidden W[208][27]:	mock grad = 0.001033205246497859, computed grad = 0.001033205245526067
Iteration: 1523, Checking gradient for hidden b[0][8]:	mock grad = -0.044902143393588112, computed grad = -0.044902165920753974
Iteration: 1523, Checking gradient for output layer W[70][1]:	mock grad = -0.112035234452134702, computed grad = -0.112035233196045067
Iteration: 1524, Checking gradient for words E[433][45]:	mock grad = 0.001757407578406589, computed grad = 0.001757407553275254
Iteration: 1524, Checking gradient for hidden W[72][4]:	mock grad = -0.000803935830878677, computed grad = -0.000803935849566724
Iteration: 1524, Checking gradient for hidden b[0][5]:	mock grad = -0.007702241815454869, computed grad = -0.007702235275597064
Iteration: 1524, Checking gradient for output layer W[143][0]:	mock grad = 0.155777212711760704, computed grad = 0.155777210046821046
Iteration: 1525, Checking gradient for words E[465][16]:	mock grad = -0.022143743954622375, computed grad = -0.022143744019740060
Iteration: 1525, Checking gradient for hidden W[216][47]:	mock grad = 0.007451507630207299, computed grad = 0.007451507736966798
Iteration: 1525, Checking gradient for hidden b[0][19]:	mock grad = -0.045931784596997538, computed grad = -0.045931797536688544
Iteration: 1525, Checking gradient for output layer W[104][1]:	mock grad = -0.037366830628077219, computed grad = -0.037366830591213110
Iteration: 1526, Checking gradient for words E[482][33]:	mock grad = 0.002879338928396535, computed grad = 0.002879338905564332
Iteration: 1526, Checking gradient for hidden W[141][30]:	mock grad = -0.001789158654660894, computed grad = -0.001789158649654772
Iteration: 1526, Checking gradient for hidden b[0][6]:	mock grad = -0.026019092626256013, computed grad = -0.026019101833510392
Iteration: 1526, Checking gradient for output layer W[4][1]:	mock grad = -0.012415244232524825, computed grad = -0.012415244228986167
Iteration: 1527, Checking gradient for words E[485][42]:	mock grad = -0.056716124412659052, computed grad = -0.060103517103996210
Iteration: 1527, Checking gradient for hidden W[163][2]:	mock grad = 0.019655637502030698, computed grad = 0.019655637895034217
Iteration: 1527, Checking gradient for hidden b[0][9]:	mock grad = -0.067964326392611074, computed grad = -0.067964337651771525
Iteration: 1527, Checking gradient for output layer W[15][0]:	mock grad = -0.121906779271679921, computed grad = -0.121906778772398069
Iteration: 1528, Checking gradient for words E[267][1]:	mock grad = 0.003930752914979574, computed grad = 0.003930752918649805
Iteration: 1528, Checking gradient for hidden W[144][21]:	mock grad = -0.002113019950136796, computed grad = -0.002113019951193514
Iteration: 1528, Checking gradient for hidden b[0][48]:	mock grad = 0.049408247868393262, computed grad = 0.049408243250325105
Iteration: 1528, Checking gradient for output layer W[47][0]:	mock grad = -0.034590517835664247, computed grad = -0.034590517770923007
current: 30, Cost = 0.29578, Correct(%) = 1, time = 2.13088
Iteration: 1529, Checking gradient for words E[504][7]:	mock grad = 0.010366424550711795, computed grad = 0.010366424597204982
Iteration: 1529, Checking gradient for hidden W[147][29]:	mock grad = -0.021453679103222711, computed grad = -0.021453679813549550
Iteration: 1529, Checking gradient for hidden b[0][32]:	mock grad = -0.045735838863802947, computed grad = -0.045735854619315099
Iteration: 1529, Checking gradient for output layer W[136][1]:	mock grad = 0.168996256845549420, computed grad = 0.168996252392040552
Iteration: 1530, Checking gradient for words E[511][4]:	mock grad = 0.009526888147759482, computed grad = 0.009526888063061498
Iteration: 1530, Checking gradient for hidden W[33][38]:	mock grad = 0.009211258329949734, computed grad = 0.009211258404837715
Iteration: 1530, Checking gradient for hidden b[0][29]:	mock grad = -0.064242093447824944, computed grad = -0.064242111164262539
Iteration: 1530, Checking gradient for output layer W[105][1]:	mock grad = 0.008404348722790278, computed grad = 0.008404348722123620
Iteration: 1531, Checking gradient for words E[409][17]:	mock grad = 0.000476367819302048, computed grad = 0.000476367791499056
Iteration: 1531, Checking gradient for hidden W[198][30]:	mock grad = -0.000704426929232227, computed grad = -0.000704426930943669
Iteration: 1531, Checking gradient for hidden b[0][43]:	mock grad = -0.012731152664119394, computed grad = -0.012731156584642689
Iteration: 1531, Checking gradient for output layer W[1][1]:	mock grad = -0.014683334454507202, computed grad = -0.014683334453413901
Iteration: 1532, Checking gradient for words E[379][33]:	mock grad = -0.032570510511004125, computed grad = -0.031614796644019595
Iteration: 1532, Checking gradient for hidden W[162][30]:	mock grad = 0.003913437879282666, computed grad = 0.003913437923691591
Iteration: 1532, Checking gradient for hidden b[0][45]:	mock grad = -0.008119491384767086, computed grad = -0.008119493577051126
Iteration: 1532, Checking gradient for output layer W[101][0]:	mock grad = 0.096047306589297499, computed grad = 0.096047305702852248
Iteration: 1533, Checking gradient for words E[143][0]:	mock grad = 0.016545279275359537, computed grad = 0.016545279263033383
Iteration: 1533, Checking gradient for hidden W[122][26]:	mock grad = -0.002951276718066165, computed grad = -0.002951276734590105
Iteration: 1533, Checking gradient for hidden b[0][35]:	mock grad = -0.007813479145607483, computed grad = -0.007813477313209976
Iteration: 1533, Checking gradient for output layer W[31][1]:	mock grad = 0.055236219371007644, computed grad = 0.055236219145450252
Iteration: 1534, Checking gradient for words E[68][46]:	mock grad = -0.054599819845263520, computed grad = -0.054696270330089516
Iteration: 1534, Checking gradient for hidden W[182][14]:	mock grad = -0.014525557970213043, computed grad = -0.014525558101361576
Iteration: 1534, Checking gradient for hidden b[0][12]:	mock grad = 0.071806939945212589, computed grad = 0.071806962354235573
Iteration: 1534, Checking gradient for output layer W[142][0]:	mock grad = 0.162177229044491167, computed grad = 0.162177227128534912
Iteration: 1535, Checking gradient for words E[383][4]:	mock grad = 0.006941927015335914, computed grad = 0.008855470068826287
Iteration: 1535, Checking gradient for hidden W[224][40]:	mock grad = -0.005121621062087467, computed grad = -0.005121621078528309
Iteration: 1535, Checking gradient for hidden b[0][39]:	mock grad = 0.075846443429977706, computed grad = 0.075846461208461308
Iteration: 1535, Checking gradient for output layer W[112][1]:	mock grad = -0.049085655185354682, computed grad = -0.049085655127267543
Iteration: 1536, Checking gradient for words E[156][27]:	mock grad = -0.018670691104644144, computed grad = -0.018670691075733867
Iteration: 1536, Checking gradient for hidden W[59][45]:	mock grad = -0.001793735494887150, computed grad = -0.001793735516432730
Iteration: 1536, Checking gradient for hidden b[0][30]:	mock grad = -0.003022907759497873, computed grad = -0.003022907900688689
Iteration: 1536, Checking gradient for output layer W[115][0]:	mock grad = 0.004426568765591998, computed grad = 0.004426568765514050
Iteration: 1537, Checking gradient for words E[524][49]:	mock grad = -0.016723508945049037, computed grad = -0.016723508971338515
Iteration: 1537, Checking gradient for hidden W[136][7]:	mock grad = -0.006531632875950155, computed grad = -0.008280986535997152
Iteration: 1537, Checking gradient for hidden b[0][45]:	mock grad = 0.007458770435481155, computed grad = 0.007458771593714911
Iteration: 1537, Checking gradient for output layer W[146][0]:	mock grad = 0.011606256868051945, computed grad = 0.011606256866711268
Iteration: 1538, Checking gradient for words E[493][47]:	mock grad = -0.000146175048870711, computed grad = -0.000146175071935526
Iteration: 1538, Checking gradient for hidden W[163][49]:	mock grad = -0.007591241967502826, computed grad = -0.007591242012392689
Iteration: 1538, Checking gradient for hidden b[0][27]:	mock grad = 0.031876473425562768, computed grad = 0.031876489292010951
Iteration: 1538, Checking gradient for output layer W[109][1]:	mock grad = -0.088713996805850348, computed grad = -0.088713996480266344
current: 40, Cost = 0.235872, Correct(%) = 1, time = 2.56645
Iteration: 1539, Checking gradient for words E[150][10]:	mock grad = -0.007900943489153045, computed grad = -0.007958734352938496
Iteration: 1539, Checking gradient for hidden W[98][39]:	mock grad = 0.002210408808892006, computed grad = 0.002210408877407088
Iteration: 1539, Checking gradient for hidden b[0][44]:	mock grad = -0.000416980230733000, computed grad = -0.000416976915234293
Iteration: 1539, Checking gradient for output layer W[130][0]:	mock grad = -0.102548155715545541, computed grad = -0.102548153851221277
Iteration: 1540, Checking gradient for words E[332][35]:	mock grad = -0.014927863781294404, computed grad = -0.014927863755021080
Iteration: 1540, Checking gradient for hidden W[100][11]:	mock grad = -0.008407899968213006, computed grad = -0.008407900017443898
Iteration: 1540, Checking gradient for hidden b[0][35]:	mock grad = -0.006641923764649782, computed grad = -0.006641921663686881
Iteration: 1540, Checking gradient for output layer W[107][1]:	mock grad = 0.034362279023827935, computed grad = 0.034362278958884773
Iteration: 1541, Checking gradient for words E[25][5]:	mock grad = 0.071954371431842690, computed grad = 0.071954371268455025
Iteration: 1541, Checking gradient for hidden W[187][23]:	mock grad = -0.005977243849430991, computed grad = -0.005977243926244290
Iteration: 1541, Checking gradient for hidden b[0][4]:	mock grad = 0.019163836795565992, computed grad = 0.019163840823835696
Iteration: 1541, Checking gradient for output layer W[106][0]:	mock grad = 0.056627002362807932, computed grad = 0.056627002313135652
Iteration: 1542, Checking gradient for words E[6][17]:	mock grad = -0.037052736280934750, computed grad = -0.037052736230965347
Iteration: 1542, Checking gradient for hidden W[242][38]:	mock grad = -0.003076732880302746, computed grad = -0.003076732881336202
Iteration: 1542, Checking gradient for hidden b[0][20]:	mock grad = 0.101738757205205310, computed grad = 0.101738780222940570
Iteration: 1542, Checking gradient for output layer W[14][0]:	mock grad = -0.035561071330708627, computed grad = -0.035561071312929189
Iteration: 1543, Checking gradient for words E[30][23]:	mock grad = -0.000613076817396196, computed grad = -0.000613076785631387
Iteration: 1543, Checking gradient for hidden W[102][46]:	mock grad = 0.003344359827794907, computed grad = 0.003344359827085654
Iteration: 1543, Checking gradient for hidden b[0][24]:	mock grad = -0.105452213534668049, computed grad = -0.105452225471165270
Iteration: 1543, Checking gradient for output layer W[20][0]:	mock grad = -0.025719082031222529, computed grad = -0.025719082026524367
Iteration: 1544, Checking gradient for words E[15][37]:	mock grad = 0.002805938244462847, computed grad = 0.002805938256007648
Iteration: 1544, Checking gradient for hidden W[77][8]:	mock grad = -0.000238974363797606, computed grad = -0.000238974360567621
Iteration: 1544, Checking gradient for hidden b[0][37]:	mock grad = -0.028357498644437928, computed grad = -0.028357505163507570
Iteration: 1544, Checking gradient for output layer W[18][1]:	mock grad = -0.051164217187843475, computed grad = -0.051164216862953298
Iteration: 1545, Checking gradient for words E[222][16]:	mock grad = -0.049176094591374397, computed grad = -0.049176094576972591
Iteration: 1545, Checking gradient for hidden W[203][3]:	mock grad = -0.016641588005877894, computed grad = -0.016641588226830921
Iteration: 1545, Checking gradient for hidden b[0][19]:	mock grad = -0.049394188959273055, computed grad = -0.049394203237842334
Iteration: 1545, Checking gradient for output layer W[125][1]:	mock grad = 0.021897461905895366, computed grad = 0.021897461900126546
Iteration: 1546, Checking gradient for words E[25][19]:	mock grad = 0.007703987634666287, computed grad = 0.007703987659250565
Iteration: 1546, Checking gradient for hidden W[83][17]:	mock grad = 0.002593676020387625, computed grad = 0.002593676013636079
Iteration: 1546, Checking gradient for hidden b[0][34]:	mock grad = -0.015082136998106899, computed grad = -0.015082135388564922
Iteration: 1546, Checking gradient for output layer W[50][1]:	mock grad = 0.128338355759136102, computed grad = 0.128338352206934320
Iteration: 1547, Checking gradient for words E[594][44]:	mock grad = -0.009825442209482338, computed grad = -0.009825442234297405
Iteration: 1547, Checking gradient for hidden W[145][39]:	mock grad = -0.007006795174263614, computed grad = -0.007006795214094224
Iteration: 1547, Checking gradient for hidden b[0][13]:	mock grad = 0.089657122655073529, computed grad = 0.089657141991095216
Iteration: 1547, Checking gradient for output layer W[122][0]:	mock grad = -0.088709321821922593, computed grad = -0.088709321009145928
Iteration: 1548, Checking gradient for words E[252][37]:	mock grad = 0.000897766916668763, computed grad = 0.000897766893975634
Iteration: 1548, Checking gradient for hidden W[226][20]:	mock grad = 0.005410556482121631, computed grad = 0.005410556506134223
Iteration: 1548, Checking gradient for hidden b[0][32]:	mock grad = 0.037057266272289002, computed grad = 0.037057273287967651
Iteration: 1548, Checking gradient for output layer W[70][0]:	mock grad = -0.121456152710419651, computed grad = -0.121456150248387920
current: 50, Cost = 0.235125, Correct(%) = 1, time = 3.45372
Iteration: 1549, Checking gradient for words E[354][41]:	mock grad = 0.007607706467127606, computed grad = 0.007607706490073777
Iteration: 1549, Checking gradient for hidden W[141][33]:	mock grad = 0.010914439051240565, computed grad = 0.010914439000657397
Iteration: 1549, Checking gradient for hidden b[0][6]:	mock grad = 0.026112470149899925, computed grad = 0.026112479152190369
Iteration: 1549, Checking gradient for output layer W[104][1]:	mock grad = -0.104228324482896761, computed grad = -0.104228322508973834
Iteration: 1550, Checking gradient for words E[102][9]:	mock grad = 0.002557008021097928, computed grad = 0.002557008045603161
Iteration: 1550, Checking gradient for hidden W[185][31]:	mock grad = 0.000375020536597104, computed grad = 0.000375020563453171
Iteration: 1550, Checking gradient for hidden b[0][15]:	mock grad = -0.091268850936104817, computed grad = -0.091268867920165933
Iteration: 1550, Checking gradient for output layer W[108][0]:	mock grad = 0.152186372043006113, computed grad = 0.152186370874531213
Iteration: 1551, Checking gradient for words E[374][26]:	mock grad = -0.012398354469894679, computed grad = -0.012398354469006778
Iteration: 1551, Checking gradient for hidden W[35][47]:	mock grad = 0.009611811233545264, computed grad = 0.009611811287637679
Iteration: 1551, Checking gradient for hidden b[0][28]:	mock grad = -0.008469853042952780, computed grad = -0.008469864987973631
Iteration: 1551, Checking gradient for output layer W[25][1]:	mock grad = -0.060015074295594051, computed grad = -0.060015074134745092
Iteration: 1552, Checking gradient for words E[42][33]:	mock grad = 0.020630633483176286, computed grad = 0.018813430204389255
Iteration: 1552, Checking gradient for hidden W[129][0]:	mock grad = 0.016896459275078657, computed grad = 0.016896459371102505
Iteration: 1552, Checking gradient for hidden b[0][37]:	mock grad = -0.038309665868652143, computed grad = -0.038309670493966011
Iteration: 1552, Checking gradient for output layer W[39][1]:	mock grad = -0.047168521311397038, computed grad = -0.047168521244296047
Iteration: 1553, Checking gradient for words E[68][23]:	mock grad = -0.009231053326286265, computed grad = -0.009231053370261253
Iteration: 1553, Checking gradient for hidden W[185][14]:	mock grad = 0.019120925062510175, computed grad = 0.019120925278140216
Iteration: 1553, Checking gradient for hidden b[0][23]:	mock grad = 0.105136089738555594, computed grad = 0.105136101770700569
Iteration: 1553, Checking gradient for output layer W[122][0]:	mock grad = 0.111562763202621706, computed grad = 0.111562761797989449
Iteration: 1554, Checking gradient for words E[119][0]:	mock grad = 0.008781656393591630, computed grad = 0.008781656423316317
Iteration: 1554, Checking gradient for hidden W[165][15]:	mock grad = -0.003478432035644574, computed grad = -0.003478432061411054
Iteration: 1554, Checking gradient for hidden b[0][4]:	mock grad = -0.012915236810517872, computed grad = -0.012915241912678217
Iteration: 1554, Checking gradient for output layer W[94][1]:	mock grad = -0.126199373119917357, computed grad = -0.126199369408372258
Iteration: 1555, Checking gradient for words E[304][31]:	mock grad = 0.010022059656528137, computed grad = 0.010890772868391782
Iteration: 1555, Checking gradient for hidden W[154][14]:	mock grad = 0.005114626739743944, computed grad = 0.005114626808110925
Iteration: 1555, Checking gradient for hidden b[0][2]:	mock grad = -0.047512978814473472, computed grad = -0.047512993411929577
Iteration: 1555, Checking gradient for output layer W[41][0]:	mock grad = -0.049416684719971737, computed grad = -0.049416684553167965
Iteration: 1556, Checking gradient for words E[102][38]:	mock grad = -0.003804589969957917, computed grad = -0.003804589998758189
Iteration: 1556, Checking gradient for hidden W[86][25]:	mock grad = 0.000814756023898022, computed grad = 0.000814756055883764
Iteration: 1556, Checking gradient for hidden b[0][42]:	mock grad = 0.078514383716171698, computed grad = 0.078514409048214329
Iteration: 1556, Checking gradient for output layer W[125][1]:	mock grad = -0.027095892989797976, computed grad = -0.027095892985322879
Iteration: 1557, Checking gradient for words E[639][22]:	mock grad = 0.006243649036424825, computed grad = 0.006243649051765951
Iteration: 1557, Checking gradient for hidden W[182][19]:	mock grad = 0.010389084830697559, computed grad = 0.010389084916995032
Iteration: 1557, Checking gradient for hidden b[0][37]:	mock grad = 0.042878221099329483, computed grad = 0.042878228671097127
Iteration: 1557, Checking gradient for output layer W[65][1]:	mock grad = 0.156829883938558456, computed grad = 0.156829880380038533
Iteration: 1558, Checking gradient for words E[642][1]:	mock grad = -0.015799907501085064, computed grad = -0.015799907475373860
Iteration: 1558, Checking gradient for hidden W[189][37]:	mock grad = -0.010751993824886785, computed grad = -0.010751994049228386
Iteration: 1558, Checking gradient for hidden b[0][1]:	mock grad = 0.100788676998364579, computed grad = 0.100788719700423052
Iteration: 1558, Checking gradient for output layer W[92][0]:	mock grad = 0.022430156591868311, computed grad = 0.022430156584734868
current: 60, Cost = 0.307161, Correct(%) = 1, time = 4.14724
Iteration: 1559, Checking gradient for words E[102][3]:	mock grad = 0.011486933484572415, computed grad = 0.010655930958244155
Iteration: 1559, Checking gradient for hidden W[217][38]:	mock grad = -0.013352792232995681, computed grad = -0.013352792331967478
Iteration: 1559, Checking gradient for hidden b[0][15]:	mock grad = -0.075526393635672040, computed grad = -0.075526410338681044
Iteration: 1559, Checking gradient for output layer W[139][0]:	mock grad = 0.046976049740132808, computed grad = 0.046976049654552605
Iteration: 1560, Checking gradient for words E[776][5]:	mock grad = 0.031773510111049452, computed grad = 0.031773510156397976
Iteration: 1560, Checking gradient for hidden W[249][14]:	mock grad = 0.015247625178849145, computed grad = 0.015247625287313960
Iteration: 1560, Checking gradient for hidden b[0][28]:	mock grad = 0.018876914179788429, computed grad = 0.018876933596280836
Iteration: 1560, Checking gradient for output layer W[61][1]:	mock grad = 0.149044961794686515, computed grad = 0.149044960286817130
Iteration: 1561, Checking gradient for words E[656][27]:	mock grad = 0.022299035032508074, computed grad = 0.022299035082887480
Iteration: 1561, Checking gradient for hidden W[74][27]:	mock grad = 0.001878030271545983, computed grad = 0.001878030272086925
Iteration: 1561, Checking gradient for hidden b[0][7]:	mock grad = -0.073882243634415534, computed grad = -0.073882253578640811
Iteration: 1561, Checking gradient for output layer W[13][1]:	mock grad = 0.059368626396799629, computed grad = 0.059368626284099169
Iteration: 1562, Checking gradient for words E[16][19]:	mock grad = 0.006238803741398513, computed grad = 0.006238803706807466
Iteration: 1562, Checking gradient for hidden W[233][20]:	mock grad = -0.007004405038091299, computed grad = -0.007004405164083162
Iteration: 1562, Checking gradient for hidden b[0][49]:	mock grad = 0.034286012073087013, computed grad = 0.034286019458751316
Iteration: 1562, Checking gradient for output layer W[110][0]:	mock grad = 0.098525760735518331, computed grad = 0.098525759995026579
Iteration: 1563, Checking gradient for words E[102][13]:	mock grad = 0.035230167616445707, computed grad = 0.035230167669252271
Iteration: 1563, Checking gradient for hidden W[2][18]:	mock grad = -0.013791202109869616, computed grad = -0.013791202170674068
Iteration: 1563, Checking gradient for hidden b[0][30]:	mock grad = -0.001366839467914494, computed grad = -0.001366843347830096
Iteration: 1563, Checking gradient for output layer W[76][1]:	mock grad = -0.023346087747067035, computed grad = -0.023346087737513878
Iteration: 1564, Checking gradient for words E[563][35]:	mock grad = -0.010374563987142205, computed grad = -0.010374564007853401
Iteration: 1564, Checking gradient for hidden W[136][47]:	mock grad = -0.002832034597444189, computed grad = -0.002832034608549789
Iteration: 1564, Checking gradient for hidden b[0][3]:	mock grad = 0.043588693560636660, computed grad = 0.043588704745454683
Iteration: 1564, Checking gradient for output layer W[102][0]:	mock grad = -0.203397233481089623, computed grad = -0.203397229303274946
Iteration: 1565, Checking gradient for words E[665][17]:	mock grad = -0.009821170058710305, computed grad = -0.009821170073447906
Iteration: 1565, Checking gradient for hidden W[193][48]:	mock grad = 0.004948952092284631, computed grad = 0.004948951990465663
Iteration: 1565, Checking gradient for hidden b[0][23]:	mock grad = 0.086037896920951318, computed grad = 0.086037910962401765
Iteration: 1565, Checking gradient for output layer W[147][1]:	mock grad = -0.032051446245032200, computed grad = -0.032051446175154069
Iteration: 1566, Checking gradient for words E[669][13]:	mock grad = -0.001671060789382173, computed grad = -0.001671060778630966
Iteration: 1566, Checking gradient for hidden W[181][30]:	mock grad = -0.000038704114246890, computed grad = -0.000038704124064130
Iteration: 1566, Checking gradient for hidden b[0][22]:	mock grad = 0.028604962094030961, computed grad = 0.028604964846538197
Iteration: 1566, Checking gradient for output layer W[93][0]:	mock grad = -0.034896548707835651, computed grad = -0.034896548663005497
Iteration: 1567, Checking gradient for words E[589][23]:	mock grad = 0.024207341456750076, computed grad = 0.024207341500939568
Iteration: 1567, Checking gradient for hidden W[147][31]:	mock grad = -0.011891193267471456, computed grad = -0.011891193442773132
Iteration: 1567, Checking gradient for hidden b[0][32]:	mock grad = -0.049478139932779275, computed grad = -0.049478156994225093
Iteration: 1567, Checking gradient for output layer W[12][1]:	mock grad = -0.051489435535068129, computed grad = -0.051489435431554681
Iteration: 1568, Checking gradient for words E[48][41]:	mock grad = -0.027694358347402082, computed grad = -0.027694358367563954
Iteration: 1568, Checking gradient for hidden W[113][1]:	mock grad = 0.012183311164537525, computed grad = 0.012183311228899197
Iteration: 1568, Checking gradient for hidden b[0][38]:	mock grad = -0.043731417954745444, computed grad = -0.043731423667381669
Iteration: 1568, Checking gradient for output layer W[139][0]:	mock grad = -0.045454513305731581, computed grad = -0.045454513179385737
current: 70, Cost = 0.239807, Correct(%) = 1, time = 4.76081
Iteration: 1569, Checking gradient for words E[679][33]:	mock grad = -0.022798926090861205, computed grad = -0.022798926070462404
Iteration: 1569, Checking gradient for hidden W[104][19]:	mock grad = 0.003594633020159721, computed grad = 0.003594633050827618
Iteration: 1569, Checking gradient for hidden b[0][42]:	mock grad = 0.046458632064455552, computed grad = 0.046458647050113440
Iteration: 1569, Checking gradient for output layer W[80][0]:	mock grad = 0.079986779136190234, computed grad = 0.079986778289547811
Iteration: 1570, Checking gradient for words E[335][46]:	mock grad = -0.010254011157923015, computed grad = -0.010254011136107091
Iteration: 1570, Checking gradient for hidden W[124][49]:	mock grad = -0.010479797575707606, computed grad = -0.010479797585247695
Iteration: 1570, Checking gradient for hidden b[0][43]:	mock grad = 0.011212503329444123, computed grad = 0.011212506522051842
Iteration: 1570, Checking gradient for output layer W[142][1]:	mock grad = 0.137912461304151579, computed grad = 0.137912460003289994
Iteration: 1571, Checking gradient for words E[78][17]:	mock grad = -0.021248717332694733, computed grad = -0.021248717341703513
Iteration: 1571, Checking gradient for hidden W[202][46]:	mock grad = -0.003581514274125785, computed grad = -0.003581514282503553
Iteration: 1571, Checking gradient for hidden b[0][43]:	mock grad = 0.010827459195339451, computed grad = 0.010827462115838354
Iteration: 1571, Checking gradient for output layer W[26][0]:	mock grad = -0.041729785348026960, computed grad = -0.041729785306947174
Iteration: 1572, Checking gradient for words E[34][10]:	mock grad = 0.013585429563311990, computed grad = 0.013585429562412155
Iteration: 1572, Checking gradient for hidden W[246][4]:	mock grad = 0.003544569051272539, computed grad = 0.003544569056010175
Iteration: 1572, Checking gradient for hidden b[0][10]:	mock grad = -0.085324296542288458, computed grad = -0.085324324265413670
Iteration: 1572, Checking gradient for output layer W[121][1]:	mock grad = 0.110881618210406652, computed grad = 0.110881617525785584
Iteration: 1573, Checking gradient for words E[49][1]:	mock grad = -0.011269351627068991, computed grad = -0.011269351683288038
Iteration: 1573, Checking gradient for hidden W[162][35]:	mock grad = -0.003111796975069359, computed grad = -0.003111797021857533
Iteration: 1573, Checking gradient for hidden b[0][47]:	mock grad = -0.076087290235810068, computed grad = -0.076087300932070806
Iteration: 1573, Checking gradient for output layer W[89][0]:	mock grad = -0.078791342370654327, computed grad = -0.078791342200398684
Iteration: 1574, Checking gradient for words E[42][42]:	mock grad = -0.028297711200719888, computed grad = -0.028297711251194811
Iteration: 1574, Checking gradient for hidden W[221][42]:	mock grad = 0.020558242736307131, computed grad = 0.020558243000181955
Iteration: 1574, Checking gradient for hidden b[0][4]:	mock grad = -0.021094075190353223, computed grad = -0.021094084134740329
Iteration: 1574, Checking gradient for output layer W[39][1]:	mock grad = -0.067865133980160586, computed grad = -0.067865133854596513
Iteration: 1575, Checking gradient for words E[25][22]:	mock grad = 0.029725442620420761, computed grad = 0.029725442571894453
Iteration: 1575, Checking gradient for hidden W[66][34]:	mock grad = 0.005438623346482041, computed grad = 0.005438623361232210
Iteration: 1575, Checking gradient for hidden b[0][48]:	mock grad = -0.066755861382272741, computed grad = -0.066755878033282506
Iteration: 1575, Checking gradient for output layer W[79][0]:	mock grad = 0.052006722311481735, computed grad = 0.052006722147717406
Iteration: 1576, Checking gradient for words E[49][48]:	mock grad = -0.000243707729080667, computed grad = 0.000640661057544827
Iteration: 1576, Checking gradient for hidden W[15][36]:	mock grad = -0.007741462758392714, computed grad = -0.007741462820880932
Iteration: 1576, Checking gradient for hidden b[0][26]:	mock grad = -0.024405508528280162, computed grad = -0.024405510435001505
Iteration: 1576, Checking gradient for output layer W[0][0]:	mock grad = -0.051769730890993682, computed grad = -0.051769730782756498
Iteration: 1577, Checking gradient for words E[698][41]:	mock grad = -0.011465239434307883, computed grad = -0.011465239439605139
Iteration: 1577, Checking gradient for hidden W[161][0]:	mock grad = -0.001028975595479542, computed grad = -0.001028975598846161
Iteration: 1577, Checking gradient for hidden b[0][39]:	mock grad = 0.054514231558791537, computed grad = 0.054514245278672185
Iteration: 1577, Checking gradient for output layer W[79][0]:	mock grad = -0.024809431337619037, computed grad = -0.024809431313688929
Iteration: 1578, Checking gradient for words E[555][46]:	mock grad = 0.019429165569961881, computed grad = 0.019429165634903035
Iteration: 1578, Checking gradient for hidden W[43][23]:	mock grad = 0.007176520949836451, computed grad = 0.007176520956260763
Iteration: 1578, Checking gradient for hidden b[0][49]:	mock grad = -0.034900924963332525, computed grad = -0.034900928528342437
Iteration: 1578, Checking gradient for output layer W[146][0]:	mock grad = 0.016952725370861232, computed grad = 0.016952725367097587
current: 80, Cost = 0.262831, Correct(%) = 1, time = 5.25216
Iteration: 1579, Checking gradient for words E[309][42]:	mock grad = -0.015005783295912734, computed grad = -0.015005783258595883
Iteration: 1579, Checking gradient for hidden W[89][28]:	mock grad = -0.004651948811024154, computed grad = -0.004651948870242267
Iteration: 1579, Checking gradient for hidden b[0][40]:	mock grad = 0.078284320589927514, computed grad = 0.078284343101949039
Iteration: 1579, Checking gradient for output layer W[75][1]:	mock grad = 0.094602405973448489, computed grad = 0.094602404881361457
Iteration: 1580, Checking gradient for words E[707][41]:	mock grad = 0.007593222804136435, computed grad = 0.007593222849928180
Iteration: 1580, Checking gradient for hidden W[176][30]:	mock grad = 0.002857815612794523, computed grad = 0.002857815622818384
Iteration: 1580, Checking gradient for hidden b[0][36]:	mock grad = 0.000093477689533383, computed grad = 0.000093468562359999
Iteration: 1580, Checking gradient for output layer W[89][0]:	mock grad = 0.076897761879718418, computed grad = 0.076897761520281480
Iteration: 1581, Checking gradient for words E[338][29]:	mock grad = -0.005163441913313971, computed grad = -0.005163441913848033
Iteration: 1581, Checking gradient for hidden W[182][22]:	mock grad = 0.000515975157128201, computed grad = 0.000515975157497228
Iteration: 1581, Checking gradient for hidden b[0][24]:	mock grad = -0.064949809574194051, computed grad = -0.064949809112294407
Iteration: 1581, Checking gradient for output layer W[87][0]:	mock grad = -0.092725713035229917, computed grad = -0.092725712156528292
Iteration: 1582, Checking gradient for words E[6][42]:	mock grad = -0.000665787025094788, computed grad = -0.000665787033918670
Iteration: 1582, Checking gradient for hidden W[35][22]:	mock grad = -0.002666720801597355, computed grad = -0.002666720804103103
Iteration: 1582, Checking gradient for hidden b[0][46]:	mock grad = 0.108653993488361245, computed grad = 0.108654010915090915
Iteration: 1582, Checking gradient for output layer W[32][1]:	mock grad = 0.075341665427219873, computed grad = 0.075341665009757558
Iteration: 1583, Checking gradient for words E[174][31]:	mock grad = -0.003138074936903656, computed grad = -0.003138074913532627
Iteration: 1583, Checking gradient for hidden W[170][48]:	mock grad = 0.005172116694374118, computed grad = 0.005172116694663990
Iteration: 1583, Checking gradient for hidden b[0][1]:	mock grad = -0.098868990259709522, computed grad = -0.098869007780035725
Iteration: 1583, Checking gradient for output layer W[82][1]:	mock grad = 0.208979374202833412, computed grad = 0.208979368700433071
Iteration: 1584, Checking gradient for words E[48][20]:	mock grad = 0.001977792157603631, computed grad = 0.001977792153068234
Iteration: 1584, Checking gradient for hidden W[152][20]:	mock grad = -0.009085821716797726, computed grad = -0.009085821819218725
Iteration: 1584, Checking gradient for hidden b[0][34]:	mock grad = -0.013623563671849581, computed grad = -0.013623559875115045
Iteration: 1584, Checking gradient for output layer W[95][0]:	mock grad = -0.076512973268522133, computed grad = -0.076512972693654785
Iteration: 1585, Checking gradient for words E[571][49]:	mock grad = 0.006845351923634446, computed grad = 0.006845351974190906
Iteration: 1585, Checking gradient for hidden W[64][31]:	mock grad = 0.007777806371456197, computed grad = 0.007777806421852441
Iteration: 1585, Checking gradient for hidden b[0][1]:	mock grad = 0.080710314717596932, computed grad = 0.080710346887716194
Iteration: 1585, Checking gradient for output layer W[129][0]:	mock grad = -0.083061247412336003, computed grad = -0.083061246673416578
Iteration: 1586, Checking gradient for words E[25][12]:	mock grad = -0.050654942817779380, computed grad = -0.050654942651260526
Iteration: 1586, Checking gradient for hidden W[15][49]:	mock grad = -0.008597771678081623, computed grad = -0.008597771748555437
Iteration: 1586, Checking gradient for hidden b[0][1]:	mock grad = -0.082971089935379716, computed grad = -0.082971103668184165
Iteration: 1586, Checking gradient for output layer W[23][0]:	mock grad = 0.083467748159038457, computed grad = 0.083467747641670351
Iteration: 1587, Checking gradient for words E[249][8]:	mock grad = 0.006755427331639474, computed grad = 0.006755427345446576
Iteration: 1587, Checking gradient for hidden W[75][24]:	mock grad = -0.005325504787767610, computed grad = -0.005325504828183857
Iteration: 1587, Checking gradient for hidden b[0][22]:	mock grad = 0.033716796300042873, computed grad = 0.033716800306727003
Iteration: 1587, Checking gradient for output layer W[87][1]:	mock grad = 0.094603695359601891, computed grad = 0.094603694654708650
Iteration: 1588, Checking gradient for words E[102][46]:	mock grad = 0.004351394697010846, computed grad = 0.004351394708646484
Iteration: 1588, Checking gradient for hidden W[244][36]:	mock grad = 0.003443616496318080, computed grad = 0.003443616455749659
Iteration: 1588, Checking gradient for hidden b[0][44]:	mock grad = 0.009354828431673567, computed grad = 0.009354837112558299
Iteration: 1588, Checking gradient for output layer W[32][0]:	mock grad = 0.076370979606832368, computed grad = 0.076370979183725318
current: 90, Cost = 0.303079, Correct(%) = 1, time = 6.20528
Iteration: 1589, Checking gradient for words E[79][18]:	mock grad = -0.012864444544402609, computed grad = -0.012864444502533736
Iteration: 1589, Checking gradient for hidden W[95][31]:	mock grad = -0.014427715730597201, computed grad = -0.014427715907065349
Iteration: 1589, Checking gradient for hidden b[0][47]:	mock grad = 0.054649203988788031, computed grad = 0.054649212391129430
Iteration: 1589, Checking gradient for output layer W[122][1]:	mock grad = 0.120171139320501830, computed grad = 0.120171137829662170
Iteration: 1590, Checking gradient for words E[719][20]:	mock grad = -0.009589892753902873, computed grad = -0.009589892788503647
Iteration: 1590, Checking gradient for hidden W[56][46]:	mock grad = 0.006089339690373841, computed grad = 0.006089339704342803
Iteration: 1590, Checking gradient for hidden b[0][23]:	mock grad = 0.102363740742711373, computed grad = 0.102363758047564937
Iteration: 1590, Checking gradient for output layer W[38][1]:	mock grad = 0.005140711225704031, computed grad = 0.005140711225550812
Iteration: 1591, Checking gradient for words E[25][26]:	mock grad = 0.011319569011875030, computed grad = 0.011319569035466666
Iteration: 1591, Checking gradient for hidden W[112][34]:	mock grad = -0.011507822726936068, computed grad = -0.011507822853647301
Iteration: 1591, Checking gradient for hidden b[0][47]:	mock grad = -0.054595978098231912, computed grad = -0.054595980687809575
Iteration: 1591, Checking gradient for output layer W[138][1]:	mock grad = -0.128635747339372886, computed grad = -0.128635745898411147
Iteration: 1592, Checking gradient for words E[581][1]:	mock grad = 0.013535598640729285, computed grad = 0.013535598628921137
Iteration: 1592, Checking gradient for hidden W[180][5]:	mock grad = -0.004927345362573665, computed grad = -0.004927345363928290
Iteration: 1592, Checking gradient for hidden b[0][21]:	mock grad = -0.034708456816037736, computed grad = -0.034708465266599778
Iteration: 1592, Checking gradient for output layer W[40][0]:	mock grad = 0.012056371972518631, computed grad = 0.012056371969327484
Iteration: 1593, Checking gradient for words E[320][49]:	mock grad = -0.013284699862364802, computed grad = -0.013284699879881149
Iteration: 1593, Checking gradient for hidden W[189][29]:	mock grad = -0.004231577092173033, computed grad = -0.004231577096566966
Iteration: 1593, Checking gradient for hidden b[0][28]:	mock grad = -0.015389984073910234, computed grad = -0.015390002765486764
Iteration: 1593, Checking gradient for output layer W[27][0]:	mock grad = -0.034240498023668797, computed grad = -0.034240497989585068
Iteration: 1594, Checking gradient for words E[152][44]:	mock grad = 0.010947772583619342, computed grad = 0.010947772602336132
Iteration: 1594, Checking gradient for hidden W[151][34]:	mock grad = 0.002181818771040955, computed grad = 0.002181818787373042
Iteration: 1594, Checking gradient for hidden b[0][3]:	mock grad = 0.020250219027212468, computed grad = 0.020250217428566284
Iteration: 1594, Checking gradient for output layer W[17][0]:	mock grad = -0.023357776784541096, computed grad = -0.023357776762690310
Iteration: 1595, Checking gradient for words E[354][17]:	mock grad = -0.017208238602572923, computed grad = -0.017208238656676075
Iteration: 1595, Checking gradient for hidden W[132][15]:	mock grad = 0.012935657795476452, computed grad = 0.012935657886408601
Iteration: 1595, Checking gradient for hidden b[0][14]:	mock grad = 0.072500993855140194, computed grad = 0.072501024607137643
Iteration: 1595, Checking gradient for output layer W[43][1]:	mock grad = 0.100249762144150045, computed grad = 0.100249761520054576
Iteration: 1596, Checking gradient for words E[267][30]:	mock grad = 0.009496434400124709, computed grad = 0.009496434431137603
Iteration: 1596, Checking gradient for hidden W[84][24]:	mock grad = 0.007861529880331952, computed grad = 0.007861529851106197
Iteration: 1596, Checking gradient for hidden b[0][11]:	mock grad = -0.072905729549571641, computed grad = -0.072905750789014123
Iteration: 1596, Checking gradient for output layer W[147][1]:	mock grad = -0.084131328737857558, computed grad = -0.084131328086868698
Iteration: 1597, Checking gradient for words E[186][26]:	mock grad = 0.017671704222954610, computed grad = 0.017671704169063392
Iteration: 1597, Checking gradient for hidden W[47][37]:	mock grad = 0.007783334184302548, computed grad = 0.007783334208007611
Iteration: 1597, Checking gradient for hidden b[0][40]:	mock grad = -0.117485148402290962, computed grad = -0.117485182456817938
Iteration: 1597, Checking gradient for output layer W[124][1]:	mock grad = -0.015329246096651428, computed grad = -0.015329246095514303
Iteration: 1598, Checking gradient for words E[772][49]:	mock grad = 0.012170874260308828, computed grad = 0.012170874316699680
Iteration: 1598, Checking gradient for hidden W[99][14]:	mock grad = 0.008450450361907613, computed grad = 0.008450450314834905
Iteration: 1598, Checking gradient for hidden b[0][7]:	mock grad = -0.044980750964068728, computed grad = -0.044980749478872838
Iteration: 1598, Checking gradient for output layer W[14][1]:	mock grad = 0.003365599549831622, computed grad = 0.003365599549736219
current: 100, Cost = 0.305784, Correct(%) = 1, time = 6.99007
Iteration: 1599, Checking gradient for words E[194][30]:	mock grad = 0.000019969840281409, computed grad = 0.000019969847592741
Iteration: 1599, Checking gradient for hidden W[99][27]:	mock grad = 0.014451136133214471, computed grad = 0.014451136238103870
Iteration: 1599, Checking gradient for hidden b[0][47]:	mock grad = -0.056598379325561998, computed grad = -0.056598384557842336
Iteration: 1599, Checking gradient for output layer W[144][1]:	mock grad = -0.061080344718911217, computed grad = -0.061080344528244845
current: 16, Correct(%) = 1, time = 7.04131
Dev start.
Dev finished. Total time taken is: 0.636226
dev:
Accuracy:	P=100/100=1
Test start.
Test finished. Total time taken is: 0.546927
test:
Accuracy:	P=64/100=0.64
##### Iteration 16
random: 0, 99
Iteration: 1600, Checking gradient for words E[22][5]:	mock grad = 0.014496171381370160, computed grad = 0.014496171344584322
Iteration: 1600, Checking gradient for hidden W[200][26]:	mock grad = -0.002424878430815069, computed grad = -0.002424878438120039
Iteration: 1600, Checking gradient for hidden b[0][1]:	mock grad = -0.064383231351008985, computed grad = -0.064383240768358163
Iteration: 1600, Checking gradient for output layer W[41][0]:	mock grad = 0.033444279369368446, computed grad = 0.033444279288533170
Iteration: 1601, Checking gradient for words E[79][14]:	mock grad = -0.001848649835101135, computed grad = -0.001848649810871843
Iteration: 1601, Checking gradient for hidden W[121][39]:	mock grad = 0.005739825338801907, computed grad = 0.005739825406769089
Iteration: 1601, Checking gradient for hidden b[0][10]:	mock grad = 0.071640604902084570, computed grad = 0.071640620976365241
Iteration: 1601, Checking gradient for output layer W[83][0]:	mock grad = -0.098879811526619843, computed grad = -0.098879810499075460
Iteration: 1602, Checking gradient for words E[104][19]:	mock grad = -0.011282591232286698, computed grad = -0.011282591301640398
Iteration: 1602, Checking gradient for hidden W[30][11]:	mock grad = 0.004928526347064421, computed grad = 0.004928526359516351
Iteration: 1602, Checking gradient for hidden b[0][8]:	mock grad = -0.038884012511231081, computed grad = -0.038884031690155384
Iteration: 1602, Checking gradient for output layer W[83][0]:	mock grad = 0.071203105370426512, computed grad = 0.071203104897652178
Iteration: 1603, Checking gradient for words E[96][45]:	mock grad = 0.016357205800898722, computed grad = 0.016357205781956270
Iteration: 1603, Checking gradient for hidden W[123][27]:	mock grad = -0.002644031241894051, computed grad = -0.002644031228309382
Iteration: 1603, Checking gradient for hidden b[0][13]:	mock grad = 0.107832331877416587, computed grad = 0.107832355627459797
Iteration: 1603, Checking gradient for output layer W[44][1]:	mock grad = -0.018280670648379571, computed grad = -0.018280670644314202
Iteration: 1604, Checking gradient for words E[163][29]:	mock grad = -0.006149471810212859, computed grad = -0.006556599894621053
Iteration: 1604, Checking gradient for hidden W[177][19]:	mock grad = -0.007095277086671681, computed grad = -0.007095277057127734
Iteration: 1604, Checking gradient for hidden b[0][14]:	mock grad = 0.047874055353391842, computed grad = 0.047874069148573756
Iteration: 1604, Checking gradient for output layer W[113][0]:	mock grad = -0.140207020365795687, computed grad = -0.140207018012577650
Iteration: 1605, Checking gradient for words E[197][21]:	mock grad = 0.015281447700299378, computed grad = 0.015281447690999619
Iteration: 1605, Checking gradient for hidden W[162][44]:	mock grad = -0.016897197708448664, computed grad = -0.016897197446027558
Iteration: 1605, Checking gradient for hidden b[0][38]:	mock grad = -0.048227714573439995, computed grad = -0.048227719174130823
Iteration: 1605, Checking gradient for output layer W[83][0]:	mock grad = 0.105381819586747305, computed grad = 0.105381818573105357
Iteration: 1606, Checking gradient for words E[267][31]:	mock grad = -0.006342054674104980, computed grad = -0.006342054676513187
Iteration: 1606, Checking gradient for hidden W[108][12]:	mock grad = -0.001247486285840815, computed grad = -0.001247486301136539
Iteration: 1606, Checking gradient for hidden b[0][27]:	mock grad = -0.018871901099934418, computed grad = -0.018871904528998507
Iteration: 1606, Checking gradient for output layer W[142][1]:	mock grad = 0.071983759997992780, computed grad = 0.071983759293923977
Iteration: 1607, Checking gradient for words E[69][19]:	mock grad = 0.021166068837980001, computed grad = 0.015978012594408668
Iteration: 1607, Checking gradient for hidden W[22][10]:	mock grad = 0.008211798537410830, computed grad = 0.008211798586089602
Iteration: 1607, Checking gradient for hidden b[0][41]:	mock grad = -0.044665529810222759, computed grad = -0.044665543935191156
Iteration: 1607, Checking gradient for output layer W[83][1]:	mock grad = 0.183488846042828557, computed grad = 0.183488844581375904
Iteration: 1608, Checking gradient for words E[247][38]:	mock grad = 0.018949834968695622, computed grad = 0.018949834986306785
Iteration: 1608, Checking gradient for hidden W[201][23]:	mock grad = 0.005796236114186115, computed grad = 0.005796236269832417
Iteration: 1608, Checking gradient for hidden b[0][32]:	mock grad = -0.043848878089058907, computed grad = -0.043848892634791858
Iteration: 1608, Checking gradient for output layer W[5][1]:	mock grad = -0.032507104525625330, computed grad = -0.032507104486892008
current: 10, Cost = 0.313465, Correct(%) = 1, time = 0.769165
Iteration: 1609, Checking gradient for words E[304][18]:	mock grad = -0.011772044711810814, computed grad = -0.012643247822320000
Iteration: 1609, Checking gradient for hidden W[189][45]:	mock grad = -0.000249210067304606, computed grad = -0.000249210065285989
Iteration: 1609, Checking gradient for hidden b[0][20]:	mock grad = 0.077868813340375986, computed grad = 0.077868824011412041
Iteration: 1609, Checking gradient for output layer W[140][1]:	mock grad = -0.080144371843393625, computed grad = -0.080144371443443427
Iteration: 1610, Checking gradient for words E[48][14]:	mock grad = 0.006092789141778909, computed grad = 0.006092789185400549
Iteration: 1610, Checking gradient for hidden W[22][33]:	mock grad = -0.003432232674543956, computed grad = -0.003432232676414847
Iteration: 1610, Checking gradient for hidden b[0][18]:	mock grad = -0.037971990175952874, computed grad = -0.037971979877611355
Iteration: 1610, Checking gradient for output layer W[10][1]:	mock grad = 0.010564022314335508, computed grad = 0.010564022312494599
Iteration: 1611, Checking gradient for words E[331][12]:	mock grad = -0.017528491507851074, computed grad = -0.017528491590766162
Iteration: 1611, Checking gradient for hidden W[152][31]:	mock grad = 0.013045717420168135, computed grad = 0.013045717507610141
Iteration: 1611, Checking gradient for hidden b[0][46]:	mock grad = 0.138977030789383393, computed grad = 0.138977060683755677
Iteration: 1611, Checking gradient for output layer W[90][1]:	mock grad = 0.062983690341134047, computed grad = 0.062983690233608600
Iteration: 1612, Checking gradient for words E[42][25]:	mock grad = 0.020472429599877051, computed grad = 0.020472429630870155
Iteration: 1612, Checking gradient for hidden W[14][37]:	mock grad = 0.005528113896258180, computed grad = 0.005528113953407122
Iteration: 1612, Checking gradient for hidden b[0][39]:	mock grad = -0.064276142025615490, computed grad = -0.064276169567736088
Iteration: 1612, Checking gradient for output layer W[9][0]:	mock grad = 0.034026226435696083, computed grad = 0.034026226383066023
Iteration: 1613, Checking gradient for words E[28][35]:	mock grad = 0.021078536390495284, computed grad = 0.021078536378893509
Iteration: 1613, Checking gradient for hidden W[1][40]:	mock grad = 0.010212927973651809, computed grad = 0.010212928085847338
Iteration: 1613, Checking gradient for hidden b[0][24]:	mock grad = -0.044364055902745836, computed grad = -0.044364048948807133
Iteration: 1613, Checking gradient for output layer W[14][0]:	mock grad = -0.005373492509036515, computed grad = -0.005373492508845776
Iteration: 1614, Checking gradient for words E[34][4]:	mock grad = 0.041460786011915385, computed grad = 0.041460786457197932
Iteration: 1614, Checking gradient for hidden W[67][24]:	mock grad = -0.005663812126716072, computed grad = -0.005663812131390279
Iteration: 1614, Checking gradient for hidden b[0][38]:	mock grad = -0.085971990761990469, computed grad = -0.085972013963233590
Iteration: 1614, Checking gradient for output layer W[90][0]:	mock grad = 0.055244629533629519, computed grad = 0.055244629506668683
Iteration: 1615, Checking gradient for words E[382][48]:	mock grad = 0.044865509194663700, computed grad = 0.044865509221109073
Iteration: 1615, Checking gradient for hidden W[187][45]:	mock grad = -0.000204749084770084, computed grad = -0.000204749066996125
Iteration: 1615, Checking gradient for hidden b[0][27]:	mock grad = 0.024940106377435711, computed grad = 0.024940116655693446
Iteration: 1615, Checking gradient for output layer W[140][0]:	mock grad = 0.112109474731170167, computed grad = 0.112109473516097444
Iteration: 1616, Checking gradient for words E[387][4]:	mock grad = -0.017285855364534974, computed grad = -0.017285855405134977
Iteration: 1616, Checking gradient for hidden W[145][15]:	mock grad = 0.004703444192066808, computed grad = 0.004703444236275819
Iteration: 1616, Checking gradient for hidden b[0][22]:	mock grad = -0.039007513601724275, computed grad = -0.039007521977482362
Iteration: 1616, Checking gradient for output layer W[114][1]:	mock grad = 0.106433170477260308, computed grad = 0.106433169661113713
Iteration: 1617, Checking gradient for words E[401][22]:	mock grad = 0.000742347880514083, computed grad = 0.000742347895338241
Iteration: 1617, Checking gradient for hidden W[151][36]:	mock grad = -0.003844665358215815, computed grad = -0.003844665352064914
Iteration: 1617, Checking gradient for hidden b[0][21]:	mock grad = 0.031672054394277227, computed grad = 0.031672060191414178
Iteration: 1617, Checking gradient for output layer W[5][1]:	mock grad = -0.035676945075807653, computed grad = -0.035676944978471035
Iteration: 1618, Checking gradient for words E[433][17]:	mock grad = 0.002559150314454528, computed grad = 0.002559150312114395
Iteration: 1618, Checking gradient for hidden W[110][23]:	mock grad = -0.002785003075961301, computed grad = -0.002785003080669640
Iteration: 1618, Checking gradient for hidden b[0][12]:	mock grad = -0.051053067771866800, computed grad = -0.051053084282303991
Iteration: 1618, Checking gradient for output layer W[68][1]:	mock grad = -0.121442902652937890, computed grad = -0.121442900002904627
current: 20, Cost = 0.418711, Correct(%) = 1, time = 1.42788
Iteration: 1619, Checking gradient for words E[54][7]:	mock grad = 0.032860096642828296, computed grad = 0.032860096588518795
Iteration: 1619, Checking gradient for hidden W[198][40]:	mock grad = 0.005201345434330262, computed grad = 0.005201345426278151
Iteration: 1619, Checking gradient for hidden b[0][6]:	mock grad = 0.041148472568169892, computed grad = 0.041148485541383471
Iteration: 1619, Checking gradient for output layer W[2][0]:	mock grad = 0.052306859101736158, computed grad = 0.052306859059414776
Iteration: 1620, Checking gradient for words E[153][22]:	mock grad = 0.004612834390294029, computed grad = 0.004612834401204540
Iteration: 1620, Checking gradient for hidden W[184][46]:	mock grad = 0.012668190599696150, computed grad = 0.012668190689463414
Iteration: 1620, Checking gradient for hidden b[0][37]:	mock grad = -0.036201997540369346, computed grad = -0.036202004324860537
Iteration: 1620, Checking gradient for output layer W[70][0]:	mock grad = 0.064308912352428749, computed grad = 0.064308912092131548
Iteration: 1621, Checking gradient for words E[102][14]:	mock grad = 0.011940544601407588, computed grad = 0.011940544759391011
Iteration: 1621, Checking gradient for hidden W[96][45]:	mock grad = 0.002115943367547723, computed grad = 0.002115943382317022
Iteration: 1621, Checking gradient for hidden b[0][0]:	mock grad = 0.194722058396101794, computed grad = 0.194722118956235840
Iteration: 1621, Checking gradient for output layer W[58][1]:	mock grad = 0.026042445068652498, computed grad = 0.026042445064839600
Iteration: 1622, Checking gradient for words E[215][11]:	mock grad = 0.002616001049338124, computed grad = 0.002616001066096123
Iteration: 1622, Checking gradient for hidden W[82][24]:	mock grad = -0.010242496226164910, computed grad = -0.010242496311671951
Iteration: 1622, Checking gradient for hidden b[0][13]:	mock grad = 0.084214305928115296, computed grad = 0.084214324167340024
Iteration: 1622, Checking gradient for output layer W[9][0]:	mock grad = 0.053565727115345574, computed grad = 0.053565726901739219
Iteration: 1623, Checking gradient for words E[457][40]:	mock grad = -0.039558881410245439, computed grad = -0.036556227546246503
Iteration: 1623, Checking gradient for hidden W[242][12]:	mock grad = 0.009335462168302833, computed grad = 0.009335462303483364
Iteration: 1623, Checking gradient for hidden b[0][20]:	mock grad = -0.084071367829535726, computed grad = -0.084071391624518654
Iteration: 1623, Checking gradient for output layer W[11][1]:	mock grad = -0.026884256559611686, computed grad = -0.026884256541148691
Iteration: 1624, Checking gradient for words E[148][48]:	mock grad = 0.012989884255798501, computed grad = 0.012989884242040072
Iteration: 1624, Checking gradient for hidden W[131][5]:	mock grad = -0.002469813929928399, computed grad = -0.002469814019543584
Iteration: 1624, Checking gradient for hidden b[0][34]:	mock grad = -0.015080844578629904, computed grad = -0.015080839370537993
Iteration: 1624, Checking gradient for output layer W[7][1]:	mock grad = 0.031655818018783144, computed grad = 0.031655817994615802
Iteration: 1625, Checking gradient for words E[51][23]:	mock grad = 0.016275519046149434, computed grad = 0.016275519048653084
Iteration: 1625, Checking gradient for hidden W[4][12]:	mock grad = 0.006253789282317790, computed grad = 0.006253789313448588
Iteration: 1625, Checking gradient for hidden b[0][3]:	mock grad = -0.034455011514306078, computed grad = -0.034455021067303727
Iteration: 1625, Checking gradient for output layer W[8][0]:	mock grad = 0.052323869270065604, computed grad = 0.052323869163880508
Iteration: 1626, Checking gradient for words E[276][31]:	mock grad = 0.002320350534212867, computed grad = 0.002320350506855318
Iteration: 1626, Checking gradient for hidden W[113][21]:	mock grad = -0.002507318904845790, computed grad = -0.002507318905430293
Iteration: 1626, Checking gradient for hidden b[0][8]:	mock grad = -0.026802876863282754, computed grad = -0.026802888344208079
Iteration: 1626, Checking gradient for output layer W[82][1]:	mock grad = -0.133756151620853525, computed grad = -0.133756146847234081
Iteration: 1627, Checking gradient for words E[485][22]:	mock grad = -0.030329290259284436, computed grad = -0.030329290546296298
Iteration: 1627, Checking gradient for hidden W[214][18]:	mock grad = 0.015606209178647967, computed grad = 0.015606209265157212
Iteration: 1627, Checking gradient for hidden b[0][10]:	mock grad = 0.103008469596155283, computed grad = 0.103008502758896683
Iteration: 1627, Checking gradient for output layer W[51][0]:	mock grad = -0.013627425040130259, computed grad = -0.013627425039347248
Iteration: 1628, Checking gradient for words E[183][17]:	mock grad = 0.003693137079777142, computed grad = 0.003693137068427776
Iteration: 1628, Checking gradient for hidden W[43][29]:	mock grad = 0.000026004643507993, computed grad = 0.000026004640026240
Iteration: 1628, Checking gradient for hidden b[0][39]:	mock grad = 0.055830264985595734, computed grad = 0.055830277892769224
Iteration: 1628, Checking gradient for output layer W[70][0]:	mock grad = -0.080470329494278237, computed grad = -0.080470328620164761
current: 30, Cost = 0.289787, Correct(%) = 1, time = 2.11788
Iteration: 1629, Checking gradient for words E[48][30]:	mock grad = 0.000907325540006054, computed grad = 0.000907325560767035
Iteration: 1629, Checking gradient for hidden W[191][16]:	mock grad = 0.001257257715991944, computed grad = 0.001257257709516587
Iteration: 1629, Checking gradient for hidden b[0][30]:	mock grad = 0.001841529976315259, computed grad = 0.001841529525379945
Iteration: 1629, Checking gradient for output layer W[119][0]:	mock grad = -0.143142409135266213, computed grad = -0.143142406263299410
Iteration: 1630, Checking gradient for words E[227][24]:	mock grad = 0.000175652314748920, computed grad = 0.000175652254299112
Iteration: 1630, Checking gradient for hidden W[188][36]:	mock grad = -0.007400720582528830, computed grad = -0.007400720664255112
Iteration: 1630, Checking gradient for hidden b[0][24]:	mock grad = -0.051013322194592980, computed grad = -0.051013316479077850
Iteration: 1630, Checking gradient for output layer W[28][1]:	mock grad = 0.021234244190815321, computed grad = 0.021234244179654950
Iteration: 1631, Checking gradient for words E[175][17]:	mock grad = 0.044688605101345091, computed grad = 0.044688605099166501
Iteration: 1631, Checking gradient for hidden W[45][1]:	mock grad = 0.032120712435329546, computed grad = 0.032120712778046748
Iteration: 1631, Checking gradient for hidden b[0][36]:	mock grad = -0.008107795440914778, computed grad = -0.008107791247941327
Iteration: 1631, Checking gradient for output layer W[94][0]:	mock grad = -0.145794236029506141, computed grad = -0.145794234868972306
Iteration: 1632, Checking gradient for words E[194][11]:	mock grad = 0.008058727866966375, computed grad = 0.008058727899533463
Iteration: 1632, Checking gradient for hidden W[2][34]:	mock grad = -0.006993305904423597, computed grad = -0.006993305924221630
Iteration: 1632, Checking gradient for hidden b[0][7]:	mock grad = 0.059629028482888202, computed grad = 0.059629039132477635
Iteration: 1632, Checking gradient for output layer W[112][0]:	mock grad = 0.001543817443883100, computed grad = 0.001543817443869876
Iteration: 1633, Checking gradient for words E[48][7]:	mock grad = 0.008170579202049222, computed grad = 0.008684562819118844
Iteration: 1633, Checking gradient for hidden W[247][2]:	mock grad = 0.006433328702037944, computed grad = 0.006433328696713521
Iteration: 1633, Checking gradient for hidden b[0][31]:	mock grad = 0.030783865842959335, computed grad = 0.030783878401753288
Iteration: 1633, Checking gradient for output layer W[76][0]:	mock grad = 0.039680873815950424, computed grad = 0.039680873726132743
Iteration: 1634, Checking gradient for words E[6][19]:	mock grad = -0.011432696841784384, computed grad = -0.013234522389683921
Iteration: 1634, Checking gradient for hidden W[244][36]:	mock grad = -0.005719936214815258, computed grad = -0.005719936265459507
Iteration: 1634, Checking gradient for hidden b[0][47]:	mock grad = -0.066794303208106420, computed grad = -0.066794309868053861
Iteration: 1634, Checking gradient for output layer W[144][1]:	mock grad = -0.022690685197779281, computed grad = -0.022690685192184163
Iteration: 1635, Checking gradient for words E[156][35]:	mock grad = 0.020968136194537967, computed grad = 0.020968136196324694
Iteration: 1635, Checking gradient for hidden W[183][43]:	mock grad = 0.000069893320631120, computed grad = 0.000069893320484340
Iteration: 1635, Checking gradient for hidden b[0][12]:	mock grad = 0.068694996762042049, computed grad = 0.068695017368413561
Iteration: 1635, Checking gradient for output layer W[96][1]:	mock grad = 0.194908190623205302, computed grad = 0.194908186750035073
Iteration: 1636, Checking gradient for words E[48][38]:	mock grad = -0.078420684139046681, computed grad = -0.078420684095997034
Iteration: 1636, Checking gradient for hidden W[201][12]:	mock grad = -0.011408738906898419, computed grad = -0.011408739016675246
Iteration: 1636, Checking gradient for hidden b[0][21]:	mock grad = -0.060053697301970521, computed grad = -0.060053713118785934
Iteration: 1636, Checking gradient for output layer W[65][0]:	mock grad = -0.197672992609004838, computed grad = -0.197672990331067555
Iteration: 1637, Checking gradient for words E[153][38]:	mock grad = -0.000305880810208548, computed grad = -0.000305880802283313
Iteration: 1637, Checking gradient for hidden W[110][32]:	mock grad = 0.007215056383430563, computed grad = 0.007215056419112627
Iteration: 1637, Checking gradient for hidden b[0][29]:	mock grad = 0.069499226140079529, computed grad = 0.069499250812882427
Iteration: 1637, Checking gradient for output layer W[67][0]:	mock grad = 0.093163748737329355, computed grad = 0.093163748033680105
Iteration: 1638, Checking gradient for words E[542][17]:	mock grad = 0.019048598510762460, computed grad = 0.019048598552556226
Iteration: 1638, Checking gradient for hidden W[95][8]:	mock grad = -0.018094950001767973, computed grad = -0.018094950220687741
Iteration: 1638, Checking gradient for hidden b[0][44]:	mock grad = 0.008513030843010361, computed grad = 0.008513034262160617
Iteration: 1638, Checking gradient for output layer W[6][0]:	mock grad = -0.039496070061928457, computed grad = -0.039496070031092387
current: 40, Cost = 0.229738, Correct(%) = 1, time = 2.55251
Iteration: 1639, Checking gradient for words E[543][13]:	mock grad = 0.025134702346954474, computed grad = 0.025134702360852166
Iteration: 1639, Checking gradient for hidden W[233][13]:	mock grad = 0.001977814917369924, computed grad = 0.001977814917524404
Iteration: 1639, Checking gradient for hidden b[0][24]:	mock grad = 0.061021700771088372, computed grad = 0.061021717055781119
Iteration: 1639, Checking gradient for output layer W[115][0]:	mock grad = -0.099060953533625096, computed grad = -0.099060951732074604
Iteration: 1640, Checking gradient for words E[555][47]:	mock grad = 0.000436568333747611, computed grad = 0.000436568354201685
Iteration: 1640, Checking gradient for hidden W[30][43]:	mock grad = 0.000657101378906400, computed grad = 0.000657101380585166
Iteration: 1640, Checking gradient for hidden b[0][19]:	mock grad = 0.021824073880877259, computed grad = 0.021824068455091196
Iteration: 1640, Checking gradient for output layer W[40][0]:	mock grad = -0.008286124199924361, computed grad = -0.008286124198942110
Iteration: 1641, Checking gradient for words E[24][20]:	mock grad = -0.012556959181869720, computed grad = -0.012556959196501159
Iteration: 1641, Checking gradient for hidden W[122][28]:	mock grad = -0.017922627386507406, computed grad = -0.017922627498392730
Iteration: 1641, Checking gradient for hidden b[0][46]:	mock grad = 0.147161805705120496, computed grad = 0.147161834638911637
Iteration: 1641, Checking gradient for output layer W[69][1]:	mock grad = 0.083000518182968497, computed grad = 0.083000518016063077
Iteration: 1642, Checking gradient for words E[278][49]:	mock grad = -0.004222276152487936, computed grad = -0.004222276221669694
Iteration: 1642, Checking gradient for hidden W[20][19]:	mock grad = -0.002090167237289720, computed grad = -0.002090167246982009
Iteration: 1642, Checking gradient for hidden b[0][39]:	mock grad = 0.080879066384292875, computed grad = 0.080879090837782011
Iteration: 1642, Checking gradient for output layer W[101][0]:	mock grad = 0.109490619204871686, computed grad = 0.109490618649908167
Iteration: 1643, Checking gradient for words E[564][32]:	mock grad = -0.011849586850409821, computed grad = -0.011849586900123252
Iteration: 1643, Checking gradient for hidden W[94][40]:	mock grad = 0.004729381507889707, computed grad = 0.004729381508626847
Iteration: 1643, Checking gradient for hidden b[0][4]:	mock grad = 0.025687009357150403, computed grad = 0.025687020762911614
Iteration: 1643, Checking gradient for output layer W[49][1]:	mock grad = 0.031357811708543348, computed grad = 0.031357811699280237
Iteration: 1644, Checking gradient for words E[364][7]:	mock grad = -0.002494419884521704, computed grad = -0.002494419897520881
Iteration: 1644, Checking gradient for hidden W[18][40]:	mock grad = -0.002391507579205254, computed grad = -0.002391507581818703
Iteration: 1644, Checking gradient for hidden b[0][33]:	mock grad = 0.054924569894948538, computed grad = 0.054924580531776103
Iteration: 1644, Checking gradient for output layer W[5][0]:	mock grad = 0.023923218611346386, computed grad = 0.023923218575643355
Iteration: 1645, Checking gradient for words E[102][1]:	mock grad = 0.057164446081031661, computed grad = 0.057164446056039125
Iteration: 1645, Checking gradient for hidden W[248][6]:	mock grad = 0.003505714292056927, computed grad = 0.003505714348413611
Iteration: 1645, Checking gradient for hidden b[0][15]:	mock grad = -0.089959253776056292, computed grad = -0.089959274484553264
Iteration: 1645, Checking gradient for output layer W[30][1]:	mock grad = -0.014969176648754745, computed grad = -0.014969176646822896
Iteration: 1646, Checking gradient for words E[547][24]:	mock grad = -0.005641994320876309, computed grad = -0.005641994336715446
Iteration: 1646, Checking gradient for hidden W[39][42]:	mock grad = -0.005271831681585848, computed grad = -0.005271831682218297
Iteration: 1646, Checking gradient for hidden b[0][25]:	mock grad = 0.001347420908320851, computed grad = 0.001347417335485997
Iteration: 1646, Checking gradient for output layer W[97][0]:	mock grad = -0.106126520263705171, computed grad = -0.106126518125206876
Iteration: 1647, Checking gradient for words E[148][47]:	mock grad = 0.015427955681490690, computed grad = 0.015427955646454036
Iteration: 1647, Checking gradient for hidden W[131][37]:	mock grad = 0.006050996740336601, computed grad = 0.006050996760763844
Iteration: 1647, Checking gradient for hidden b[0][43]:	mock grad = 0.008065865030520980, computed grad = 0.008065866863045029
Iteration: 1647, Checking gradient for output layer W[82][0]:	mock grad = 0.143858887420883486, computed grad = 0.143858883696283912
Iteration: 1648, Checking gradient for words E[97][5]:	mock grad = 0.001581821576057685, computed grad = 0.001581821588207371
Iteration: 1648, Checking gradient for hidden W[161][44]:	mock grad = -0.008807591728887676, computed grad = -0.008807591822829412
Iteration: 1648, Checking gradient for hidden b[0][14]:	mock grad = -0.039508433224366701, computed grad = -0.039508450622262124
Iteration: 1648, Checking gradient for output layer W[102][1]:	mock grad = -0.096070250715785122, computed grad = -0.096070249418251583
current: 50, Cost = 0.228392, Correct(%) = 1, time = 3.44161
Iteration: 1649, Checking gradient for words E[610][18]:	mock grad = -0.002327738019711201, computed grad = -0.002327738019462082
Iteration: 1649, Checking gradient for hidden W[118][29]:	mock grad = -0.010184109786967599, computed grad = -0.010184109833402446
Iteration: 1649, Checking gradient for hidden b[0][30]:	mock grad = -0.000038612882585776, computed grad = -0.000038610995855540
Iteration: 1649, Checking gradient for output layer W[67][0]:	mock grad = -0.067647179082613862, computed grad = -0.067647178499974370
Iteration: 1650, Checking gradient for words E[48][17]:	mock grad = 0.032634187827035310, computed grad = 0.032634188055895505
Iteration: 1650, Checking gradient for hidden W[100][25]:	mock grad = 0.002035228838598302, computed grad = 0.002035228862909585
Iteration: 1650, Checking gradient for hidden b[0][4]:	mock grad = 0.023442925229683054, computed grad = 0.023442933864885940
Iteration: 1650, Checking gradient for output layer W[53][0]:	mock grad = -0.040054380659987343, computed grad = -0.040054380636581961
Iteration: 1651, Checking gradient for words E[444][0]:	mock grad = 0.041486478332075638, computed grad = 0.041486478264521252
Iteration: 1651, Checking gradient for hidden W[164][13]:	mock grad = 0.010304802295107995, computed grad = 0.010304802339247869
Iteration: 1651, Checking gradient for hidden b[0][48]:	mock grad = -0.072355047977867848, computed grad = -0.072355062768814954
Iteration: 1651, Checking gradient for output layer W[91][1]:	mock grad = -0.031477635527743519, computed grad = -0.031477635502895403
Iteration: 1652, Checking gradient for words E[451][41]:	mock grad = -0.008407783124525503, computed grad = -0.008407783134917154
Iteration: 1652, Checking gradient for hidden W[178][20]:	mock grad = 0.001652052008027649, computed grad = 0.001652052034951042
Iteration: 1652, Checking gradient for hidden b[0][46]:	mock grad = -0.123480201503212150, computed grad = -0.123480226748908017
Iteration: 1652, Checking gradient for output layer W[131][1]:	mock grad = 0.151916274177932298, computed grad = 0.151916271728540109
Iteration: 1653, Checking gradient for words E[624][25]:	mock grad = 0.001982075242379366, computed grad = 0.000047107779616771
Iteration: 1653, Checking gradient for hidden W[186][4]:	mock grad = 0.003998817695971546, computed grad = 0.003998817703491264
Iteration: 1653, Checking gradient for hidden b[0][0]:	mock grad = -0.134587070323460845, computed grad = -0.134587108757606072
Iteration: 1653, Checking gradient for output layer W[26][0]:	mock grad = 0.037777755520718159, computed grad = 0.037777755462682423
Iteration: 1654, Checking gradient for words E[631][34]:	mock grad = 0.002904776707335355, computed grad = 0.002904776705365614
Iteration: 1654, Checking gradient for hidden W[220][48]:	mock grad = -0.000213234228607639, computed grad = -0.000213234214559939
Iteration: 1654, Checking gradient for hidden b[0][39]:	mock grad = -0.051949312610224951, computed grad = -0.051949332522089824
Iteration: 1654, Checking gradient for output layer W[105][0]:	mock grad = -0.042958658866554256, computed grad = -0.042958658707631152
Iteration: 1655, Checking gradient for words E[634][42]:	mock grad = -0.000149021037731956, computed grad = 0.001054077842143166
Iteration: 1655, Checking gradient for hidden W[184][30]:	mock grad = 0.000455286414513534, computed grad = 0.000455286427124557
Iteration: 1655, Checking gradient for hidden b[0][37]:	mock grad = -0.033497219590172822, computed grad = -0.033497227145027043
Iteration: 1655, Checking gradient for output layer W[33][0]:	mock grad = 0.014082514746827091, computed grad = 0.014082514742781179
Iteration: 1656, Checking gradient for words E[187][16]:	mock grad = 0.005671742884905129, computed grad = 0.005671742980757359
Iteration: 1656, Checking gradient for hidden W[46][35]:	mock grad = -0.005629145052976625, computed grad = -0.005629145101362853
Iteration: 1656, Checking gradient for hidden b[0][49]:	mock grad = -0.036693982079588627, computed grad = -0.036693977844084416
Iteration: 1656, Checking gradient for output layer W[5][0]:	mock grad = 0.068986437259471733, computed grad = 0.068986437182038590
Iteration: 1657, Checking gradient for words E[102][31]:	mock grad = 0.008385690526163669, computed grad = 0.008385690523443285
Iteration: 1657, Checking gradient for hidden W[131][16]:	mock grad = 0.000447551758125941, computed grad = 0.000447551760320593
Iteration: 1657, Checking gradient for hidden b[0][42]:	mock grad = -0.046864280919434176, computed grad = -0.046864287728698877
Iteration: 1657, Checking gradient for output layer W[43][1]:	mock grad = -0.095366461638213273, computed grad = -0.095366460783698426
Iteration: 1658, Checking gradient for words E[102][17]:	mock grad = -0.034416511372759029, computed grad = -0.034416511523622448
Iteration: 1658, Checking gradient for hidden W[62][48]:	mock grad = 0.009223390026541489, computed grad = 0.009223390061907312
Iteration: 1658, Checking gradient for hidden b[0][0]:	mock grad = 0.135698396894423645, computed grad = 0.135698418932616166
Iteration: 1658, Checking gradient for output layer W[23][1]:	mock grad = 0.064889148856855927, computed grad = 0.064889148673932542
current: 60, Cost = 0.299504, Correct(%) = 1, time = 4.13112
Iteration: 1659, Checking gradient for words E[271][23]:	mock grad = 0.002498959232682951, computed grad = 0.002498959168479708
Iteration: 1659, Checking gradient for hidden W[68][40]:	mock grad = 0.003179287201032777, computed grad = 0.003179287216711379
Iteration: 1659, Checking gradient for hidden b[0][13]:	mock grad = -0.095141662978442954, computed grad = -0.095141681615762183
Iteration: 1659, Checking gradient for output layer W[79][0]:	mock grad = -0.105945250935352009, computed grad = -0.105945249877452810
Iteration: 1660, Checking gradient for words E[654][42]:	mock grad = -0.028796939055664161, computed grad = -0.028796939098967897
Iteration: 1660, Checking gradient for hidden W[27][7]:	mock grad = -0.002329506564147099, computed grad = -0.002329506562805888
Iteration: 1660, Checking gradient for hidden b[0][24]:	mock grad = -0.081857645356925302, computed grad = -0.081857645832214207
Iteration: 1660, Checking gradient for output layer W[37][0]:	mock grad = 0.041931371897535596, computed grad = 0.041931371861812297
Iteration: 1661, Checking gradient for words E[48][10]:	mock grad = -0.039940104096553020, computed grad = -0.039940104140190606
Iteration: 1661, Checking gradient for hidden W[12][1]:	mock grad = -0.007436227622309444, computed grad = -0.007436227730871443
Iteration: 1661, Checking gradient for hidden b[0][13]:	mock grad = 0.101147092193665245, computed grad = 0.101147110627962819
Iteration: 1661, Checking gradient for output layer W[125][0]:	mock grad = -0.147242757228549515, computed grad = -0.147242755354632354
Iteration: 1662, Checking gradient for words E[102][35]:	mock grad = 0.000625464068593207, computed grad = 0.000625464026500699
Iteration: 1662, Checking gradient for hidden W[192][13]:	mock grad = -0.010683303043823589, computed grad = -0.010683303085519354
Iteration: 1662, Checking gradient for hidden b[0][42]:	mock grad = -0.054302536786510203, computed grad = -0.054302548685060598
Iteration: 1662, Checking gradient for output layer W[108][1]:	mock grad = -0.048959226136985334, computed grad = -0.048959226039884111
Iteration: 1663, Checking gradient for words E[361][37]:	mock grad = -0.004761480698250997, computed grad = -0.005554196009653221
Iteration: 1663, Checking gradient for hidden W[108][45]:	mock grad = -0.000049893292525249, computed grad = -0.000049893312698910
Iteration: 1663, Checking gradient for hidden b[0][47]:	mock grad = 0.062232231707620711, computed grad = 0.062232243519269315
Iteration: 1663, Checking gradient for output layer W[102][0]:	mock grad = -0.180665979840094471, computed grad = -0.180665975051958527
Iteration: 1664, Checking gradient for words E[664][37]:	mock grad = -0.014226601711353482, computed grad = -0.014226601785949099
Iteration: 1664, Checking gradient for hidden W[4][40]:	mock grad = -0.005599367637498798, computed grad = -0.005599367661544443
Iteration: 1664, Checking gradient for hidden b[0][38]:	mock grad = -0.056261831013981745, computed grad = -0.056261837752925190
Iteration: 1664, Checking gradient for output layer W[77][0]:	mock grad = 0.112020885476959986, computed grad = 0.112020884729810935
Iteration: 1665, Checking gradient for words E[409][7]:	mock grad = 0.003871367515007051, computed grad = 0.003871367499920517
Iteration: 1665, Checking gradient for hidden W[205][14]:	mock grad = 0.005901985779543928, computed grad = 0.005901985884209429
Iteration: 1665, Checking gradient for hidden b[0][45]:	mock grad = -0.005350149623467582, computed grad = -0.005350150234589586
Iteration: 1665, Checking gradient for output layer W[33][0]:	mock grad = -0.012760105194214999, computed grad = -0.012760105189546624
Iteration: 1666, Checking gradient for words E[776][37]:	mock grad = -0.017364389484325615, computed grad = -0.017364389469994662
Iteration: 1666, Checking gradient for hidden W[118][33]:	mock grad = -0.008352188766241886, computed grad = -0.008352188786010017
Iteration: 1666, Checking gradient for hidden b[0][42]:	mock grad = -0.041838856635556532, computed grad = -0.041838858589422954
Iteration: 1666, Checking gradient for output layer W[77][0]:	mock grad = -0.032682809659928536, computed grad = -0.032682809620630332
Iteration: 1667, Checking gradient for words E[69][28]:	mock grad = -0.013617928518866451, computed grad = -0.013617928561783909
Iteration: 1667, Checking gradient for hidden W[208][10]:	mock grad = 0.002758005099745553, computed grad = 0.002758005073477671
Iteration: 1667, Checking gradient for hidden b[0][43]:	mock grad = 0.009779497577466767, computed grad = 0.009779500444058458
Iteration: 1667, Checking gradient for output layer W[39][1]:	mock grad = -0.034197564093868538, computed grad = -0.034197564061309693
Iteration: 1668, Checking gradient for words E[274][2]:	mock grad = 0.026282552962914041, computed grad = 0.026976886338436406
Iteration: 1668, Checking gradient for hidden W[101][13]:	mock grad = 0.005518970107865062, computed grad = 0.005518970136410645
Iteration: 1668, Checking gradient for hidden b[0][47]:	mock grad = 0.053307918180478531, computed grad = 0.053307929538432962
Iteration: 1668, Checking gradient for output layer W[33][0]:	mock grad = -0.001080842717993136, computed grad = -0.001080842717964529
current: 70, Cost = 0.234367, Correct(%) = 1, time = 4.74257
Iteration: 1669, Checking gradient for words E[311][8]:	mock grad = 0.011576602317861506, computed grad = 0.011576602337369070
Iteration: 1669, Checking gradient for hidden W[140][31]:	mock grad = -0.001142979404719635, computed grad = -0.001142979434235982
Iteration: 1669, Checking gradient for hidden b[0][11]:	mock grad = 0.060841900192523668, computed grad = 0.060841915107857039
Iteration: 1669, Checking gradient for output layer W[23][0]:	mock grad = -0.034187780083183572, computed grad = -0.034187780012868520
Iteration: 1670, Checking gradient for words E[682][0]:	mock grad = 0.000149047914704825, computed grad = 0.000149047960737617
Iteration: 1670, Checking gradient for hidden W[25][5]:	mock grad = -0.002818173781227173, computed grad = -0.002818173863355598
Iteration: 1670, Checking gradient for hidden b[0][44]:	mock grad = -0.005556282860280648, computed grad = -0.005556281441497263
Iteration: 1670, Checking gradient for output layer W[123][1]:	mock grad = 0.153658865629180408, computed grad = 0.153658863705578103
Iteration: 1671, Checking gradient for words E[48][23]:	mock grad = -0.049929024289935287, computed grad = -0.049929024371886357
Iteration: 1671, Checking gradient for hidden W[0][3]:	mock grad = 0.007288765378782314, computed grad = 0.007288765465856268
Iteration: 1671, Checking gradient for hidden b[0][14]:	mock grad = 0.060682590121791113, computed grad = 0.060682613156893980
Iteration: 1671, Checking gradient for output layer W[122][0]:	mock grad = -0.097214664163847164, computed grad = -0.097214663601864731
Iteration: 1672, Checking gradient for words E[60][20]:	mock grad = 0.002061964595484467, computed grad = 0.002061964596158140
Iteration: 1672, Checking gradient for hidden W[117][9]:	mock grad = 0.002944391287951209, computed grad = 0.002944391301326419
Iteration: 1672, Checking gradient for hidden b[0][17]:	mock grad = -0.020242913413515140, computed grad = -0.020242921142367924
Iteration: 1672, Checking gradient for output layer W[68][1]:	mock grad = -0.172962814048049918, computed grad = -0.172962811309140624
Iteration: 1673, Checking gradient for words E[231][34]:	mock grad = 0.036150908785254066, computed grad = 0.036150908797939925
Iteration: 1673, Checking gradient for hidden W[173][18]:	mock grad = 0.016484251076281931, computed grad = 0.016484251411232494
Iteration: 1673, Checking gradient for hidden b[0][24]:	mock grad = -0.090574645783825236, computed grad = -0.090574648916007999
Iteration: 1673, Checking gradient for output layer W[1][1]:	mock grad = 0.000954530879448967, computed grad = 0.000954530879462424
Iteration: 1674, Checking gradient for words E[68][2]:	mock grad = -0.014374007125561583, computed grad = -0.014374007158361308
Iteration: 1674, Checking gradient for hidden W[242][48]:	mock grad = -0.011369703703467682, computed grad = -0.011369703776725051
Iteration: 1674, Checking gradient for hidden b[0][43]:	mock grad = 0.011460341915625616, computed grad = 0.011460345057922122
Iteration: 1674, Checking gradient for output layer W[17][0]:	mock grad = -0.016566495840975026, computed grad = -0.016566495839081919
Iteration: 1675, Checking gradient for words E[341][18]:	mock grad = 0.017872089436127281, computed grad = 0.017872089450584459
Iteration: 1675, Checking gradient for hidden W[113][20]:	mock grad = 0.006449488128890124, computed grad = 0.006449488167942352
Iteration: 1675, Checking gradient for hidden b[0][4]:	mock grad = -0.015681970946840318, computed grad = -0.015681977928765224
Iteration: 1675, Checking gradient for output layer W[108][0]:	mock grad = -0.136826604964335186, computed grad = -0.136826601737600917
Iteration: 1676, Checking gradient for words E[104][40]:	mock grad = -0.018899645108871876, computed grad = -0.018899645119736554
Iteration: 1676, Checking gradient for hidden W[211][17]:	mock grad = 0.002293849603013376, computed grad = 0.002293849631877943
Iteration: 1676, Checking gradient for hidden b[0][27]:	mock grad = 0.027743555568726652, computed grad = 0.027743569670635834
Iteration: 1676, Checking gradient for output layer W[108][1]:	mock grad = -0.090379110382260430, computed grad = -0.090379109766588681
Iteration: 1677, Checking gradient for words E[397][1]:	mock grad = 0.038809833438291363, computed grad = 0.038809833389520085
Iteration: 1677, Checking gradient for hidden W[221][42]:	mock grad = 0.000175517768263855, computed grad = 0.000175517768390386
Iteration: 1677, Checking gradient for hidden b[0][39]:	mock grad = 0.053261710750707181, computed grad = 0.053261724016947544
Iteration: 1677, Checking gradient for output layer W[148][1]:	mock grad = -0.088955714077645909, computed grad = -0.088955712899881162
Iteration: 1678, Checking gradient for words E[553][9]:	mock grad = -0.011842612528734087, computed grad = -0.011842612537867898
Iteration: 1678, Checking gradient for hidden W[228][49]:	mock grad = 0.001753465652371000, computed grad = 0.001753465660068831
Iteration: 1678, Checking gradient for hidden b[0][3]:	mock grad = 0.032645986540874583, computed grad = 0.032645990686352599
Iteration: 1678, Checking gradient for output layer W[20][0]:	mock grad = 0.013197228245553916, computed grad = 0.013197228243635688
current: 80, Cost = 0.257168, Correct(%) = 1, time = 5.22751
Iteration: 1679, Checking gradient for words E[403][48]:	mock grad = 0.024350523287780534, computed grad = 0.024350523342197609
Iteration: 1679, Checking gradient for hidden W[29][41]:	mock grad = -0.000084648232145934, computed grad = -0.000084648210093380
Iteration: 1679, Checking gradient for hidden b[0][10]:	mock grad = 0.064995402563217120, computed grad = 0.064995416341635137
Iteration: 1679, Checking gradient for output layer W[39][0]:	mock grad = -0.020779674895221589, computed grad = -0.020779674882856619
Iteration: 1680, Checking gradient for words E[116][38]:	mock grad = 0.013502834159934141, computed grad = 0.013502834184883756
Iteration: 1680, Checking gradient for hidden W[167][22]:	mock grad = 0.000382057851161521, computed grad = 0.000382057847099850
Iteration: 1680, Checking gradient for hidden b[0][23]:	mock grad = -0.112717970380926680, computed grad = -0.112717995096620352
Iteration: 1680, Checking gradient for output layer W[19][0]:	mock grad = -0.048587598909688445, computed grad = -0.048587598814007239
Iteration: 1681, Checking gradient for words E[716][23]:	mock grad = 0.018305503498572406, computed grad = 0.018305503484823099
Iteration: 1681, Checking gradient for hidden W[5][11]:	mock grad = 0.005304931324057183, computed grad = 0.005304931371573193
Iteration: 1681, Checking gradient for hidden b[0][36]:	mock grad = -0.006407117790901973, computed grad = -0.006407115126931759
Iteration: 1681, Checking gradient for output layer W[95][1]:	mock grad = 0.071936844317915805, computed grad = 0.071936843872955958
Iteration: 1682, Checking gradient for words E[148][12]:	mock grad = 0.000854165250363526, computed grad = 0.000854165291692862
Iteration: 1682, Checking gradient for hidden W[176][42]:	mock grad = 0.004003620800274721, computed grad = 0.004003620806015404
Iteration: 1682, Checking gradient for hidden b[0][34]:	mock grad = -0.016193046390694521, computed grad = -0.016193043547102548
Iteration: 1682, Checking gradient for output layer W[33][1]:	mock grad = 0.009084668616099467, computed grad = 0.009084668615287785
Iteration: 1683, Checking gradient for words E[776][45]:	mock grad = -0.028958670227646754, computed grad = -0.028958670578906102
Iteration: 1683, Checking gradient for hidden W[148][49]:	mock grad = -0.010616348637415562, computed grad = -0.010616348742081392
Iteration: 1683, Checking gradient for hidden b[0][42]:	mock grad = -0.046393199664512830, computed grad = -0.046393199114386025
Iteration: 1683, Checking gradient for output layer W[78][1]:	mock grad = 0.128472021458769792, computed grad = 0.128472020076445786
Iteration: 1684, Checking gradient for words E[732][35]:	mock grad = -0.002943156982265771, computed grad = -0.002943157002758367
Iteration: 1684, Checking gradient for hidden W[54][37]:	mock grad = 0.010834476879523214, computed grad = 0.010834476899621978
Iteration: 1684, Checking gradient for hidden b[0][25]:	mock grad = 0.003323419427242458, computed grad = 0.003323416785278393
Iteration: 1684, Checking gradient for output layer W[73][1]:	mock grad = 0.032404422479065387, computed grad = 0.032404422431777428
Iteration: 1685, Checking gradient for words E[432][32]:	mock grad = -0.013049306505585889, computed grad = -0.013049306521971792
Iteration: 1685, Checking gradient for hidden W[178][15]:	mock grad = -0.013403796075367458, computed grad = -0.013403796227049018
Iteration: 1685, Checking gradient for hidden b[0][22]:	mock grad = -0.029610301164273434, computed grad = -0.029610305492465988
Iteration: 1685, Checking gradient for output layer W[84][1]:	mock grad = -0.095682090230003736, computed grad = -0.095682089017079600
Iteration: 1686, Checking gradient for words E[201][12]:	mock grad = -0.007822146442099109, computed grad = -0.007822146444545312
Iteration: 1686, Checking gradient for hidden W[51][19]:	mock grad = 0.004797330503708563, computed grad = 0.008302949998088303
Iteration: 1686, Checking gradient for hidden b[0][0]:	mock grad = -0.131475035563433096, computed grad = -0.131475062816383548
Iteration: 1686, Checking gradient for output layer W[134][0]:	mock grad = 0.099031434451085643, computed grad = 0.099031433516337128
Iteration: 1687, Checking gradient for words E[201][32]:	mock grad = 0.011416754726545353, computed grad = 0.011416754761790011
Iteration: 1687, Checking gradient for hidden W[183][1]:	mock grad = 0.005943801775182278, computed grad = 0.005943801767588491
Iteration: 1687, Checking gradient for hidden b[0][18]:	mock grad = 0.061496594740417798, computed grad = 0.061496610541765410
Iteration: 1687, Checking gradient for output layer W[88][1]:	mock grad = 0.077069923881734059, computed grad = 0.077069923471558194
Iteration: 1688, Checking gradient for words E[112][2]:	mock grad = -0.014940997102114162, computed grad = -0.014940997190642041
Iteration: 1688, Checking gradient for hidden W[211][16]:	mock grad = 0.001104017131131618, computed grad = 0.001104017131297385
Iteration: 1688, Checking gradient for hidden b[0][21]:	mock grad = 0.041655722906069892, computed grad = 0.041655731628797481
Iteration: 1688, Checking gradient for output layer W[44][0]:	mock grad = 0.014404984592208958, computed grad = 0.014404984589189233
current: 90, Cost = 0.296479, Correct(%) = 1, time = 6.17468
Iteration: 1689, Checking gradient for words E[776][21]:	mock grad = 0.006214153880690354, computed grad = 0.006214153885368357
Iteration: 1689, Checking gradient for hidden W[118][36]:	mock grad = -0.004509664861374052, computed grad = -0.004509664869492003
Iteration: 1689, Checking gradient for hidden b[0][19]:	mock grad = 0.032701155482012245, computed grad = 0.032701153183261479
Iteration: 1689, Checking gradient for output layer W[68][0]:	mock grad = 0.183113496983106083, computed grad = 0.183113491356447389
Iteration: 1690, Checking gradient for words E[305][43]:	mock grad = -0.006094115806209999, computed grad = -0.006094115810608773
Iteration: 1690, Checking gradient for hidden W[89][13]:	mock grad = 0.006702430379118551, computed grad = 0.006702430407125017
Iteration: 1690, Checking gradient for hidden b[0][3]:	mock grad = -0.029403571333297407, computed grad = -0.029403579904794145
Iteration: 1690, Checking gradient for output layer W[28][0]:	mock grad = -0.049781696003059484, computed grad = -0.049781695840140076
Iteration: 1691, Checking gradient for words E[32][36]:	mock grad = -0.013173037165858936, computed grad = -0.008570203188466274
Iteration: 1691, Checking gradient for hidden W[196][27]:	mock grad = -0.001373627037587477, computed grad = -0.003625491864920932
Iteration: 1691, Checking gradient for hidden b[0][3]:	mock grad = -0.037575621319174690, computed grad = -0.037575634243871683
Iteration: 1691, Checking gradient for output layer W[21][0]:	mock grad = 0.001366966217164478, computed grad = 0.001366966217099672
Iteration: 1692, Checking gradient for words E[755][7]:	mock grad = 0.016323005913684852, computed grad = 0.016323005910157347
Iteration: 1692, Checking gradient for hidden W[103][5]:	mock grad = 0.003712333176739135, computed grad = 0.003712333228116448
Iteration: 1692, Checking gradient for hidden b[0][9]:	mock grad = -0.029385628322678436, computed grad = -0.029385625208101854
Iteration: 1692, Checking gradient for output layer W[28][0]:	mock grad = -0.039895649669288713, computed grad = -0.039895649544858476
Iteration: 1693, Checking gradient for words E[756][5]:	mock grad = 0.006488051164404984, computed grad = 0.006569614583835029
Iteration: 1693, Checking gradient for hidden W[115][35]:	mock grad = -0.001030860844547510, computed grad = -0.001030860853960859
Iteration: 1693, Checking gradient for hidden b[0][39]:	mock grad = -0.070357732228742975, computed grad = -0.070357760787883333
Iteration: 1693, Checking gradient for output layer W[84][0]:	mock grad = 0.095420167743026640, computed grad = 0.095420166956837957
Iteration: 1694, Checking gradient for words E[257][35]:	mock grad = -0.004880560378742183, computed grad = -0.004880560377171394
Iteration: 1694, Checking gradient for hidden W[97][18]:	mock grad = -0.008254019779996291, computed grad = -0.008254019774992544
Iteration: 1694, Checking gradient for hidden b[0][16]:	mock grad = -0.014622572412900658, computed grad = -0.014622579653713783
Iteration: 1694, Checking gradient for output layer W[124][0]:	mock grad = -0.033681859600828701, computed grad = -0.033681859529512602
Iteration: 1695, Checking gradient for words E[472][39]:	mock grad = 0.007790899255954997, computed grad = 0.007790899279560590
Iteration: 1695, Checking gradient for hidden W[125][41]:	mock grad = -0.004856865927799214, computed grad = -0.004856866089968786
Iteration: 1695, Checking gradient for hidden b[0][1]:	mock grad = 0.095201246178427956, computed grad = 0.095201282685312430
Iteration: 1695, Checking gradient for output layer W[89][0]:	mock grad = 0.076452237393082045, computed grad = 0.076452237094972939
Iteration: 1696, Checking gradient for words E[161][25]:	mock grad = -0.009413778007949514, computed grad = -0.009413777977268414
Iteration: 1696, Checking gradient for hidden W[127][20]:	mock grad = -0.000684637727205173, computed grad = -0.000684637717886939
Iteration: 1696, Checking gradient for hidden b[0][12]:	mock grad = 0.055680905782812262, computed grad = 0.055680923791928877
Iteration: 1696, Checking gradient for output layer W[47][1]:	mock grad = 0.035259397476505860, computed grad = 0.035259397424100246
Iteration: 1697, Checking gradient for words E[411][47]:	mock grad = -0.003602189071805206, computed grad = -0.003602189116556107
Iteration: 1697, Checking gradient for hidden W[31][23]:	mock grad = -0.007325741010866249, computed grad = -0.007325741075262689
Iteration: 1697, Checking gradient for hidden b[0][34]:	mock grad = 0.031162716336574636, computed grad = 0.031162724042333338
Iteration: 1697, Checking gradient for output layer W[115][0]:	mock grad = -0.087863872204385629, computed grad = -0.087863871986764791
Iteration: 1698, Checking gradient for words E[279][39]:	mock grad = 0.016168812164679691, computed grad = 0.016154875361288006
Iteration: 1698, Checking gradient for hidden W[184][41]:	mock grad = 0.000836071882698297, computed grad = 0.000836071899201821
Iteration: 1698, Checking gradient for hidden b[0][16]:	mock grad = -0.013839918310637467, computed grad = -0.013839923441486849
Iteration: 1698, Checking gradient for output layer W[112][0]:	mock grad = -0.018981565224451824, computed grad = -0.018981565215249206
current: 100, Cost = 0.299235, Correct(%) = 1, time = 6.95707
Iteration: 1699, Checking gradient for words E[118][3]:	mock grad = 0.012561654420123469, computed grad = 0.012561654482373997
Iteration: 1699, Checking gradient for hidden W[141][27]:	mock grad = -0.002225864620952756, computed grad = -0.002225864629493663
Iteration: 1699, Checking gradient for hidden b[0][35]:	mock grad = 0.011667664502723607, computed grad = 0.011667665303579584
Iteration: 1699, Checking gradient for output layer W[121][0]:	mock grad = 0.066718145108557447, computed grad = 0.066718144843709470
current: 17, Correct(%) = 1, time = 7.00817
Dev start.
Dev finished. Total time taken is: 0.635615
dev:
Accuracy:	P=100/100=1
Test start.
Test finished. Total time taken is: 0.545135
test:
Accuracy:	P=64/100=0.64
##### Iteration 17
random: 0, 99
Iteration: 1700, Checking gradient for words E[3][8]:	mock grad = -0.008773620578952923, computed grad = -0.008773620624656470
Iteration: 1700, Checking gradient for hidden W[53][30]:	mock grad = -0.001887143611586928, computed grad = -0.001887143619518670
Iteration: 1700, Checking gradient for hidden b[0][43]:	mock grad = -0.006922940475287409, computed grad = -0.006922942426512970
Iteration: 1700, Checking gradient for output layer W[127][1]:	mock grad = -0.054631390007858771, computed grad = -0.054631389633379671
Iteration: 1701, Checking gradient for words E[98][13]:	mock grad = 0.013388054057844601, computed grad = 0.013388054088800134
Iteration: 1701, Checking gradient for hidden W[120][49]:	mock grad = -0.001785254651165413, computed grad = -0.001785254646368094
Iteration: 1701, Checking gradient for hidden b[0][36]:	mock grad = -0.007182784244885720, computed grad = -0.007182782326691015
Iteration: 1701, Checking gradient for output layer W[19][0]:	mock grad = 0.041276317785610139, computed grad = 0.041276317706394006
Iteration: 1702, Checking gradient for words E[30][14]:	mock grad = -0.029903859660146415, computed grad = -0.029903859613262508
Iteration: 1702, Checking gradient for hidden W[239][21]:	mock grad = 0.006000932933125647, computed grad = 0.006000933038097318
Iteration: 1702, Checking gradient for hidden b[0][44]:	mock grad = 0.007855105849746691, computed grad = 0.007855115818053846
Iteration: 1702, Checking gradient for output layer W[121][0]:	mock grad = -0.087859389848926606, computed grad = -0.087859388906184083
Iteration: 1703, Checking gradient for words E[139][38]:	mock grad = -0.012026701423778485, computed grad = -0.012026701487762786
Iteration: 1703, Checking gradient for hidden W[224][39]:	mock grad = -0.004424807809738640, computed grad = -0.004424807832271407
Iteration: 1703, Checking gradient for hidden b[0][22]:	mock grad = -0.036468175730719476, computed grad = -0.036468182086697387
Iteration: 1703, Checking gradient for output layer W[144][1]:	mock grad = 0.106111040567069548, computed grad = 0.106111039704152033
Iteration: 1704, Checking gradient for words E[147][2]:	mock grad = -0.009975993410848982, computed grad = -0.009975993397736466
Iteration: 1704, Checking gradient for hidden W[0][8]:	mock grad = 0.009140009130881310, computed grad = 0.009140009119996340
Iteration: 1704, Checking gradient for hidden b[0][23]:	mock grad = -0.112026456434571875, computed grad = -0.112026485293719377
Iteration: 1704, Checking gradient for output layer W[71][0]:	mock grad = 0.051250841664435276, computed grad = 0.051250841541927862
Iteration: 1705, Checking gradient for words E[184][0]:	mock grad = 0.000172619534777319, computed grad = 0.000172619543105538
Iteration: 1705, Checking gradient for hidden W[192][10]:	mock grad = -0.000660072195457495, computed grad = -0.000660072192626881
Iteration: 1705, Checking gradient for hidden b[0][1]:	mock grad = 0.091788455226449184, computed grad = 0.091788495486860716
Iteration: 1705, Checking gradient for output layer W[39][1]:	mock grad = -0.034040831303788766, computed grad = -0.034040831268031056
Iteration: 1706, Checking gradient for words E[260][26]:	mock grad = 0.001189068193699350, computed grad = 0.001189068136225168
Iteration: 1706, Checking gradient for hidden W[170][0]:	mock grad = -0.005645151788136182, computed grad = -0.005645151803255746
Iteration: 1706, Checking gradient for hidden b[0][9]:	mock grad = 0.035280602633344649, computed grad = 0.035280609331344331
Iteration: 1706, Checking gradient for output layer W[116][0]:	mock grad = -0.066377918533824798, computed grad = -0.066377917949586085
Iteration: 1707, Checking gradient for words E[69][48]:	mock grad = -0.002740024837050159, computed grad = -0.002740024895207418
Iteration: 1707, Checking gradient for hidden W[38][1]:	mock grad = -0.025710487779606606, computed grad = -0.025710487968538975
Iteration: 1707, Checking gradient for hidden b[0][19]:	mock grad = -0.061224558461164813, computed grad = -0.061224577455950342
Iteration: 1707, Checking gradient for output layer W[63][0]:	mock grad = -0.080141202244504717, computed grad = -0.080141202113934801
Iteration: 1708, Checking gradient for words E[284][3]:	mock grad = 0.004976329035621951, computed grad = 0.004976329040650090
Iteration: 1708, Checking gradient for hidden W[121][6]:	mock grad = -0.003796629280294894, computed grad = -0.003796629327330091
Iteration: 1708, Checking gradient for hidden b[0][0]:	mock grad = 0.125508010231284439, computed grad = 0.125508044975035343
Iteration: 1708, Checking gradient for output layer W[130][0]:	mock grad = -0.060870994621209373, computed grad = -0.060870994351681793
current: 10, Cost = 0.307245, Correct(%) = 1, time = 0.77374
Iteration: 1709, Checking gradient for words E[25][15]:	mock grad = 0.019902577009622835, computed grad = 0.019902577014822759
Iteration: 1709, Checking gradient for hidden W[95][13]:	mock grad = -0.007278104059010859, computed grad = -0.007278104092321878
Iteration: 1709, Checking gradient for hidden b[0][29]:	mock grad = -0.064724858995945933, computed grad = -0.064724870982860047
Iteration: 1709, Checking gradient for output layer W[95][1]:	mock grad = 0.049263137170074689, computed grad = 0.049263137071403486
Iteration: 1710, Checking gradient for words E[267][40]:	mock grad = 0.000209122863428779, computed grad = -0.000714680899180906
Iteration: 1710, Checking gradient for hidden W[148][15]:	mock grad = -0.000013643462734581, computed grad = -0.000013643461951157
Iteration: 1710, Checking gradient for hidden b[0][44]:	mock grad = -0.001726571949078881, computed grad = -0.001726569361693829
Iteration: 1710, Checking gradient for output layer W[128][1]:	mock grad = 0.051352900560561188, computed grad = 0.051352900337727860
Iteration: 1711, Checking gradient for words E[73][26]:	mock grad = 0.011885712010967220, computed grad = 0.011885712009996420
Iteration: 1711, Checking gradient for hidden W[68][13]:	mock grad = 0.002477192941441331, computed grad = 0.002477192929848812
Iteration: 1711, Checking gradient for hidden b[0][43]:	mock grad = -0.010954774817079072, computed grad = -0.010954777735612133
Iteration: 1711, Checking gradient for output layer W[97][1]:	mock grad = 0.124226324575621438, computed grad = 0.124226323703276562
Iteration: 1712, Checking gradient for words E[359][1]:	mock grad = 0.002070156179312388, computed grad = 0.002070156197215907
Iteration: 1712, Checking gradient for hidden W[152][45]:	mock grad = -0.002251344268178190, computed grad = -0.002251344294891513
Iteration: 1712, Checking gradient for hidden b[0][29]:	mock grad = 0.060000860199055372, computed grad = 0.060000881118240722
Iteration: 1712, Checking gradient for output layer W[5][0]:	mock grad = 0.027337615151834527, computed grad = 0.027337615123136910
Iteration: 1713, Checking gradient for words E[309][32]:	mock grad = -0.007870248594402107, computed grad = -0.005387573451180624
Iteration: 1713, Checking gradient for hidden W[211][34]:	mock grad = -0.000921387216482050, computed grad = -0.000921387214828611
Iteration: 1713, Checking gradient for hidden b[0][6]:	mock grad = 0.026219882955008655, computed grad = 0.026219891619897163
Iteration: 1713, Checking gradient for output layer W[118][0]:	mock grad = -0.016854034612850954, computed grad = -0.016854034605260155
Iteration: 1714, Checking gradient for words E[102][37]:	mock grad = 0.011960175734682110, computed grad = 0.011960175724568338
Iteration: 1714, Checking gradient for hidden W[76][2]:	mock grad = 0.009311871311395992, computed grad = 0.009311871284183981
Iteration: 1714, Checking gradient for hidden b[0][10]:	mock grad = -0.106074669915134923, computed grad = -0.106074702368459950
Iteration: 1714, Checking gradient for output layer W[87][1]:	mock grad = -0.034943912851526093, computed grad = -0.034943912844092602
Iteration: 1715, Checking gradient for words E[378][41]:	mock grad = 0.025158291950044820, computed grad = 0.025158291972077959
Iteration: 1715, Checking gradient for hidden W[147][49]:	mock grad = 0.005930500237560921, computed grad = 0.005930500276211156
Iteration: 1715, Checking gradient for hidden b[0][8]:	mock grad = 0.019142248955023033, computed grad = 0.019142257253543037
Iteration: 1715, Checking gradient for output layer W[15][1]:	mock grad = 0.082656084399002072, computed grad = 0.082656083878413319
Iteration: 1716, Checking gradient for words E[197][27]:	mock grad = 0.041639347495320278, computed grad = 0.041639347483459321
Iteration: 1716, Checking gradient for hidden W[218][22]:	mock grad = -0.002834468854712169, computed grad = -0.002834468853047323
Iteration: 1716, Checking gradient for hidden b[0][48]:	mock grad = -0.074101536108095445, computed grad = -0.074101551236828836
Iteration: 1716, Checking gradient for output layer W[18][1]:	mock grad = -0.076802415984039696, computed grad = -0.076802415661441095
Iteration: 1717, Checking gradient for words E[401][37]:	mock grad = 0.002344016123728698, computed grad = 0.002344016146437246
Iteration: 1717, Checking gradient for hidden W[103][38]:	mock grad = 0.000391771905805527, computed grad = 0.000391771905786640
Iteration: 1717, Checking gradient for hidden b[0][24]:	mock grad = 0.057300044578170728, computed grad = 0.057300060847684277
Iteration: 1717, Checking gradient for output layer W[135][1]:	mock grad = 0.058034844478721537, computed grad = 0.058034844032015628
Iteration: 1718, Checking gradient for words E[305][9]:	mock grad = -0.014266114925928775, computed grad = -0.014266114944378816
Iteration: 1718, Checking gradient for hidden W[42][38]:	mock grad = 0.000807228852151010, computed grad = 0.000807228854338423
Iteration: 1718, Checking gradient for hidden b[0][16]:	mock grad = -0.012301107632700647, computed grad = -0.012301111483744638
Iteration: 1718, Checking gradient for output layer W[9][1]:	mock grad = -0.026037165132150597, computed grad = -0.026037165104489408
current: 20, Cost = 0.414121, Correct(%) = 1, time = 1.43314
Iteration: 1719, Checking gradient for words E[42][47]:	mock grad = 0.035583212293910371, computed grad = 0.035583212385275780
Iteration: 1719, Checking gradient for hidden W[76][7]:	mock grad = 0.001018441869921061, computed grad = 0.001018441870226106
Iteration: 1719, Checking gradient for hidden b[0][38]:	mock grad = 0.067419286623709995, computed grad = 0.067419299221815110
Iteration: 1719, Checking gradient for output layer W[24][0]:	mock grad = -0.064358709855494300, computed grad = -0.064358709773284256
Iteration: 1720, Checking gradient for words E[68][43]:	mock grad = -0.030265967841586194, computed grad = -0.029304112312061183
Iteration: 1720, Checking gradient for hidden W[243][37]:	mock grad = 0.006541680278804307, computed grad = 0.006541680341809407
Iteration: 1720, Checking gradient for hidden b[0][36]:	mock grad = 0.005856510807644710, computed grad = 0.005856506807782695
Iteration: 1720, Checking gradient for output layer W[55][1]:	mock grad = -0.089966565870402970, computed grad = -0.089966565119550923
Iteration: 1721, Checking gradient for words E[284][0]:	mock grad = -0.017516044585719204, computed grad = -0.017516044588478306
Iteration: 1721, Checking gradient for hidden W[136][32]:	mock grad = -0.003615082319025076, computed grad = -0.003615082321225924
Iteration: 1721, Checking gradient for hidden b[0][24]:	mock grad = 0.114312442199865361, computed grad = 0.114312475634225710
Iteration: 1721, Checking gradient for output layer W[107][0]:	mock grad = 0.051336818909369919, computed grad = 0.051336818879413777
Iteration: 1722, Checking gradient for words E[225][24]:	mock grad = 0.003560217110093467, computed grad = 0.003560217117702863
Iteration: 1722, Checking gradient for hidden W[235][9]:	mock grad = -0.007729214725332723, computed grad = -0.007729214771285181
Iteration: 1722, Checking gradient for hidden b[0][12]:	mock grad = -0.051283328601658962, computed grad = -0.051283344325300004
Iteration: 1722, Checking gradient for output layer W[80][1]:	mock grad = -0.080194936573158015, computed grad = -0.080194935812560500
Iteration: 1723, Checking gradient for words E[421][46]:	mock grad = 0.020811203493287245, computed grad = 0.020811203497369240
Iteration: 1723, Checking gradient for hidden W[45][22]:	mock grad = 0.005633946836697268, computed grad = 0.005633946858086694
Iteration: 1723, Checking gradient for hidden b[0][49]:	mock grad = -0.028037224808652450, computed grad = -0.028037223945058722
Iteration: 1723, Checking gradient for output layer W[75][1]:	mock grad = -0.135306898694148581, computed grad = -0.135306896207227023
Iteration: 1724, Checking gradient for words E[433][29]:	mock grad = 0.002192782345922240, computed grad = 0.002192782335289171
Iteration: 1724, Checking gradient for hidden W[64][44]:	mock grad = 0.014643970838862375, computed grad = 0.014643971042630415
Iteration: 1724, Checking gradient for hidden b[0][39]:	mock grad = 0.065010626238565239, computed grad = 0.065010638953957742
Iteration: 1724, Checking gradient for output layer W[124][1]:	mock grad = -0.063332605171440015, computed grad = -0.063332604968240863
Iteration: 1725, Checking gradient for words E[51][6]:	mock grad = -0.039457121145641816, computed grad = -0.039457121127928499
Iteration: 1725, Checking gradient for hidden W[103][30]:	mock grad = 0.008571255474876249, computed grad = 0.008571255533404000
Iteration: 1725, Checking gradient for hidden b[0][37]:	mock grad = 0.043541065465502182, computed grad = 0.043541072066706640
Iteration: 1725, Checking gradient for output layer W[3][0]:	mock grad = 0.019286726462924841, computed grad = 0.019286726457398373
Iteration: 1726, Checking gradient for words E[54][10]:	mock grad = -0.006363805829856517, computed grad = -0.006363805865101165
Iteration: 1726, Checking gradient for hidden W[23][9]:	mock grad = -0.010419500544242566, computed grad = -0.010419500708204405
Iteration: 1726, Checking gradient for hidden b[0][33]:	mock grad = 0.059451929593584696, computed grad = 0.059451941527673205
Iteration: 1726, Checking gradient for output layer W[60][0]:	mock grad = 0.040843558904976596, computed grad = 0.040843558760066762
Iteration: 1727, Checking gradient for words E[487][20]:	mock grad = 0.000263320785426346, computed grad = -0.000942453959798585
Iteration: 1727, Checking gradient for hidden W[189][44]:	mock grad = 0.003881651878756553, computed grad = 0.003881651862803024
Iteration: 1727, Checking gradient for hidden b[0][33]:	mock grad = -0.110731929212432023, computed grad = -0.110731952098417727
Iteration: 1727, Checking gradient for output layer W[126][1]:	mock grad = -0.091588666037145661, computed grad = -0.091588665794995000
Iteration: 1728, Checking gradient for words E[175][14]:	mock grad = 0.002986380601199823, computed grad = 0.002986380597960499
Iteration: 1728, Checking gradient for hidden W[151][31]:	mock grad = 0.007067216636583162, computed grad = 0.007067216648233670
Iteration: 1728, Checking gradient for hidden b[0][34]:	mock grad = -0.001548434413176092, computed grad = -0.001548426357275221
Iteration: 1728, Checking gradient for output layer W[16][1]:	mock grad = -0.017592177712175139, computed grad = -0.017592177702450851
current: 30, Cost = 0.284871, Correct(%) = 1, time = 2.12358
Iteration: 1729, Checking gradient for words E[25][45]:	mock grad = -0.000730140694210624, computed grad = -0.000730140633660046
Iteration: 1729, Checking gradient for hidden W[27][6]:	mock grad = -0.001900001367988180, computed grad = -0.001900001392810122
Iteration: 1729, Checking gradient for hidden b[0][42]:	mock grad = 0.053408875944044309, computed grad = 0.053408892948324654
Iteration: 1729, Checking gradient for output layer W[81][1]:	mock grad = -0.006560672077188245, computed grad = -0.006560672076984471
Iteration: 1730, Checking gradient for words E[102][0]:	mock grad = 0.011366699757181520, computed grad = 0.011366699734588460
Iteration: 1730, Checking gradient for hidden W[116][38]:	mock grad = 0.000681988262779543, computed grad = 0.000681988264492238
Iteration: 1730, Checking gradient for hidden b[0][23]:	mock grad = 0.101940442135961540, computed grad = 0.101940459849264531
Iteration: 1730, Checking gradient for output layer W[145][1]:	mock grad = -0.076736083487238815, computed grad = -0.076736082926375362
Iteration: 1731, Checking gradient for words E[409][39]:	mock grad = -0.016328587656877502, computed grad = -0.016328587721494862
Iteration: 1731, Checking gradient for hidden W[110][46]:	mock grad = 0.005984304410344699, computed grad = 0.005984304437091798
Iteration: 1731, Checking gradient for hidden b[0][24]:	mock grad = -0.092082250400749022, computed grad = -0.092082256349764566
Iteration: 1731, Checking gradient for output layer W[125][0]:	mock grad = -0.006954607603082419, computed grad = -0.006954607602975172
Iteration: 1732, Checking gradient for words E[102][19]:	mock grad = 0.006212797390270364, computed grad = 0.006212797456636615
Iteration: 1732, Checking gradient for hidden W[112][18]:	mock grad = 0.001883305572603078, computed grad = 0.001883305656823124
Iteration: 1732, Checking gradient for hidden b[0][6]:	mock grad = 0.029128220443458419, computed grad = 0.029128229874199822
Iteration: 1732, Checking gradient for output layer W[65][0]:	mock grad = -0.123659279170157399, computed grad = -0.123659277052400207
Iteration: 1733, Checking gradient for words E[347][41]:	mock grad = -0.018159186559937868, computed grad = -0.018159186637312121
Iteration: 1733, Checking gradient for hidden W[175][7]:	mock grad = -0.005709082851468317, computed grad = -0.005709082972524752
Iteration: 1733, Checking gradient for hidden b[0][42]:	mock grad = 0.044877760248182574, computed grad = 0.044877773729072708
Iteration: 1733, Checking gradient for output layer W[48][1]:	mock grad = 0.064329594164372339, computed grad = 0.064329593757497999
Iteration: 1734, Checking gradient for words E[48][10]:	mock grad = -0.026815072066699308, computed grad = -0.026815072037062425
Iteration: 1734, Checking gradient for hidden W[219][1]:	mock grad = -0.004532166001214311, computed grad = -0.004532166072135838
Iteration: 1734, Checking gradient for hidden b[0][4]:	mock grad = 0.015594012444164473, computed grad = 0.015594014796834440
Iteration: 1734, Checking gradient for output layer W[62][0]:	mock grad = -0.116214801241409926, computed grad = -0.116214800430208473
Iteration: 1735, Checking gradient for words E[201][7]:	mock grad = 0.008771888943048811, computed grad = 0.008771888914885805
Iteration: 1735, Checking gradient for hidden W[49][5]:	mock grad = -0.018924685877913516, computed grad = -0.018924686216882161
Iteration: 1735, Checking gradient for hidden b[0][30]:	mock grad = -0.000534949448988131, computed grad = -0.000534947001448259
Iteration: 1735, Checking gradient for output layer W[7][0]:	mock grad = -0.031121757777280434, computed grad = -0.031121757760654942
Iteration: 1736, Checking gradient for words E[48][31]:	mock grad = 0.027283420170026718, computed grad = 0.028017294289724094
Iteration: 1736, Checking gradient for hidden W[149][7]:	mock grad = 0.004263807348608983, computed grad = 0.004263807365423844
Iteration: 1736, Checking gradient for hidden b[0][22]:	mock grad = 0.053260393279047635, computed grad = 0.053260409510824537
Iteration: 1736, Checking gradient for output layer W[103][0]:	mock grad = 0.060993078297655323, computed grad = 0.060993078226878793
Iteration: 1737, Checking gradient for words E[535][11]:	mock grad = -0.005437512848593951, computed grad = -0.005437512881645662
Iteration: 1737, Checking gradient for hidden W[167][36]:	mock grad = 0.007124842330563652, computed grad = 0.007124842375782744
Iteration: 1737, Checking gradient for hidden b[0][4]:	mock grad = -0.016390182995956737, computed grad = -0.016390189675050841
Iteration: 1737, Checking gradient for output layer W[27][0]:	mock grad = -0.044886269217392361, computed grad = -0.044886269133187835
Iteration: 1738, Checking gradient for words E[48][40]:	mock grad = 0.042877132883728519, computed grad = 0.042877133187092374
Iteration: 1738, Checking gradient for hidden W[223][4]:	mock grad = 0.006318168954161374, computed grad = 0.006318169050310080
Iteration: 1738, Checking gradient for hidden b[0][41]:	mock grad = -0.034563969558615293, computed grad = -0.034563977743643316
Iteration: 1738, Checking gradient for output layer W[57][1]:	mock grad = 0.090653831230497195, computed grad = 0.090653830829449097
current: 40, Cost = 0.224072, Correct(%) = 1, time = 2.55967
Iteration: 1739, Checking gradient for words E[102][25]:	mock grad = -0.004960227753092461, computed grad = -0.004960227760764022
Iteration: 1739, Checking gradient for hidden W[231][29]:	mock grad = 0.004246726761927833, computed grad = 0.004246726773418884
Iteration: 1739, Checking gradient for hidden b[0][1]:	mock grad = 0.068601940062931743, computed grad = 0.068601965969923628
Iteration: 1739, Checking gradient for output layer W[86][0]:	mock grad = 0.055094961489199346, computed grad = 0.055094961158374402
Iteration: 1740, Checking gradient for words E[102][48]:	mock grad = 0.019859361392940422, computed grad = 0.019859361437777213
Iteration: 1740, Checking gradient for hidden W[129][45]:	mock grad = -0.002133355145358951, computed grad = -0.002133355168047648
Iteration: 1740, Checking gradient for hidden b[0][18]:	mock grad = -0.041565607322616160, computed grad = -0.041565600644005808
Iteration: 1740, Checking gradient for output layer W[104][0]:	mock grad = -0.053006993840643002, computed grad = -0.053006993568195340
Iteration: 1741, Checking gradient for words E[225][10]:	mock grad = 0.028339641242464131, computed grad = 0.028339641271195086
Iteration: 1741, Checking gradient for hidden W[208][4]:	mock grad = -0.007100497273787587, computed grad = -0.007100497417625925
Iteration: 1741, Checking gradient for hidden b[0][23]:	mock grad = 0.152420962357402434, computed grad = 0.152420993555037937
Iteration: 1741, Checking gradient for output layer W[113][1]:	mock grad = -0.164317664022722854, computed grad = -0.164317662625283217
Iteration: 1742, Checking gradient for words E[34][37]:	mock grad = -0.067756883290692738, computed grad = -0.067756883173644200
Iteration: 1742, Checking gradient for hidden W[64][1]:	mock grad = 0.011847926491309613, computed grad = 0.011847926487893099
Iteration: 1742, Checking gradient for hidden b[0][34]:	mock grad = -0.030853169621453125, computed grad = -0.030853177405234643
Iteration: 1742, Checking gradient for output layer W[52][1]:	mock grad = -0.004989644872832599, computed grad = -0.004989644872769306
Iteration: 1743, Checking gradient for words E[10][9]:	mock grad = -0.024099349804490577, computed grad = -0.024099349844783513
Iteration: 1743, Checking gradient for hidden W[247][29]:	mock grad = 0.013557327583846313, computed grad = 0.013557327633014675
Iteration: 1743, Checking gradient for hidden b[0][23]:	mock grad = 0.152086220004427064, computed grad = 0.152086256545604676
Iteration: 1743, Checking gradient for output layer W[145][1]:	mock grad = -0.121850141084400843, computed grad = -0.121850140504353638
Iteration: 1744, Checking gradient for words E[571][32]:	mock grad = -0.005277140408141423, computed grad = -0.005807961196533994
Iteration: 1744, Checking gradient for hidden W[116][15]:	mock grad = -0.004486851774218592, computed grad = -0.004486851779502986
Iteration: 1744, Checking gradient for hidden b[0][6]:	mock grad = -0.022648911215297773, computed grad = -0.022648919194856482
Iteration: 1744, Checking gradient for output layer W[63][0]:	mock grad = 0.054857119178533531, computed grad = 0.054857118716168628
Iteration: 1745, Checking gradient for words E[519][12]:	mock grad = -0.017082954482300350, computed grad = -0.017082954395410275
Iteration: 1745, Checking gradient for hidden W[100][21]:	mock grad = -0.005894089361191401, computed grad = -0.005894089400583911
Iteration: 1745, Checking gradient for hidden b[0][11]:	mock grad = -0.085374625441780871, computed grad = -0.085374651184338485
Iteration: 1745, Checking gradient for output layer W[25][1]:	mock grad = 0.063780911156052777, computed grad = 0.063780910990743872
Iteration: 1746, Checking gradient for words E[267][14]:	mock grad = -0.002256383582963095, computed grad = -0.002256383585540703
Iteration: 1746, Checking gradient for hidden W[32][9]:	mock grad = -0.003462403707854556, computed grad = -0.003462403707438978
Iteration: 1746, Checking gradient for hidden b[0][45]:	mock grad = -0.005375789110625973, computed grad = -0.005375789435602552
Iteration: 1746, Checking gradient for output layer W[75][1]:	mock grad = 0.126121925423341996, computed grad = 0.126121921634268136
Iteration: 1747, Checking gradient for words E[102][15]:	mock grad = 0.014932061291300647, computed grad = 0.014932061265607473
Iteration: 1747, Checking gradient for hidden W[48][3]:	mock grad = 0.007360488209956495, computed grad = 0.007360488238661585
Iteration: 1747, Checking gradient for hidden b[0][26]:	mock grad = 0.023353499240430509, computed grad = 0.023353504994116155
Iteration: 1747, Checking gradient for output layer W[62][0]:	mock grad = 0.121924895511854592, computed grad = 0.121924893102501264
Iteration: 1748, Checking gradient for words E[54][7]:	mock grad = -0.001337025596667707, computed grad = -0.001337025604308774
Iteration: 1748, Checking gradient for hidden W[201][22]:	mock grad = 0.003279891094568055, computed grad = 0.003279891104711413
Iteration: 1748, Checking gradient for hidden b[0][18]:	mock grad = 0.045595549815743985, computed grad = 0.045595554451183892
Iteration: 1748, Checking gradient for output layer W[102][1]:	mock grad = -0.094676116031283675, computed grad = -0.094676114717362911
current: 50, Cost = 0.222388, Correct(%) = 1, time = 3.44675
Iteration: 1749, Checking gradient for words E[613][15]:	mock grad = -0.014205835667704947, computed grad = -0.014205835697799254
Iteration: 1749, Checking gradient for hidden W[34][4]:	mock grad = -0.000420297161699001, computed grad = -0.000420297164339107
Iteration: 1749, Checking gradient for hidden b[0][40]:	mock grad = 0.066052453178921944, computed grad = 0.066052470632166613
Iteration: 1749, Checking gradient for output layer W[141][1]:	mock grad = -0.073213045518050190, computed grad = -0.073213044726166576
Iteration: 1750, Checking gradient for words E[304][27]:	mock grad = 0.022909418153360672, computed grad = 0.023054480235201444
Iteration: 1750, Checking gradient for hidden W[59][14]:	mock grad = -0.021874898454649072, computed grad = -0.021874898763152878
Iteration: 1750, Checking gradient for hidden b[0][42]:	mock grad = -0.061264459350368616, computed grad = -0.061264468609968495
Iteration: 1750, Checking gradient for output layer W[120][0]:	mock grad = 0.106081161460408468, computed grad = 0.106081160993568416
Iteration: 1751, Checking gradient for words E[621][38]:	mock grad = 0.000026079182091632, computed grad = 0.000026079153468086
Iteration: 1751, Checking gradient for hidden W[134][23]:	mock grad = 0.008890353748652879, computed grad = 0.008890353900494374
Iteration: 1751, Checking gradient for hidden b[0][46]:	mock grad = -0.115921056954126911, computed grad = -0.115921080318528144
Iteration: 1751, Checking gradient for output layer W[87][1]:	mock grad = -0.019842304370215613, computed grad = -0.019842304363506039
Iteration: 1752, Checking gradient for words E[107][19]:	mock grad = -0.000877245801417237, computed grad = -0.000877245732327448
Iteration: 1752, Checking gradient for hidden W[244][22]:	mock grad = -0.001027578384993344, computed grad = -0.001027578386971565
Iteration: 1752, Checking gradient for hidden b[0][48]:	mock grad = -0.076492508544745430, computed grad = -0.076492526584811843
Iteration: 1752, Checking gradient for output layer W[17][0]:	mock grad = -0.026670058209055103, computed grad = -0.026670058194680008
Iteration: 1753, Checking gradient for words E[625][16]:	mock grad = -0.012181620264256621, computed grad = -0.012181620251348687
Iteration: 1753, Checking gradient for hidden W[238][18]:	mock grad = -0.006461255960293899, computed grad = -0.006461255966189463
Iteration: 1753, Checking gradient for hidden b[0][0]:	mock grad = -0.132479922022837870, computed grad = -0.132479959633634570
Iteration: 1753, Checking gradient for output layer W[133][0]:	mock grad = 0.002636445592840708, computed grad = 0.002636445592808883
Iteration: 1754, Checking gradient for words E[240][31]:	mock grad = 0.006529576635619372, computed grad = 0.005172062282795627
Iteration: 1754, Checking gradient for hidden W[41][6]:	mock grad = 0.000511131868205505, computed grad = 0.000511131864828388
Iteration: 1754, Checking gradient for hidden b[0][0]:	mock grad = 0.093964785853065114, computed grad = 0.093964797752562321
Iteration: 1754, Checking gradient for output layer W[65][0]:	mock grad = 0.098589038814850261, computed grad = 0.098589036757822296
Iteration: 1755, Checking gradient for words E[475][15]:	mock grad = 0.002277296494601622, computed grad = 0.002926747810379103
Iteration: 1755, Checking gradient for hidden W[65][8]:	mock grad = -0.011976506337718629, computed grad = -0.011976506388672384
Iteration: 1755, Checking gradient for hidden b[0][4]:	mock grad = -0.014315140627829681, computed grad = -0.014315146660684530
Iteration: 1755, Checking gradient for output layer W[87][1]:	mock grad = -0.021146931733384555, computed grad = -0.021146931718747097
Iteration: 1756, Checking gradient for words E[119][12]:	mock grad = 0.017694409544716239, computed grad = 0.021737947639848272
Iteration: 1756, Checking gradient for hidden W[127][43]:	mock grad = -0.001545946596126502, computed grad = -0.001545946600709119
Iteration: 1756, Checking gradient for hidden b[0][32]:	mock grad = -0.064213585528827277, computed grad = -0.064213607339404261
Iteration: 1756, Checking gradient for output layer W[97][1]:	mock grad = -0.104613393326707937, computed grad = -0.104613393041715588
Iteration: 1757, Checking gradient for words E[10][32]:	mock grad = 0.000408983417576891, computed grad = 0.000408983397986494
Iteration: 1757, Checking gradient for hidden W[211][7]:	mock grad = 0.009152979857096621, computed grad = 0.009152980047417332
Iteration: 1757, Checking gradient for hidden b[0][44]:	mock grad = 0.001689211937411228, computed grad = 0.001689208460067277
Iteration: 1757, Checking gradient for output layer W[98][0]:	mock grad = 0.002939288984843413, computed grad = 0.002939288984824255
Iteration: 1758, Checking gradient for words E[342][16]:	mock grad = -0.005140068221054506, computed grad = -0.005140068247364302
Iteration: 1758, Checking gradient for hidden W[118][7]:	mock grad = -0.005640916941967999, computed grad = -0.005640916913728018
Iteration: 1758, Checking gradient for hidden b[0][3]:	mock grad = 0.030507641944399477, computed grad = 0.030507642784965965
Iteration: 1758, Checking gradient for output layer W[139][0]:	mock grad = -0.016002260946390567, computed grad = -0.016002260943441801
current: 60, Cost = 0.291523, Correct(%) = 1, time = 4.14098
Iteration: 1759, Checking gradient for words E[649][34]:	mock grad = 0.003315055262492761, computed grad = 0.003315055266014497
Iteration: 1759, Checking gradient for hidden W[77][47]:	mock grad = -0.007222131149620115, computed grad = -0.007222131152405120
Iteration: 1759, Checking gradient for hidden b[0][43]:	mock grad = -0.009062572778939515, computed grad = -0.009062575127475941
Iteration: 1759, Checking gradient for output layer W[2][1]:	mock grad = -0.033965401187718847, computed grad = -0.033965401149957539
Iteration: 1760, Checking gradient for words E[454][17]:	mock grad = 0.018921283047423909, computed grad = 0.018921283113653806
Iteration: 1760, Checking gradient for hidden W[52][36]:	mock grad = 0.003892268695682155, computed grad = 0.003892268718041227
Iteration: 1760, Checking gradient for hidden b[0][7]:	mock grad = 0.075092765249445970, computed grad = 0.075092781673283718
Iteration: 1760, Checking gradient for output layer W[145][0]:	mock grad = 0.094697772597823748, computed grad = 0.094697772160715801
Iteration: 1761, Checking gradient for words E[386][2]:	mock grad = 0.024583745975970128, computed grad = 0.024583746015952049
Iteration: 1761, Checking gradient for hidden W[151][42]:	mock grad = 0.010702190018030411, computed grad = 0.010702190048514530
Iteration: 1761, Checking gradient for hidden b[0][32]:	mock grad = -0.051522987929208952, computed grad = -0.051523005256069260
Iteration: 1761, Checking gradient for output layer W[141][0]:	mock grad = -0.141720033231262921, computed grad = -0.141720031431198851
Iteration: 1762, Checking gradient for words E[104][2]:	mock grad = -0.001998855601675897, computed grad = -0.001998855644400394
Iteration: 1762, Checking gradient for hidden W[24][32]:	mock grad = 0.006600029131942353, computed grad = 0.006600029166308639
Iteration: 1762, Checking gradient for hidden b[0][45]:	mock grad = -0.007446184981718673, computed grad = -0.007446186109124732
Iteration: 1762, Checking gradient for output layer W[22][0]:	mock grad = 0.057738526566070147, computed grad = 0.057738526393192109
Iteration: 1763, Checking gradient for words E[68][42]:	mock grad = 0.002438585400166282, computed grad = 0.002438585387675185
Iteration: 1763, Checking gradient for hidden W[4][39]:	mock grad = 0.003633641238953711, computed grad = 0.003633641234771195
Iteration: 1763, Checking gradient for hidden b[0][16]:	mock grad = -0.015084655546032666, computed grad = -0.015084660621092351
Iteration: 1763, Checking gradient for output layer W[9][1]:	mock grad = -0.038291906149096056, computed grad = -0.038291906099908180
Iteration: 1764, Checking gradient for words E[112][40]:	mock grad = -0.022997667552454581, computed grad = -0.022540088332614682
Iteration: 1764, Checking gradient for hidden W[90][13]:	mock grad = 0.018942464907162337, computed grad = 0.018942465024676836
Iteration: 1764, Checking gradient for hidden b[0][25]:	mock grad = -0.006216158185634590, computed grad = -0.006216156234669392
Iteration: 1764, Checking gradient for output layer W[8][1]:	mock grad = 0.065963301391025464, computed grad = 0.065963301229414392
Iteration: 1765, Checking gradient for words E[665][3]:	mock grad = -0.004527444947069470, computed grad = -0.004527444935082055
Iteration: 1765, Checking gradient for hidden W[185][24]:	mock grad = 0.001252439466234434, computed grad = 0.001252439467361538
Iteration: 1765, Checking gradient for hidden b[0][14]:	mock grad = -0.031758925504263291, computed grad = -0.031758937700911004
Iteration: 1765, Checking gradient for output layer W[9][0]:	mock grad = -0.026021407541432229, computed grad = -0.026021407498764273
Iteration: 1766, Checking gradient for words E[643][48]:	mock grad = 0.013405803193117238, computed grad = 0.013405803230973956
Iteration: 1766, Checking gradient for hidden W[220][47]:	mock grad = -0.002463703814625795, computed grad = -0.002463703822931276
Iteration: 1766, Checking gradient for hidden b[0][27]:	mock grad = 0.035437704875296294, computed grad = 0.035437727970961320
Iteration: 1766, Checking gradient for output layer W[99][1]:	mock grad = 0.086266265065360059, computed grad = 0.086266264300781076
Iteration: 1767, Checking gradient for words E[34][3]:	mock grad = -0.015715736053412144, computed grad = -0.015715736101127597
Iteration: 1767, Checking gradient for hidden W[118][40]:	mock grad = 0.003931849404809062, computed grad = 0.003931849430327443
Iteration: 1767, Checking gradient for hidden b[0][31]:	mock grad = 0.034892724034091271, computed grad = 0.034892737384800671
Iteration: 1767, Checking gradient for output layer W[129][1]:	mock grad = 0.090712294835620932, computed grad = 0.090712294185060549
Iteration: 1768, Checking gradient for words E[335][26]:	mock grad = 0.017911043952309469, computed grad = 0.017911043910526909
Iteration: 1768, Checking gradient for hidden W[159][18]:	mock grad = 0.000832992880311911, computed grad = 0.000832992852256305
Iteration: 1768, Checking gradient for hidden b[0][17]:	mock grad = -0.012843169552356759, computed grad = -0.012843172651656725
Iteration: 1768, Checking gradient for output layer W[64][1]:	mock grad = -0.089250534538051762, computed grad = -0.089250533429191023
current: 70, Cost = 0.228896, Correct(%) = 1, time = 4.75296
Iteration: 1769, Checking gradient for words E[320][49]:	mock grad = 0.006486498968519272, computed grad = 0.006486498976905732
Iteration: 1769, Checking gradient for hidden W[48][11]:	mock grad = -0.005304724563809371, computed grad = -0.005304724600358136
Iteration: 1769, Checking gradient for hidden b[0][16]:	mock grad = -0.011521182176746358, computed grad = -0.011521185733064477
Iteration: 1769, Checking gradient for output layer W[115][1]:	mock grad = 0.084647869177736923, computed grad = 0.084647868042697458
Iteration: 1770, Checking gradient for words E[259][28]:	mock grad = 0.002756678201459861, computed grad = 0.002756678290571907
Iteration: 1770, Checking gradient for hidden W[133][9]:	mock grad = -0.002054291105307859, computed grad = -0.002054291101621367
Iteration: 1770, Checking gradient for hidden b[0][16]:	mock grad = -0.016850374903065735, computed grad = -0.016850380225498743
Iteration: 1770, Checking gradient for output layer W[41][1]:	mock grad = 0.051212919010806823, computed grad = 0.051212918935477282
Iteration: 1771, Checking gradient for words E[686][29]:	mock grad = 0.007530819957607404, computed grad = 0.007530819875857202
Iteration: 1771, Checking gradient for hidden W[119][45]:	mock grad = -0.001240904353300154, computed grad = -0.001240904369289621
Iteration: 1771, Checking gradient for hidden b[0][27]:	mock grad = -0.027893216908714269, computed grad = -0.027893224893943195
Iteration: 1771, Checking gradient for output layer W[141][0]:	mock grad = -0.124004478051387146, computed grad = -0.124004476806147249
Iteration: 1772, Checking gradient for words E[689][39]:	mock grad = -0.017338358274332499, computed grad = -0.017338358270134843
Iteration: 1772, Checking gradient for hidden W[196][48]:	mock grad = -0.012004204563959009, computed grad = -0.013110160953546179
Iteration: 1772, Checking gradient for hidden b[0][38]:	mock grad = -0.051585089015676600, computed grad = -0.051585091691820850
Iteration: 1772, Checking gradient for output layer W[49][1]:	mock grad = -0.033986783690459488, computed grad = -0.033986783668552754
Iteration: 1773, Checking gradient for words E[49][33]:	mock grad = 0.017736046958732876, computed grad = 0.017736046990049735
Iteration: 1773, Checking gradient for hidden W[229][6]:	mock grad = 0.001843811168633636, computed grad = 0.001843811185862127
Iteration: 1773, Checking gradient for hidden b[0][36]:	mock grad = -0.013363364535640132, computed grad = -0.013363366278095822
Iteration: 1773, Checking gradient for output layer W[75][0]:	mock grad = -0.074248885682803856, computed grad = -0.074248885518656910
Iteration: 1774, Checking gradient for words E[693][7]:	mock grad = 0.004505864003134796, computed grad = 0.004505863965653155
Iteration: 1774, Checking gradient for hidden W[110][16]:	mock grad = -0.002075278582008844, computed grad = -0.002075278579380437
Iteration: 1774, Checking gradient for hidden b[0][27]:	mock grad = -0.047757335788822797, computed grad = -0.047757359757224151
Iteration: 1774, Checking gradient for output layer W[106][0]:	mock grad = -0.048440128625842904, computed grad = -0.048440128573674558
Iteration: 1775, Checking gradient for words E[308][16]:	mock grad = -0.012436215358097025, computed grad = -0.012436215388571602
Iteration: 1775, Checking gradient for hidden W[70][5]:	mock grad = 0.007944598212461518, computed grad = 0.007944598268830851
Iteration: 1775, Checking gradient for hidden b[0][25]:	mock grad = -0.003277870464862831, computed grad = -0.003277867946671947
Iteration: 1775, Checking gradient for output layer W[142][1]:	mock grad = 0.094722103461231111, computed grad = 0.094722102324773322
Iteration: 1776, Checking gradient for words E[82][26]:	mock grad = 0.002687297390391308, computed grad = 0.002687297435322253
Iteration: 1776, Checking gradient for hidden W[222][8]:	mock grad = -0.014495838151162843, computed grad = -0.014495838228056682
Iteration: 1776, Checking gradient for hidden b[0][49]:	mock grad = 0.035195822883582473, computed grad = 0.035195832080070842
Iteration: 1776, Checking gradient for output layer W[130][0]:	mock grad = 0.082233465790060301, computed grad = 0.082233465297658959
Iteration: 1777, Checking gradient for words E[94][30]:	mock grad = -0.005781217160091834, computed grad = -0.005781217232472734
Iteration: 1777, Checking gradient for hidden W[85][42]:	mock grad = 0.000411180142614875, computed grad = 0.000411180139381771
Iteration: 1777, Checking gradient for hidden b[0][47]:	mock grad = -0.055037876961094856, computed grad = -0.055037890246279489
Iteration: 1777, Checking gradient for output layer W[79][1]:	mock grad = 0.024540384759103029, computed grad = 0.024540384732882441
Iteration: 1778, Checking gradient for words E[54][20]:	mock grad = 0.003487144983299606, computed grad = 0.001439310675319880
Iteration: 1778, Checking gradient for hidden W[218][35]:	mock grad = -0.003468896358937990, computed grad = -0.003468896390400088
Iteration: 1778, Checking gradient for hidden b[0][11]:	mock grad = 0.077714057154787408, computed grad = 0.077714079525699822
Iteration: 1778, Checking gradient for output layer W[93][0]:	mock grad = -0.024490372729696341, computed grad = -0.024490372716988687
current: 80, Cost = 0.249668, Correct(%) = 1, time = 5.23732
Iteration: 1779, Checking gradient for words E[636][15]:	mock grad = 0.004718528538241729, computed grad = 0.004718528564117967
Iteration: 1779, Checking gradient for hidden W[91][7]:	mock grad = 0.002496053606998450, computed grad = 0.002496053615090999
Iteration: 1779, Checking gradient for hidden b[0][48]:	mock grad = 0.051191738731123926, computed grad = 0.051191734234295766
Iteration: 1779, Checking gradient for output layer W[16][0]:	mock grad = 0.022820124382816331, computed grad = 0.022820124365261516
Iteration: 1780, Checking gradient for words E[107][4]:	mock grad = 0.011738131416888509, computed grad = 0.011738131422935597
Iteration: 1780, Checking gradient for hidden W[177][28]:	mock grad = -0.011178244918724767, computed grad = -0.011178244978235075
Iteration: 1780, Checking gradient for hidden b[0][37]:	mock grad = -0.039897909206138005, computed grad = -0.039897918351922898
Iteration: 1780, Checking gradient for output layer W[72][0]:	mock grad = 0.079164059524405861, computed grad = 0.079164059083718763
Iteration: 1781, Checking gradient for words E[713][14]:	mock grad = -0.008440014276855923, computed grad = -0.008440014317904095
Iteration: 1781, Checking gradient for hidden W[249][38]:	mock grad = 0.005860526815976153, computed grad = 0.005860526798726614
Iteration: 1781, Checking gradient for hidden b[0][40]:	mock grad = 0.076659401069767430, computed grad = 0.076659421520576837
Iteration: 1781, Checking gradient for output layer W[120][0]:	mock grad = 0.109544820846602109, computed grad = 0.109544819174779484
Iteration: 1782, Checking gradient for words E[524][39]:	mock grad = -0.017246550800431582, computed grad = -0.017203280181796496
Iteration: 1782, Checking gradient for hidden W[212][7]:	mock grad = 0.003322820417039907, computed grad = 0.003322820431597436
Iteration: 1782, Checking gradient for hidden b[0][28]:	mock grad = 0.012443745104034898, computed grad = 0.012443757736992887
Iteration: 1782, Checking gradient for output layer W[36][0]:	mock grad = 0.024262249440326622, computed grad = 0.024262249424028010
Iteration: 1783, Checking gradient for words E[121][46]:	mock grad = -0.000606752942416966, computed grad = -0.000606752954731894
Iteration: 1783, Checking gradient for hidden W[158][15]:	mock grad = -0.013974205065686585, computed grad = -0.013974205456895688
Iteration: 1783, Checking gradient for hidden b[0][19]:	mock grad = -0.035403314766146998, computed grad = -0.035403317117363498
Iteration: 1783, Checking gradient for output layer W[16][0]:	mock grad = 0.033733082522358071, computed grad = 0.033733082495315085
Iteration: 1784, Checking gradient for words E[25][19]:	mock grad = -0.012499763294065991, computed grad = -0.012499763349357783
Iteration: 1784, Checking gradient for hidden W[105][49]:	mock grad = 0.009077423196460210, computed grad = 0.011106763378692971
Iteration: 1784, Checking gradient for hidden b[0][26]:	mock grad = -0.017476990821954641, computed grad = -0.017476990138529023
Iteration: 1784, Checking gradient for output layer W[117][1]:	mock grad = -0.103834021938878363, computed grad = -0.103834020243936190
Iteration: 1785, Checking gradient for words E[582][44]:	mock grad = -0.006844758987281652, computed grad = -0.006844759065450078
Iteration: 1785, Checking gradient for hidden W[136][31]:	mock grad = -0.001890897807221847, computed grad = -0.001890897800515894
Iteration: 1785, Checking gradient for hidden b[0][33]:	mock grad = 0.071679278177902428, computed grad = 0.071679296885297603
Iteration: 1785, Checking gradient for output layer W[143][1]:	mock grad = 0.120927101003198700, computed grad = 0.120927098376783185
Iteration: 1786, Checking gradient for words E[48][3]:	mock grad = -0.000763332595243860, computed grad = -0.000763332502071390
Iteration: 1786, Checking gradient for hidden W[122][35]:	mock grad = 0.002079744639060799, computed grad = 0.002079744649316377
Iteration: 1786, Checking gradient for hidden b[0][29]:	mock grad = -0.056549346407946954, computed grad = -0.056549352697968493
Iteration: 1786, Checking gradient for output layer W[83][0]:	mock grad = -0.135153409848931538, computed grad = -0.135153407319683477
Iteration: 1787, Checking gradient for words E[184][29]:	mock grad = 0.016917374027064502, computed grad = 0.016917374013505241
Iteration: 1787, Checking gradient for hidden W[223][4]:	mock grad = 0.001738699121428544, computed grad = 0.001738699131214697
Iteration: 1787, Checking gradient for hidden b[0][26]:	mock grad = -0.024656058374317169, computed grad = -0.024656062461401292
Iteration: 1787, Checking gradient for output layer W[10][1]:	mock grad = -0.030582808573959008, computed grad = -0.030582808546712283
Iteration: 1788, Checking gradient for words E[58][10]:	mock grad = -0.002457894176938202, computed grad = -0.002457894197855061
Iteration: 1788, Checking gradient for hidden W[48][9]:	mock grad = -0.001365487238724761, computed grad = -0.001365487244095370
Iteration: 1788, Checking gradient for hidden b[0][19]:	mock grad = 0.023242313743704335, computed grad = 0.023242306293345383
Iteration: 1788, Checking gradient for output layer W[40][1]:	mock grad = 0.037525230233703688, computed grad = 0.037525230175934829
current: 90, Cost = 0.289094, Correct(%) = 1, time = 6.18448
Iteration: 1789, Checking gradient for words E[697][13]:	mock grad = 0.018887217218627184, computed grad = 0.018887217315584941
Iteration: 1789, Checking gradient for hidden W[149][40]:	mock grad = 0.006625523869346894, computed grad = 0.006625523946939448
Iteration: 1789, Checking gradient for hidden b[0][36]:	mock grad = 0.005052240501596250, computed grad = 0.005052235590251647
Iteration: 1789, Checking gradient for output layer W[123][0]:	mock grad = -0.112383178026836950, computed grad = -0.112383176627332187
Iteration: 1790, Checking gradient for words E[119][44]:	mock grad = -0.000788047893829358, computed grad = -0.000788047886790358
Iteration: 1790, Checking gradient for hidden W[169][26]:	mock grad = -0.004368602105342401, computed grad = -0.004368602128953168
Iteration: 1790, Checking gradient for hidden b[0][29]:	mock grad = -0.048195352117974544, computed grad = -0.048195354874262360
Iteration: 1790, Checking gradient for output layer W[110][0]:	mock grad = 0.132093053448079090, computed grad = 0.132093050160316028
Iteration: 1791, Checking gradient for words E[84][30]:	mock grad = 0.021948269688987221, computed grad = 0.021948269745314070
Iteration: 1791, Checking gradient for hidden W[86][20]:	mock grad = -0.000339015213068894, computed grad = -0.000339015237115064
Iteration: 1791, Checking gradient for hidden b[0][15]:	mock grad = -0.077109955353427129, computed grad = -0.077109968065996681
Iteration: 1791, Checking gradient for output layer W[89][1]:	mock grad = 0.157418842551076965, computed grad = 0.157418839594228588
Iteration: 1792, Checking gradient for words E[755][22]:	mock grad = -0.013655020207267721, computed grad = -0.013655020216708069
Iteration: 1792, Checking gradient for hidden W[28][12]:	mock grad = 0.007542327300075202, computed grad = 0.007542327404478168
Iteration: 1792, Checking gradient for hidden b[0][38]:	mock grad = 0.043562402512922760, computed grad = 0.043562414445652196
Iteration: 1792, Checking gradient for output layer W[139][1]:	mock grad = -0.101080311971493697, computed grad = -0.101080309795577944
Iteration: 1793, Checking gradient for words E[759][33]:	mock grad = -0.001190073870366293, computed grad = -0.001190073888599096
Iteration: 1793, Checking gradient for hidden W[37][3]:	mock grad = -0.009682978022451660, computed grad = -0.009682978053980409
Iteration: 1793, Checking gradient for hidden b[0][16]:	mock grad = -0.014764030477620871, computed grad = -0.014764035523551856
Iteration: 1793, Checking gradient for output layer W[31][0]:	mock grad = -0.056922468862380393, computed grad = -0.056922468684676167
Iteration: 1794, Checking gradient for words E[452][40]:	mock grad = 0.001795860885486777, computed grad = 0.001795860909108868
Iteration: 1794, Checking gradient for hidden W[35][16]:	mock grad = 0.000819240194460669, computed grad = 0.000819240202514618
Iteration: 1794, Checking gradient for hidden b[0][29]:	mock grad = 0.053049466945551127, computed grad = 0.053049485650510535
Iteration: 1794, Checking gradient for output layer W[54][1]:	mock grad = -0.097712152897527771, computed grad = -0.097712151038273146
Iteration: 1795, Checking gradient for words E[554][12]:	mock grad = 0.005481851925753789, computed grad = 0.005481851923392267
Iteration: 1795, Checking gradient for hidden W[22][28]:	mock grad = -0.007890271114308067, computed grad = -0.007890271150386111
Iteration: 1795, Checking gradient for hidden b[0][16]:	mock grad = -0.015320778613825414, computed grad = -0.015320783104784668
Iteration: 1795, Checking gradient for output layer W[103][1]:	mock grad = 0.106212241517217310, computed grad = 0.106212240665890412
Iteration: 1796, Checking gradient for words E[765][46]:	mock grad = -0.010406107582905388, computed grad = -0.010406107610393260
Iteration: 1796, Checking gradient for hidden W[121][6]:	mock grad = -0.001406137651677186, computed grad = -0.001406137654813281
Iteration: 1796, Checking gradient for hidden b[0][25]:	mock grad = 0.005378931056676528, computed grad = 0.005378930358112640
Iteration: 1796, Checking gradient for output layer W[111][1]:	mock grad = -0.018807356818695276, computed grad = -0.018807356810145819
Iteration: 1797, Checking gradient for words E[411][5]:	mock grad = 0.028174275053222120, computed grad = 0.028174275080836308
Iteration: 1797, Checking gradient for hidden W[191][14]:	mock grad = -0.012620108564898036, computed grad = -0.012620108593174025
Iteration: 1797, Checking gradient for hidden b[0][46]:	mock grad = -0.148913892208946130, computed grad = -0.148913920877928152
Iteration: 1797, Checking gradient for output layer W[98][0]:	mock grad = -0.028913168540978251, computed grad = -0.028913168532501806
Iteration: 1798, Checking gradient for words E[773][16]:	mock grad = -0.007891028139972445, computed grad = -0.007891028131565794
Iteration: 1798, Checking gradient for hidden W[107][31]:	mock grad = 0.006290684988585760, computed grad = 0.006290685020490116
Iteration: 1798, Checking gradient for hidden b[0][1]:	mock grad = 0.075507662655099894, computed grad = 0.075507691924069212
Iteration: 1798, Checking gradient for output layer W[53][0]:	mock grad = 0.098442720095681668, computed grad = 0.098442718711186275
current: 100, Cost = 0.292506, Correct(%) = 1, time = 6.96985
Iteration: 1799, Checking gradient for words E[25][3]:	mock grad = -0.017130558805172891, computed grad = -0.019052134722211705
Iteration: 1799, Checking gradient for hidden W[247][33]:	mock grad = -0.000293976398862972, computed grad = -0.000293976395860910
Iteration: 1799, Checking gradient for hidden b[0][23]:	mock grad = 0.112993243185782744, computed grad = 0.112993265076889365
Iteration: 1799, Checking gradient for output layer W[58][1]:	mock grad = 0.019014377852427655, computed grad = 0.019014377845879674
current: 18, Correct(%) = 1, time = 7.02109
Dev start.
Dev finished. Total time taken is: 0.634792
dev:
Accuracy:	P=100/100=1
Test start.
Test finished. Total time taken is: 0.54497
test:
Accuracy:	P=63/100=0.63
##### Iteration 18
random: 0, 99
Iteration: 1800, Checking gradient for words E[51][29]:	mock grad = 0.011757767347436721, computed grad = 0.011757767389325807
Iteration: 1800, Checking gradient for hidden W[88][20]:	mock grad = 0.003893131645199555, computed grad = 0.003893131660075911
Iteration: 1800, Checking gradient for hidden b[0][8]:	mock grad = 0.017102954732561848, computed grad = 0.017102965274460216
Iteration: 1800, Checking gradient for output layer W[55][1]:	mock grad = 0.084517134812139849, computed grad = 0.084517133350997184
Iteration: 1801, Checking gradient for words E[57][41]:	mock grad = -0.005439752275460119, computed grad = -0.005439752269980174
Iteration: 1801, Checking gradient for hidden W[1][36]:	mock grad = -0.010552738609503898, computed grad = -0.010552738761565188
Iteration: 1801, Checking gradient for hidden b[0][44]:	mock grad = 0.002182822344626301, computed grad = 0.002182819526445182
Iteration: 1801, Checking gradient for output layer W[106][1]:	mock grad = -0.024826998470367512, computed grad = -0.024826998452076379
Iteration: 1802, Checking gradient for words E[111][18]:	mock grad = 0.000790381443377397, computed grad = 0.000790381449769909
Iteration: 1802, Checking gradient for hidden W[35][17]:	mock grad = -0.001313107481332176, computed grad = -0.001313107496550121
Iteration: 1802, Checking gradient for hidden b[0][11]:	mock grad = 0.064474763688343595, computed grad = 0.064474779598552287
Iteration: 1802, Checking gradient for output layer W[118][1]:	mock grad = 0.035812714137967161, computed grad = 0.035812714071188023
Iteration: 1803, Checking gradient for words E[776][4]:	mock grad = 0.004612439973489257, computed grad = 0.004090173525742278
Iteration: 1803, Checking gradient for hidden W[126][9]:	mock grad = -0.001268073832938477, computed grad = -0.001268073825281711
Iteration: 1803, Checking gradient for hidden b[0][5]:	mock grad = 0.009142524212663039, computed grad = 0.009142518844632221
Iteration: 1803, Checking gradient for output layer W[76][1]:	mock grad = -0.066458013530279469, computed grad = -0.066458013299949437
Iteration: 1804, Checking gradient for words E[42][49]:	mock grad = 0.000903943826263687, computed grad = 0.000903943866421963
Iteration: 1804, Checking gradient for hidden W[186][30]:	mock grad = 0.000291000213009651, computed grad = 0.000291000209545037
Iteration: 1804, Checking gradient for hidden b[0][48]:	mock grad = -0.061123054667933108, computed grad = -0.061123062950016911
Iteration: 1804, Checking gradient for output layer W[93][0]:	mock grad = -0.008930292172332210, computed grad = -0.008930292171599885
Iteration: 1805, Checking gradient for words E[210][28]:	mock grad = -0.010368430239215964, computed grad = -0.010368430288020741
Iteration: 1805, Checking gradient for hidden W[28][13]:	mock grad = 0.006312418100673511, computed grad = 0.006312418115919010
Iteration: 1805, Checking gradient for hidden b[0][9]:	mock grad = 0.039506511950782031, computed grad = 0.039506515573923894
Iteration: 1805, Checking gradient for output layer W[125][0]:	mock grad = -0.082564334792867289, computed grad = -0.082564334256821109
Iteration: 1806, Checking gradient for words E[48][42]:	mock grad = -0.026631075972935458, computed grad = -0.026631076008384678
Iteration: 1806, Checking gradient for hidden W[34][2]:	mock grad = 0.002421889897033513, computed grad = 0.002421889898509457
Iteration: 1806, Checking gradient for hidden b[0][38]:	mock grad = -0.036798221131409403, computed grad = -0.036798225097561033
Iteration: 1806, Checking gradient for output layer W[6][1]:	mock grad = -0.023547490146158534, computed grad = -0.023547490118700894
Iteration: 1807, Checking gradient for words E[6][5]:	mock grad = 0.037080407620015965, computed grad = 0.037080407732285818
Iteration: 1807, Checking gradient for hidden W[243][9]:	mock grad = -0.005036035001237948, computed grad = -0.005036035002519223
Iteration: 1807, Checking gradient for hidden b[0][29]:	mock grad = -0.093410556555789670, computed grad = -0.093410585835819904
Iteration: 1807, Checking gradient for output layer W[81][0]:	mock grad = -0.069797707937718778, computed grad = -0.069797707845476398
Iteration: 1808, Checking gradient for words E[301][13]:	mock grad = -0.014107233752003179, computed grad = -0.014107233798543856
Iteration: 1808, Checking gradient for hidden W[56][20]:	mock grad = 0.005976660336165507, computed grad = 0.005976660398830939
Iteration: 1808, Checking gradient for hidden b[0][25]:	mock grad = -0.005398393106564869, computed grad = -0.005398392015949924
Iteration: 1808, Checking gradient for output layer W[141][0]:	mock grad = -0.111697194158072444, computed grad = -0.111697192384050209
current: 10, Cost = 0.301983, Correct(%) = 1, time = 0.769665
Iteration: 1809, Checking gradient for words E[25][18]:	mock grad = 0.029481946559078986, computed grad = 0.029533424977163193
Iteration: 1809, Checking gradient for hidden W[69][4]:	mock grad = 0.001356699715154841, computed grad = 0.001356699816852524
Iteration: 1809, Checking gradient for hidden b[0][2]:	mock grad = 0.051907710636761584, computed grad = 0.051907739445073500
Iteration: 1809, Checking gradient for output layer W[33][1]:	mock grad = 0.008789262389452546, computed grad = 0.008789262388916048
Iteration: 1810, Checking gradient for words E[321][42]:	mock grad = -0.025793893546913194, computed grad = -0.025158611065928586
Iteration: 1810, Checking gradient for hidden W[175][32]:	mock grad = -0.001192988906315029, computed grad = -0.001192988908817045
Iteration: 1810, Checking gradient for hidden b[0][43]:	mock grad = 0.007360145214185820, computed grad = 0.007360147231451127
Iteration: 1810, Checking gradient for output layer W[107][1]:	mock grad = 0.029614020373783245, computed grad = 0.029614020328768680
Iteration: 1811, Checking gradient for words E[34][23]:	mock grad = -0.016822051620846157, computed grad = -0.016822051662793443
Iteration: 1811, Checking gradient for hidden W[112][19]:	mock grad = -0.012403102416125300, computed grad = -0.012403102522613998
Iteration: 1811, Checking gradient for hidden b[0][36]:	mock grad = -0.013781192367973150, computed grad = -0.013781195169630462
Iteration: 1811, Checking gradient for output layer W[46][0]:	mock grad = -0.064373476707207944, computed grad = -0.064373476578631919
Iteration: 1812, Checking gradient for words E[337][32]:	mock grad = -0.004297718883924939, computed grad = -0.004297718857041816
Iteration: 1812, Checking gradient for hidden W[219][36]:	mock grad = 0.009034699013518721, computed grad = 0.009034699004092071
Iteration: 1812, Checking gradient for hidden b[0][28]:	mock grad = -0.000624127498558003, computed grad = -0.000624129926433612
Iteration: 1812, Checking gradient for output layer W[0][1]:	mock grad = -0.043204301909918752, computed grad = -0.043204301789666749
Iteration: 1813, Checking gradient for words E[102][15]:	mock grad = 0.006390619939830589, computed grad = 0.006390619930805280
Iteration: 1813, Checking gradient for hidden W[228][31]:	mock grad = -0.009098305789642658, computed grad = -0.009098305870773930
Iteration: 1813, Checking gradient for hidden b[0][4]:	mock grad = 0.011127413599196223, computed grad = 0.011127415426590109
Iteration: 1813, Checking gradient for output layer W[68][1]:	mock grad = 0.088307172478194196, computed grad = 0.088307171305636881
Iteration: 1814, Checking gradient for words E[376][11]:	mock grad = -0.017009983833415898, computed grad = -0.017009983848931317
Iteration: 1814, Checking gradient for hidden W[111][48]:	mock grad = 0.001649249666224595, computed grad = 0.001649249695661555
Iteration: 1814, Checking gradient for hidden b[0][9]:	mock grad = 0.066588623271635061, computed grad = 0.066588635356070028
Iteration: 1814, Checking gradient for output layer W[115][1]:	mock grad = -0.025479200230466015, computed grad = -0.025479200227330853
Iteration: 1815, Checking gradient for words E[383][49]:	mock grad = -0.019031136828301376, computed grad = -0.015975691405220743
Iteration: 1815, Checking gradient for hidden W[32][9]:	mock grad = 0.007662952666243061, computed grad = 0.007662952662256401
Iteration: 1815, Checking gradient for hidden b[0][6]:	mock grad = 0.030009863187130392, computed grad = 0.030009872574802799
Iteration: 1815, Checking gradient for output layer W[146][1]:	mock grad = -0.023811807717460942, computed grad = -0.023811807704201059
Iteration: 1816, Checking gradient for words E[42][37]:	mock grad = -0.027302163126990964, computed grad = -0.027302163132282026
Iteration: 1816, Checking gradient for hidden W[17][48]:	mock grad = -0.003997670752542426, computed grad = -0.003997670757029688
Iteration: 1816, Checking gradient for hidden b[0][12]:	mock grad = -0.063325990811713417, computed grad = -0.063326010867136476
Iteration: 1816, Checking gradient for output layer W[31][0]:	mock grad = -0.034359011271578588, computed grad = -0.034359011240989133
Iteration: 1817, Checking gradient for words E[186][40]:	mock grad = 0.000475687502166089, computed grad = 0.000475687520190278
Iteration: 1817, Checking gradient for hidden W[101][2]:	mock grad = 0.001603218484746982, computed grad = 0.001603218506839279
Iteration: 1817, Checking gradient for hidden b[0][22]:	mock grad = -0.025234227223849826, computed grad = -0.025234231403629018
Iteration: 1817, Checking gradient for output layer W[98][0]:	mock grad = 0.023776758108912111, computed grad = 0.023776758076310262
Iteration: 1818, Checking gradient for words E[25][1]:	mock grad = -0.022836203918130682, computed grad = -0.022836203964832477
Iteration: 1818, Checking gradient for hidden W[78][10]:	mock grad = -0.004192984499454266, computed grad = -0.004192984494208468
Iteration: 1818, Checking gradient for hidden b[0][9]:	mock grad = 0.039402199152507866, computed grad = 0.039402208066864834
Iteration: 1818, Checking gradient for output layer W[13][0]:	mock grad = -0.029073433493842504, computed grad = -0.029073433453287684
current: 20, Cost = 0.409401, Correct(%) = 1, time = 1.43016
Iteration: 1819, Checking gradient for words E[435][40]:	mock grad = 0.052292518919971887, computed grad = 0.053617114843942361
Iteration: 1819, Checking gradient for hidden W[92][29]:	mock grad = -0.006658247860835687, computed grad = -0.006658247873448386
Iteration: 1819, Checking gradient for hidden b[0][42]:	mock grad = -0.071257773382848999, computed grad = -0.071257792004540105
Iteration: 1819, Checking gradient for output layer W[16][0]:	mock grad = 0.041050155856897641, computed grad = 0.041050155834634020
Iteration: 1820, Checking gradient for words E[315][47]:	mock grad = 0.002879544938327872, computed grad = 0.003220333381498040
Iteration: 1820, Checking gradient for hidden W[124][16]:	mock grad = -0.001294216199404508, computed grad = -0.001294216195314525
Iteration: 1820, Checking gradient for hidden b[0][37]:	mock grad = -0.034934293483140921, computed grad = -0.034934299985805800
Iteration: 1820, Checking gradient for output layer W[87][1]:	mock grad = -0.013591486961339605, computed grad = -0.013591486958624763
Iteration: 1821, Checking gradient for words E[284][20]:	mock grad = -0.009228530128302381, computed grad = -0.009228530059930379
Iteration: 1821, Checking gradient for hidden W[202][35]:	mock grad = 0.000793481661043316, computed grad = 0.000793481667762434
Iteration: 1821, Checking gradient for hidden b[0][25]:	mock grad = -0.006269649342749428, computed grad = -0.006269644987595538
Iteration: 1821, Checking gradient for output layer W[25][0]:	mock grad = 0.085421806400143296, computed grad = 0.085421806256503377
Iteration: 1822, Checking gradient for words E[294][32]:	mock grad = 0.003266935616835775, computed grad = 0.003266935636776221
Iteration: 1822, Checking gradient for hidden W[222][29]:	mock grad = -0.002198853245899413, computed grad = 0.006434986835854759
Iteration: 1822, Checking gradient for hidden b[0][32]:	mock grad = -0.038407603031892501, computed grad = -0.038407614884341357
Iteration: 1822, Checking gradient for output layer W[104][1]:	mock grad = 0.060436293236404426, computed grad = 0.060436292887218579
Iteration: 1823, Checking gradient for words E[776][18]:	mock grad = 0.023943551877764202, computed grad = 0.023943551907113385
Iteration: 1823, Checking gradient for hidden W[1][4]:	mock grad = -0.003127530209534601, computed grad = -0.003127530252682606
Iteration: 1823, Checking gradient for hidden b[0][37]:	mock grad = -0.037752688634040688, computed grad = -0.037752696919824066
Iteration: 1823, Checking gradient for output layer W[132][0]:	mock grad = 0.015277911996708360, computed grad = 0.015277911993012136
Iteration: 1824, Checking gradient for words E[104][27]:	mock grad = 0.008000981464639789, computed grad = 0.008000981450636017
Iteration: 1824, Checking gradient for hidden W[190][34]:	mock grad = -0.006986515696388862, computed grad = -0.006986515675115114
Iteration: 1824, Checking gradient for hidden b[0][10]:	mock grad = 0.079198074039998900, computed grad = 0.079198094524343746
Iteration: 1824, Checking gradient for output layer W[55][0]:	mock grad = -0.136488865492889744, computed grad = -0.136488863296940433
Iteration: 1825, Checking gradient for words E[465][12]:	mock grad = -0.049257960542337109, computed grad = -0.049257960344074148
Iteration: 1825, Checking gradient for hidden W[131][43]:	mock grad = 0.001948329258316672, computed grad = 0.001948329269369331
Iteration: 1825, Checking gradient for hidden b[0][45]:	mock grad = -0.008599846449547366, computed grad = -0.008599848842942159
Iteration: 1825, Checking gradient for output layer W[102][0]:	mock grad = 0.119997338289101751, computed grad = 0.119997336874581098
Iteration: 1826, Checking gradient for words E[42][2]:	mock grad = -0.011473555957736137, computed grad = -0.011473555974061056
Iteration: 1826, Checking gradient for hidden W[36][34]:	mock grad = 0.001337538991327825, computed grad = 0.001337538995285161
Iteration: 1826, Checking gradient for hidden b[0][40]:	mock grad = -0.061829834649307358, computed grad = -0.061829848781921062
Iteration: 1826, Checking gradient for output layer W[81][0]:	mock grad = 0.029834097066402943, computed grad = 0.029834097005740839
Iteration: 1827, Checking gradient for words E[485][39]:	mock grad = 0.025440372032642378, computed grad = 0.025440372039188301
Iteration: 1827, Checking gradient for hidden W[84][44]:	mock grad = -0.016665300196533339, computed grad = -0.016665300377604480
Iteration: 1827, Checking gradient for hidden b[0][11]:	mock grad = -0.102604097211606016, computed grad = -0.102604128076547008
Iteration: 1827, Checking gradient for output layer W[49][1]:	mock grad = 0.021001460970448971, computed grad = 0.021001460967375499
Iteration: 1828, Checking gradient for words E[25][43]:	mock grad = -0.000170777344085660, computed grad = -0.000170777341020932
Iteration: 1828, Checking gradient for hidden W[26][34]:	mock grad = 0.006678017028735606, computed grad = 0.006678016940780475
Iteration: 1828, Checking gradient for hidden b[0][21]:	mock grad = -0.032091436429210196, computed grad = -0.032091442134620909
Iteration: 1828, Checking gradient for output layer W[115][1]:	mock grad = -0.041608477713894754, computed grad = -0.041608477577411158
current: 30, Cost = 0.280566, Correct(%) = 1, time = 2.12199
Iteration: 1829, Checking gradient for words E[25][20]:	mock grad = 0.009564636082537437, computed grad = 0.009564636087163115
Iteration: 1829, Checking gradient for hidden W[57][40]:	mock grad = -0.005850118717398711, computed grad = -0.005850118737659171
Iteration: 1829, Checking gradient for hidden b[0][19]:	mock grad = 0.026132314887877817, computed grad = 0.026132309064996983
Iteration: 1829, Checking gradient for output layer W[23][1]:	mock grad = 0.046441850541312224, computed grad = 0.046441850433706078
Iteration: 1830, Checking gradient for words E[377][25]:	mock grad = 0.023392345326667119, computed grad = 0.023392345350932719
Iteration: 1830, Checking gradient for hidden W[57][12]:	mock grad = 0.003715830564854361, computed grad = 0.003715830571933478
Iteration: 1830, Checking gradient for hidden b[0][22]:	mock grad = 0.034236666125647686, computed grad = 0.034236674478544268
Iteration: 1830, Checking gradient for output layer W[112][1]:	mock grad = -0.023404073191096719, computed grad = -0.023404073174238604
Iteration: 1831, Checking gradient for words E[216][4]:	mock grad = 0.019126055303769762, computed grad = 0.019126055266684587
Iteration: 1831, Checking gradient for hidden W[138][35]:	mock grad = 0.000739945725164448, computed grad = 0.000739945738129528
Iteration: 1831, Checking gradient for hidden b[0][28]:	mock grad = 0.012485456844540943, computed grad = 0.012485471722938495
Iteration: 1831, Checking gradient for output layer W[137][0]:	mock grad = 0.152787259868059078, computed grad = 0.152787258333206388
Iteration: 1832, Checking gradient for words E[515][16]:	mock grad = 0.002755630073791870, computed grad = 0.002755630069206058
Iteration: 1832, Checking gradient for hidden W[200][39]:	mock grad = -0.005573473084052871, computed grad = -0.005573473116934448
Iteration: 1832, Checking gradient for hidden b[0][27]:	mock grad = 0.024570650729971444, computed grad = 0.024570663044926916
Iteration: 1832, Checking gradient for output layer W[149][0]:	mock grad = 0.090762165171121012, computed grad = 0.090762164290857691
Iteration: 1833, Checking gradient for words E[215][37]:	mock grad = -0.000288547445187470, computed grad = -0.000288547465307774
Iteration: 1833, Checking gradient for hidden W[7][32]:	mock grad = -0.000924187836134904, computed grad = -0.000924187835998525
Iteration: 1833, Checking gradient for hidden b[0][45]:	mock grad = 0.005927660176208160, computed grad = 0.005927660938668928
Iteration: 1833, Checking gradient for output layer W[119][1]:	mock grad = 0.109392350956541429, computed grad = 0.109392348853891347
Iteration: 1834, Checking gradient for words E[776][47]:	mock grad = -0.003749744878156491, computed grad = -0.004961655015028942
Iteration: 1834, Checking gradient for hidden W[234][45]:	mock grad = 0.000343335152697133, computed grad = 0.000343335134516181
Iteration: 1834, Checking gradient for hidden b[0][7]:	mock grad = 0.071183262345481113, computed grad = 0.071183275901435195
Iteration: 1834, Checking gradient for output layer W[124][1]:	mock grad = -0.058513946990024213, computed grad = -0.058513946879350826
Iteration: 1835, Checking gradient for words E[201][3]:	mock grad = -0.006770326814231620, computed grad = -0.004277004285388446
Iteration: 1835, Checking gradient for hidden W[56][17]:	mock grad = -0.000393775503176874, computed grad = -0.000393775503454532
Iteration: 1835, Checking gradient for hidden b[0][46]:	mock grad = 0.127516567071467302, computed grad = 0.127516603086749836
Iteration: 1835, Checking gradient for output layer W[13][0]:	mock grad = 0.025896870202490874, computed grad = 0.025896870192383105
Iteration: 1836, Checking gradient for words E[156][10]:	mock grad = 0.056882765204546715, computed grad = 0.056882765242874542
Iteration: 1836, Checking gradient for hidden W[103][22]:	mock grad = 0.001191708599329599, computed grad = 0.001191708581301213
Iteration: 1836, Checking gradient for hidden b[0][31]:	mock grad = -0.044644901099399892, computed grad = -0.044644915403844944
Iteration: 1836, Checking gradient for output layer W[50][0]:	mock grad = -0.164691329631094785, computed grad = -0.164691328163896616
Iteration: 1837, Checking gradient for words E[524][41]:	mock grad = -0.010070057331723614, computed grad = -0.010070057330972979
Iteration: 1837, Checking gradient for hidden W[44][12]:	mock grad = 0.002129559866836939, computed grad = 0.002129559885095340
Iteration: 1837, Checking gradient for hidden b[0][4]:	mock grad = -0.016196706337207045, computed grad = -0.016196712946783029
Iteration: 1837, Checking gradient for output layer W[31][1]:	mock grad = 0.047202854988936949, computed grad = 0.047202854886756927
Iteration: 1838, Checking gradient for words E[197][9]:	mock grad = 0.008494969776146055, computed grad = 0.008494969757400194
Iteration: 1838, Checking gradient for hidden W[243][2]:	mock grad = -0.010656469423331316, computed grad = -0.010656469467698807
Iteration: 1838, Checking gradient for hidden b[0][2]:	mock grad = 0.057652216064768558, computed grad = 0.057652247592269161
Iteration: 1838, Checking gradient for output layer W[25][0]:	mock grad = -0.051772479984141251, computed grad = -0.051772479904564524
current: 40, Cost = 0.218717, Correct(%) = 1, time = 2.55632
Iteration: 1839, Checking gradient for words E[546][34]:	mock grad = -0.002655031730247615, computed grad = -0.003448082482262629
Iteration: 1839, Checking gradient for hidden W[110][7]:	mock grad = -0.001652501862264844, computed grad = -0.001652501860093798
Iteration: 1839, Checking gradient for hidden b[0][48]:	mock grad = -0.058831694088809505, computed grad = -0.058831710899438380
Iteration: 1839, Checking gradient for output layer W[62][0]:	mock grad = 0.075414486133351666, computed grad = 0.075414485229735401
Iteration: 1840, Checking gradient for words E[42][26]:	mock grad = 0.009658147262989192, computed grad = 0.009658147290577646
Iteration: 1840, Checking gradient for hidden W[246][24]:	mock grad = 0.002514965511457579, computed grad = 0.002514965495767570
Iteration: 1840, Checking gradient for hidden b[0][17]:	mock grad = -0.011951548613192697, computed grad = -0.011951551714898399
Iteration: 1840, Checking gradient for output layer W[114][1]:	mock grad = 0.074744306510679537, computed grad = 0.074744305696621577
Iteration: 1841, Checking gradient for words E[225][20]:	mock grad = -0.042531355295083451, computed grad = -0.042531355384413334
Iteration: 1841, Checking gradient for hidden W[121][35]:	mock grad = 0.002382828001662451, computed grad = 0.002382828020728974
Iteration: 1841, Checking gradient for hidden b[0][9]:	mock grad = -0.052677093762526495, computed grad = -0.052677092061425117
Iteration: 1841, Checking gradient for output layer W[4][1]:	mock grad = 0.021979471022365304, computed grad = 0.021979471018793637
Iteration: 1842, Checking gradient for words E[278][0]:	mock grad = -0.011733618935361179, computed grad = -0.012378800689136259
Iteration: 1842, Checking gradient for hidden W[183][29]:	mock grad = -0.008565025419682293, computed grad = -0.008565025449361743
Iteration: 1842, Checking gradient for hidden b[0][23]:	mock grad = 0.128026804015135554, computed grad = 0.128026816105146823
Iteration: 1842, Checking gradient for output layer W[1][0]:	mock grad = -0.010837360280069319, computed grad = -0.010837360279476876
Iteration: 1843, Checking gradient for words E[216][11]:	mock grad = -0.009571731116791993, computed grad = -0.009571731139315195
Iteration: 1843, Checking gradient for hidden W[115][35]:	mock grad = -0.002985048406900503, computed grad = -0.002985048426789118
Iteration: 1843, Checking gradient for hidden b[0][22]:	mock grad = 0.045302215111758803, computed grad = 0.045302223180380873
Iteration: 1843, Checking gradient for output layer W[49][0]:	mock grad = -0.028790136526929411, computed grad = -0.028790136518698245
Iteration: 1844, Checking gradient for words E[48][1]:	mock grad = 0.012661429441729322, computed grad = 0.012854862256941997
Iteration: 1844, Checking gradient for hidden W[64][20]:	mock grad = -0.003600122102223602, computed grad = -0.003600122117826916
Iteration: 1844, Checking gradient for hidden b[0][46]:	mock grad = -0.075838929574564840, computed grad = -0.075838944091987645
Iteration: 1844, Checking gradient for output layer W[128][1]:	mock grad = 0.033286094582571368, computed grad = 0.033286094473258122
Iteration: 1845, Checking gradient for words E[194][29]:	mock grad = 0.042770540221420683, computed grad = 0.042770540304107249
Iteration: 1845, Checking gradient for hidden W[41][3]:	mock grad = 0.010140496221028306, computed grad = 0.010140496442213849
Iteration: 1845, Checking gradient for hidden b[0][48]:	mock grad = 0.080194341714162620, computed grad = 0.080194346779969672
Iteration: 1845, Checking gradient for output layer W[82][1]:	mock grad = 0.145955254166635351, computed grad = 0.145955252054714740
Iteration: 1846, Checking gradient for words E[48][14]:	mock grad = 0.011097356695910499, computed grad = 0.011097356767521046
Iteration: 1846, Checking gradient for hidden W[152][28]:	mock grad = 0.000429031847701267, computed grad = 0.000429031868800856
Iteration: 1846, Checking gradient for hidden b[0][43]:	mock grad = -0.007236498802937197, computed grad = -0.007236500758529453
Iteration: 1846, Checking gradient for output layer W[11][1]:	mock grad = 0.018270947449244845, computed grad = 0.018270947437127299
Iteration: 1847, Checking gradient for words E[33][49]:	mock grad = 0.017776011632830713, computed grad = 0.017776011678495716
Iteration: 1847, Checking gradient for hidden W[97][34]:	mock grad = 0.007343719051366993, computed grad = 0.007343719141507506
Iteration: 1847, Checking gradient for hidden b[0][47]:	mock grad = 0.050374128473640312, computed grad = 0.050374138054876706
Iteration: 1847, Checking gradient for output layer W[41][1]:	mock grad = 0.041283474244174778, computed grad = 0.041283474145313435
Iteration: 1848, Checking gradient for words E[405][13]:	mock grad = -0.001883913315028907, computed grad = -0.001883913315710511
Iteration: 1848, Checking gradient for hidden W[84][47]:	mock grad = -0.005301746854799450, computed grad = -0.005301746834542986
Iteration: 1848, Checking gradient for hidden b[0][15]:	mock grad = -0.062155306462974291, computed grad = -0.062155319899828969
Iteration: 1848, Checking gradient for output layer W[16][0]:	mock grad = 0.026304552673106496, computed grad = 0.026304552643295284
current: 50, Cost = 0.2169, Correct(%) = 1, time = 3.44108
Iteration: 1849, Checking gradient for words E[98][40]:	mock grad = 0.008710184517507491, computed grad = 0.008710184529034300
Iteration: 1849, Checking gradient for hidden W[75][36]:	mock grad = 0.003993148006642411, computed grad = 0.003993148033461617
Iteration: 1849, Checking gradient for hidden b[0][12]:	mock grad = 0.045092319609391085, computed grad = 0.045092333149135917
Iteration: 1849, Checking gradient for output layer W[55][0]:	mock grad = -0.103680216472418207, computed grad = -0.103680214073213262
Iteration: 1850, Checking gradient for words E[304][49]:	mock grad = 0.015710646926514471, computed grad = 0.015710646894874548
Iteration: 1850, Checking gradient for hidden W[132][2]:	mock grad = -0.018943200180004949, computed grad = -0.018943200273289409
Iteration: 1850, Checking gradient for hidden b[0][16]:	mock grad = 0.018479082645739275, computed grad = 0.018479088354072026
Iteration: 1850, Checking gradient for output layer W[43][1]:	mock grad = -0.104514955860057768, computed grad = -0.104514955378079130
Iteration: 1851, Checking gradient for words E[54][45]:	mock grad = 0.003585246007298259, computed grad = 0.003585246018980498
Iteration: 1851, Checking gradient for hidden W[137][18]:	mock grad = -0.005630083506819039, computed grad = -0.005630083512214440
Iteration: 1851, Checking gradient for hidden b[0][30]:	mock grad = 0.000698110075803005, computed grad = 0.000698108427465373
Iteration: 1851, Checking gradient for output layer W[4][1]:	mock grad = -0.027741345785120552, computed grad = -0.027741345765724269
Iteration: 1852, Checking gradient for words E[119][46]:	mock grad = -0.011255844539009852, computed grad = -0.011255844537693609
Iteration: 1852, Checking gradient for hidden W[146][36]:	mock grad = -0.008027828339113308, computed grad = -0.008027828382424964
Iteration: 1852, Checking gradient for hidden b[0][30]:	mock grad = 0.004712054318456449, computed grad = 0.004712056804383417
Iteration: 1852, Checking gradient for output layer W[25][0]:	mock grad = 0.073094054387906615, computed grad = 0.073094054070013986
Iteration: 1853, Checking gradient for words E[442][32]:	mock grad = 0.005884376723275242, computed grad = 0.005884376743922121
Iteration: 1853, Checking gradient for hidden W[30][48]:	mock grad = -0.005810365414166130, computed grad = -0.005810365395086629
Iteration: 1853, Checking gradient for hidden b[0][40]:	mock grad = 0.075360785119882312, computed grad = 0.075360804465624709
Iteration: 1853, Checking gradient for output layer W[95][0]:	mock grad = -0.072597224451020903, computed grad = -0.072597223983993472
Iteration: 1854, Checking gradient for words E[48][19]:	mock grad = -0.002048459156786531, computed grad = -0.002048459154814724
Iteration: 1854, Checking gradient for hidden W[141][15]:	mock grad = 0.008695941448533251, computed grad = 0.008695941484056184
Iteration: 1854, Checking gradient for hidden b[0][48]:	mock grad = -0.050341098807221707, computed grad = -0.050341109571032494
Iteration: 1854, Checking gradient for output layer W[137][0]:	mock grad = -0.078436866200445876, computed grad = -0.078436865099478892
Iteration: 1855, Checking gradient for words E[513][34]:	mock grad = -0.019136581892079119, computed grad = -0.019136581910016374
Iteration: 1855, Checking gradient for hidden W[94][12]:	mock grad = -0.002000562848381016, computed grad = -0.002000562861770512
Iteration: 1855, Checking gradient for hidden b[0][3]:	mock grad = 0.026911744731225595, computed grad = 0.026911748280289267
Iteration: 1855, Checking gradient for output layer W[38][0]:	mock grad = -0.002283928084143017, computed grad = -0.002283928084105259
Iteration: 1856, Checking gradient for words E[49][29]:	mock grad = -0.003982225916965287, computed grad = -0.003982225901730942
Iteration: 1856, Checking gradient for hidden W[106][25]:	mock grad = 0.002085273509561114, computed grad = 0.002085273520177737
Iteration: 1856, Checking gradient for hidden b[0][12]:	mock grad = -0.079324405608677262, computed grad = -0.079324428408221695
Iteration: 1856, Checking gradient for output layer W[136][1]:	mock grad = 0.099917908128105415, computed grad = 0.099917907866413677
Iteration: 1857, Checking gradient for words E[640][8]:	mock grad = -0.006857100332990917, computed grad = -0.006857100353414909
Iteration: 1857, Checking gradient for hidden W[172][1]:	mock grad = -0.008654482254649709, computed grad = -0.008654482250219800
Iteration: 1857, Checking gradient for hidden b[0][27]:	mock grad = 0.028660634193322654, computed grad = 0.028660650808016837
Iteration: 1857, Checking gradient for output layer W[50][1]:	mock grad = 0.145769727608713318, computed grad = 0.145769724261917727
Iteration: 1858, Checking gradient for words E[44][47]:	mock grad = 0.017804074940175330, computed grad = 0.017804074970751871
Iteration: 1858, Checking gradient for hidden W[125][34]:	mock grad = -0.006401017383389496, computed grad = -0.006401017411949993
Iteration: 1858, Checking gradient for hidden b[0][29]:	mock grad = 0.071000504670432063, computed grad = 0.071000529052350492
Iteration: 1858, Checking gradient for output layer W[142][0]:	mock grad = -0.115134178770320839, computed grad = -0.115134177620299136
current: 60, Cost = 0.284833, Correct(%) = 1, time = 4.13417
Iteration: 1859, Checking gradient for words E[649][35]:	mock grad = -0.006735204404723083, computed grad = -0.006735204461928697
Iteration: 1859, Checking gradient for hidden W[141][3]:	mock grad = 0.001609595241064410, computed grad = 0.001609595200756280
Iteration: 1859, Checking gradient for hidden b[0][48]:	mock grad = 0.061946457086237139, computed grad = 0.061946454017355761
Iteration: 1859, Checking gradient for output layer W[111][0]:	mock grad = 0.000106750287193202, computed grad = 0.000106750287219242
Iteration: 1860, Checking gradient for words E[6][14]:	mock grad = -0.014700321863569954, computed grad = -0.014700321925940384
Iteration: 1860, Checking gradient for hidden W[79][39]:	mock grad = 0.005729141745985933, computed grad = 0.005729141804570410
Iteration: 1860, Checking gradient for hidden b[0][41]:	mock grad = -0.033713643643545899, computed grad = -0.033713650609525657
Iteration: 1860, Checking gradient for output layer W[128][1]:	mock grad = -0.077668745888825885, computed grad = -0.077668745632950606
Iteration: 1861, Checking gradient for words E[34][47]:	mock grad = 0.059469312110482786, computed grad = 0.059469311973756753
Iteration: 1861, Checking gradient for hidden W[212][34]:	mock grad = 0.005912359574755666, computed grad = 0.005912359614408008
Iteration: 1861, Checking gradient for hidden b[0][35]:	mock grad = -0.011638491586057897, computed grad = -0.011638491048113979
Iteration: 1861, Checking gradient for output layer W[19][0]:	mock grad = -0.049045402732622589, computed grad = -0.049045402651610191
Iteration: 1862, Checking gradient for words E[16][7]:	mock grad = -0.009234807821112412, computed grad = -0.009234807883905549
Iteration: 1862, Checking gradient for hidden W[159][18]:	mock grad = -0.003084758174815017, computed grad = -0.003084758205534071
Iteration: 1862, Checking gradient for hidden b[0][20]:	mock grad = 0.081957599080556554, computed grad = 0.081957619419780955
Iteration: 1862, Checking gradient for output layer W[84][0]:	mock grad = -0.062685011800123203, computed grad = -0.062685011564736076
Iteration: 1863, Checking gradient for words E[68][19]:	mock grad = -0.021186827171282285, computed grad = -0.021186827197131511
Iteration: 1863, Checking gradient for hidden W[14][28]:	mock grad = 0.000852276481111414, computed grad = 0.000852276417838911
Iteration: 1863, Checking gradient for hidden b[0][11]:	mock grad = 0.073080934066549474, computed grad = 0.073080951877947220
Iteration: 1863, Checking gradient for output layer W[69][0]:	mock grad = 0.054230494719198497, computed grad = 0.054230494567960773
Iteration: 1864, Checking gradient for words E[423][35]:	mock grad = -0.037957426137380512, computed grad = -0.037957426156532484
Iteration: 1864, Checking gradient for hidden W[188][48]:	mock grad = 0.001757961810538999, computed grad = 0.001757961795565387
Iteration: 1864, Checking gradient for hidden b[0][30]:	mock grad = 0.003301347458378734, computed grad = 0.003301348108439905
Iteration: 1864, Checking gradient for output layer W[33][1]:	mock grad = -0.018833606308776840, computed grad = -0.018833606304898273
Iteration: 1865, Checking gradient for words E[118][34]:	mock grad = 0.019209830743410761, computed grad = 0.019209830791406510
Iteration: 1865, Checking gradient for hidden W[84][17]:	mock grad = 0.003295030644487329, computed grad = 0.003295030666689103
Iteration: 1865, Checking gradient for hidden b[0][48]:	mock grad = 0.041739597677822871, computed grad = 0.041739592931209657
Iteration: 1865, Checking gradient for output layer W[35][1]:	mock grad = 0.048785602214745927, computed grad = 0.048785601915043116
Iteration: 1866, Checking gradient for words E[194][32]:	mock grad = -0.014137333360475512, computed grad = -0.014137333335901302
Iteration: 1866, Checking gradient for hidden W[89][45]:	mock grad = -0.003019714433394327, computed grad = -0.003019714498400739
Iteration: 1866, Checking gradient for hidden b[0][1]:	mock grad = -0.059699222618697645, computed grad = -0.059699225705717937
Iteration: 1866, Checking gradient for output layer W[21][1]:	mock grad = 0.006624831649731933, computed grad = 0.006624831649269279
Iteration: 1867, Checking gradient for words E[304][33]:	mock grad = -0.007545209742276793, computed grad = -0.007545209707145345
Iteration: 1867, Checking gradient for hidden W[167][37]:	mock grad = 0.000797181732486996, computed grad = 0.000797181751717673
Iteration: 1867, Checking gradient for hidden b[0][20]:	mock grad = -0.087116388469726802, computed grad = -0.087116415285278345
Iteration: 1867, Checking gradient for output layer W[73][1]:	mock grad = -0.038748428433654780, computed grad = -0.038748428379101203
Iteration: 1868, Checking gradient for words E[166][9]:	mock grad = -0.018390157052142198, computed grad = -0.018390157077456702
Iteration: 1868, Checking gradient for hidden W[123][12]:	mock grad = -0.000446369161744009, computed grad = -0.000446369183207656
Iteration: 1868, Checking gradient for hidden b[0][7]:	mock grad = -0.049163070070218362, computed grad = -0.049163074176935162
Iteration: 1868, Checking gradient for output layer W[134][0]:	mock grad = -0.037230628142800870, computed grad = -0.037230628056631132
current: 70, Cost = 0.224768, Correct(%) = 1, time = 4.74988
Iteration: 1869, Checking gradient for words E[147][33]:	mock grad = -0.034913828577731731, computed grad = -0.034913828482924993
Iteration: 1869, Checking gradient for hidden W[171][33]:	mock grad = -0.003689037744322721, computed grad = -0.003689037747270606
Iteration: 1869, Checking gradient for hidden b[0][40]:	mock grad = -0.063839986085750811, computed grad = -0.063839999688707239
Iteration: 1869, Checking gradient for output layer W[101][1]:	mock grad = 0.058292335017334773, computed grad = 0.058292334628575318
Iteration: 1870, Checking gradient for words E[335][9]:	mock grad = 0.025331599402267369, computed grad = 0.025331599465299934
Iteration: 1870, Checking gradient for hidden W[102][1]:	mock grad = 0.005334161877978971, computed grad = 0.005334161909773060
Iteration: 1870, Checking gradient for hidden b[0][33]:	mock grad = 0.090201624201452502, computed grad = 0.090201643591236613
Iteration: 1870, Checking gradient for output layer W[64][0]:	mock grad = 0.105805731256375379, computed grad = 0.105805730550321983
Iteration: 1871, Checking gradient for words E[58][13]:	mock grad = -0.013091308565055249, computed grad = -0.013091308609279304
Iteration: 1871, Checking gradient for hidden W[195][48]:	mock grad = 0.004363251421668490, computed grad = 0.004363251419360244
Iteration: 1871, Checking gradient for hidden b[0][29]:	mock grad = 0.074865025102843763, computed grad = 0.074865051991178183
Iteration: 1871, Checking gradient for output layer W[148][1]:	mock grad = 0.136644901851612488, computed grad = 0.136644900092918437
Iteration: 1872, Checking gradient for words E[25][16]:	mock grad = -0.014263441693140866, computed grad = -0.014263441648801875
Iteration: 1872, Checking gradient for hidden W[22][44]:	mock grad = -0.011385099616734173, computed grad = -0.011385099618843498
Iteration: 1872, Checking gradient for hidden b[0][25]:	mock grad = -0.006732359663891296, computed grad = -0.006732358511539886
Iteration: 1872, Checking gradient for output layer W[95][1]:	mock grad = -0.070709818318970274, computed grad = -0.070709818111473616
Iteration: 1873, Checking gradient for words E[49][30]:	mock grad = -0.009323510336528562, computed grad = -0.009323510374223135
Iteration: 1873, Checking gradient for hidden W[167][8]:	mock grad = -0.007010341811164444, computed grad = -0.007010341809259597
Iteration: 1873, Checking gradient for hidden b[0][37]:	mock grad = 0.048745928040466735, computed grad = 0.048745933771307100
Iteration: 1873, Checking gradient for output layer W[15][1]:	mock grad = 0.081618735513155904, computed grad = 0.081618735279169422
Iteration: 1874, Checking gradient for words E[25][14]:	mock grad = -0.038743821345665896, computed grad = -0.038666785704461873
Iteration: 1874, Checking gradient for hidden W[120][34]:	mock grad = 0.010939027733025686, computed grad = 0.010939027842451592
Iteration: 1874, Checking gradient for hidden b[0][34]:	mock grad = 0.030268242566938719, computed grad = 0.030268251776166454
Iteration: 1874, Checking gradient for output layer W[70][1]:	mock grad = -0.078602299260099873, computed grad = -0.078602299024427974
Iteration: 1875, Checking gradient for words E[414][4]:	mock grad = 0.003627275938639096, computed grad = 0.003627275960882267
Iteration: 1875, Checking gradient for hidden W[123][18]:	mock grad = 0.014659006428774379, computed grad = 0.014659006592506121
Iteration: 1875, Checking gradient for hidden b[0][30]:	mock grad = -0.001308858022103232, computed grad = -0.001308861454847575
Iteration: 1875, Checking gradient for output layer W[14][0]:	mock grad = -0.002519515836324482, computed grad = -0.002519515836310264
Iteration: 1876, Checking gradient for words E[150][28]:	mock grad = -0.022139476142379877, computed grad = -0.022139476179036714
Iteration: 1876, Checking gradient for hidden W[233][45]:	mock grad = -0.001368290143577422, computed grad = -0.001368290143250443
Iteration: 1876, Checking gradient for hidden b[0][46]:	mock grad = 0.109917813707949286, computed grad = 0.109917836186075007
Iteration: 1876, Checking gradient for output layer W[28][1]:	mock grad = 0.024124401429237441, computed grad = 0.024124401415982395
Iteration: 1877, Checking gradient for words E[397][9]:	mock grad = 0.006973225971637631, computed grad = 0.006973225906984088
Iteration: 1877, Checking gradient for hidden W[17][42]:	mock grad = 0.011846311331331627, computed grad = 0.011846311477927801
Iteration: 1877, Checking gradient for hidden b[0][42]:	mock grad = -0.044251993323890515, computed grad = -0.044252006185857978
Iteration: 1877, Checking gradient for output layer W[3][1]:	mock grad = 0.015030067666801927, computed grad = 0.015030067660463272
Iteration: 1878, Checking gradient for words E[215][6]:	mock grad = 0.006304798557787983, computed grad = 0.006130183922720904
Iteration: 1878, Checking gradient for hidden W[126][24]:	mock grad = -0.000061673768703141, computed grad = -0.000061673769160375
Iteration: 1878, Checking gradient for hidden b[0][39]:	mock grad = -0.070692187443488486, computed grad = -0.070692216058054869
Iteration: 1878, Checking gradient for output layer W[70][1]:	mock grad = -0.088895248958803519, computed grad = -0.088895248315759123
current: 80, Cost = 0.244065, Correct(%) = 1, time = 5.23364
Iteration: 1879, Checking gradient for words E[156][28]:	mock grad = -0.007337907933682719, computed grad = -0.007337907956813010
Iteration: 1879, Checking gradient for hidden W[39][3]:	mock grad = -0.000474375669634597, computed grad = -0.000474375665120268
Iteration: 1879, Checking gradient for hidden b[0][18]:	mock grad = 0.053496090175977895, computed grad = 0.053496102144914452
Iteration: 1879, Checking gradient for output layer W[52][1]:	mock grad = 0.023064845068240802, computed grad = 0.023064845048838802
Iteration: 1880, Checking gradient for words E[48][46]:	mock grad = 0.014515255811448080, computed grad = 0.014515255838847949
Iteration: 1880, Checking gradient for hidden W[4][2]:	mock grad = 0.006220173887072633, computed grad = 0.006220173876296866
Iteration: 1880, Checking gradient for hidden b[0][36]:	mock grad = 0.000318614792355065, computed grad = 0.000318606184761934
Iteration: 1880, Checking gradient for output layer W[142][1]:	mock grad = 0.114230300373135973, computed grad = 0.114230298978839920
Iteration: 1881, Checking gradient for words E[708][38]:	mock grad = 0.005846272470438896, computed grad = 0.006064375780224575
Iteration: 1881, Checking gradient for hidden W[208][49]:	mock grad = -0.006363412305504523, computed grad = -0.006363412360844947
Iteration: 1881, Checking gradient for hidden b[0][17]:	mock grad = 0.013666059663658414, computed grad = 0.013666062675606659
Iteration: 1881, Checking gradient for output layer W[141][0]:	mock grad = 0.095684405583940091, computed grad = 0.095684404395328201
Iteration: 1882, Checking gradient for words E[438][17]:	mock grad = -0.003081485130701500, computed grad = -0.003379521573313404
Iteration: 1882, Checking gradient for hidden W[66][48]:	mock grad = 0.004702459493050037, computed grad = 0.004702459491526484
Iteration: 1882, Checking gradient for hidden b[0][0]:	mock grad = -0.125697098001587015, computed grad = -0.125697131897373027
Iteration: 1882, Checking gradient for output layer W[137][0]:	mock grad = 0.103764340402906496, computed grad = 0.103764339039500483
Iteration: 1883, Checking gradient for words E[217][28]:	mock grad = -0.001024437924379562, computed grad = -0.001312049196899092
Iteration: 1883, Checking gradient for hidden W[49][11]:	mock grad = -0.004173430084547514, computed grad = -0.004173430097710177
Iteration: 1883, Checking gradient for hidden b[0][46]:	mock grad = 0.101900356856393648, computed grad = 0.101900360236025056
Iteration: 1883, Checking gradient for output layer W[24][0]:	mock grad = -0.039530316555402845, computed grad = -0.039530316508840715
Iteration: 1884, Checking gradient for words E[561][11]:	mock grad = -0.014570597862154089, computed grad = -0.014570597887645189
Iteration: 1884, Checking gradient for hidden W[137][41]:	mock grad = 0.003921013428651032, computed grad = 0.003921013438517147
Iteration: 1884, Checking gradient for hidden b[0][43]:	mock grad = -0.007666428886277110, computed grad = -0.007666430947536946
Iteration: 1884, Checking gradient for output layer W[122][1]:	mock grad = -0.114893558634476811, computed grad = -0.114893556172461733
Iteration: 1885, Checking gradient for words E[166][30]:	mock grad = 0.017188776497603642, computed grad = 0.017188776511598031
Iteration: 1885, Checking gradient for hidden W[12][35]:	mock grad = -0.000951652446817830, computed grad = -0.000951652444745129
Iteration: 1885, Checking gradient for hidden b[0][45]:	mock grad = 0.003426159469799295, computed grad = 0.003426157581479424
Iteration: 1885, Checking gradient for output layer W[22][0]:	mock grad = -0.021227198122891755, computed grad = -0.021227198107587907
Iteration: 1886, Checking gradient for words E[447][9]:	mock grad = 0.022978514992055832, computed grad = 0.023704282792422999
Iteration: 1886, Checking gradient for hidden W[183][2]:	mock grad = -0.004859623744635222, computed grad = -0.004859623759111934
Iteration: 1886, Checking gradient for hidden b[0][38]:	mock grad = 0.052686652788291877, computed grad = 0.052686666119811758
Iteration: 1886, Checking gradient for output layer W[147][0]:	mock grad = -0.009514808504418326, computed grad = -0.009514808503484897
Iteration: 1887, Checking gradient for words E[48][23]:	mock grad = 0.040501153297900494, computed grad = 0.040501153264417049
Iteration: 1887, Checking gradient for hidden W[86][37]:	mock grad = 0.012865705402098637, computed grad = 0.012865705466604094
Iteration: 1887, Checking gradient for hidden b[0][17]:	mock grad = 0.013164754259303857, computed grad = 0.013164755349108305
Iteration: 1887, Checking gradient for output layer W[80][0]:	mock grad = -0.073644074498918632, computed grad = -0.073644074095427045
Iteration: 1888, Checking gradient for words E[124][36]:	mock grad = 0.001379004443136100, computed grad = 0.001379004451758663
Iteration: 1888, Checking gradient for hidden W[34][38]:	mock grad = 0.005263724353693489, computed grad = 0.005263724358891248
Iteration: 1888, Checking gradient for hidden b[0][45]:	mock grad = 0.005752444403483992, computed grad = 0.005752444320664261
Iteration: 1888, Checking gradient for output layer W[87][1]:	mock grad = -0.030507998209250431, computed grad = -0.030507998175944583
current: 90, Cost = 0.283177, Correct(%) = 1, time = 6.17778
Iteration: 1889, Checking gradient for words E[776][5]:	mock grad = 0.001279249674718264, computed grad = 0.001279249695049463
Iteration: 1889, Checking gradient for hidden W[45][10]:	mock grad = 0.003459928942489698, computed grad = 0.003459928953019551
Iteration: 1889, Checking gradient for hidden b[0][24]:	mock grad = 0.076171794914148272, computed grad = 0.076171817216708820
Iteration: 1889, Checking gradient for output layer W[87][0]:	mock grad = 0.025273275806841911, computed grad = 0.025273275789970778
Iteration: 1890, Checking gradient for words E[330][46]:	mock grad = 0.000011131988109092, computed grad = -0.000404862049653869
Iteration: 1890, Checking gradient for hidden W[223][46]:	mock grad = -0.018907646308796711, computed grad = -0.018907646589928769
Iteration: 1890, Checking gradient for hidden b[0][42]:	mock grad = -0.040384402650792128, computed grad = -0.040384406533361165
Iteration: 1890, Checking gradient for output layer W[89][1]:	mock grad = 0.098788188789458720, computed grad = 0.098788187332377625
Iteration: 1891, Checking gradient for words E[84][34]:	mock grad = -0.004620273401007013, computed grad = -0.004620273386962923
Iteration: 1891, Checking gradient for hidden W[79][26]:	mock grad = -0.004492916079207321, computed grad = -0.004492916150998763
Iteration: 1891, Checking gradient for hidden b[0][8]:	mock grad = 0.026613629079852430, computed grad = 0.026613646468462294
Iteration: 1891, Checking gradient for output layer W[81][0]:	mock grad = -0.063581995038253236, computed grad = -0.063581994832062644
Iteration: 1892, Checking gradient for words E[486][21]:	mock grad = 0.004325259987877916, computed grad = 0.004325259980695910
Iteration: 1892, Checking gradient for hidden W[47][3]:	mock grad = 0.000829415216452301, computed grad = 0.000829415211911238
Iteration: 1892, Checking gradient for hidden b[0][18]:	mock grad = 0.044688412424304103, computed grad = 0.044688420470487170
Iteration: 1892, Checking gradient for output layer W[89][0]:	mock grad = -0.103562103943577455, computed grad = -0.103562101408659027
Iteration: 1893, Checking gradient for words E[331][29]:	mock grad = 0.010993984362828169, computed grad = 0.010993984373953672
Iteration: 1893, Checking gradient for hidden W[206][45]:	mock grad = 0.002106417329478250, computed grad = 0.002106417343598174
Iteration: 1893, Checking gradient for hidden b[0][47]:	mock grad = 0.055377793346161885, computed grad = 0.055377801588238719
Iteration: 1893, Checking gradient for output layer W[55][1]:	mock grad = -0.113607059515685727, computed grad = -0.113607058029417840
Iteration: 1894, Checking gradient for words E[269][45]:	mock grad = 0.010168379137365235, computed grad = 0.010168379108775532
Iteration: 1894, Checking gradient for hidden W[32][32]:	mock grad = 0.005245565788514206, computed grad = 0.005245565815862684
Iteration: 1894, Checking gradient for hidden b[0][40]:	mock grad = -0.059107409864325722, computed grad = -0.059107420390410836
Iteration: 1894, Checking gradient for output layer W[63][1]:	mock grad = -0.039149912981054080, computed grad = -0.039149912853865189
Iteration: 1895, Checking gradient for words E[49][3]:	mock grad = 0.004045624185239349, computed grad = 0.004045624173772413
Iteration: 1895, Checking gradient for hidden W[124][13]:	mock grad = 0.007674157973258655, computed grad = 0.007674158150738055
Iteration: 1895, Checking gradient for hidden b[0][11]:	mock grad = 0.079175139302889441, computed grad = 0.079175159658506286
Iteration: 1895, Checking gradient for output layer W[104][1]:	mock grad = 0.066164586752182064, computed grad = 0.066164586533158931
Iteration: 1896, Checking gradient for words E[132][33]:	mock grad = 0.001802595575500776, computed grad = 0.001802595531100967
Iteration: 1896, Checking gradient for hidden W[69][5]:	mock grad = 0.009336156878120283, computed grad = 0.009336156885894731
Iteration: 1896, Checking gradient for hidden b[0][26]:	mock grad = -0.022083446767906523, computed grad = -0.022083450211739075
Iteration: 1896, Checking gradient for output layer W[19][0]:	mock grad = 0.018714455295887333, computed grad = 0.018714455286917734
Iteration: 1897, Checking gradient for words E[186][36]:	mock grad = -0.006419652026107281, computed grad = -0.006419651972712295
Iteration: 1897, Checking gradient for hidden W[25][33]:	mock grad = 0.014025479030316257, computed grad = 0.014025479140028770
Iteration: 1897, Checking gradient for hidden b[0][36]:	mock grad = 0.006046823633865106, computed grad = 0.006046816911455997
Iteration: 1897, Checking gradient for output layer W[104][1]:	mock grad = 0.031019167954782212, computed grad = 0.031019167943497204
Iteration: 1898, Checking gradient for words E[550][21]:	mock grad = -0.008655656393297129, computed grad = -0.008655656376344443
Iteration: 1898, Checking gradient for hidden W[206][31]:	mock grad = 0.012433221361213276, computed grad = 0.012433221579270362
Iteration: 1898, Checking gradient for hidden b[0][44]:	mock grad = 0.002629836978268463, computed grad = 0.002629843724921005
Iteration: 1898, Checking gradient for output layer W[56][1]:	mock grad = -0.100887257034984468, computed grad = -0.100887255454776684
current: 100, Cost = 0.286366, Correct(%) = 1, time = 6.95978
Iteration: 1899, Checking gradient for words E[118][23]:	mock grad = 0.001741578201736038, computed grad = 0.001741578194696572
Iteration: 1899, Checking gradient for hidden W[28][22]:	mock grad = -0.004205725015887785, computed grad = -0.004205725071450973
Iteration: 1899, Checking gradient for hidden b[0][31]:	mock grad = -0.024099967291968394, computed grad = -0.024099966715640270
Iteration: 1899, Checking gradient for output layer W[93][1]:	mock grad = 0.015841431826812968, computed grad = 0.015841431822788458
current: 19, Correct(%) = 1, time = 7.01095
Dev start.
Dev finished. Total time taken is: 0.637006
dev:
Accuracy:	P=100/100=1
Test start.
Test finished. Total time taken is: 0.545507
test:
Accuracy:	P=63/100=0.63
##### Iteration 19
random: 0, 99
Iteration: 1900, Checking gradient for words E[10][41]:	mock grad = 0.007153009343605077, computed grad = 0.008063411330075258
Iteration: 1900, Checking gradient for hidden W[121][1]:	mock grad = 0.008028568587795104, computed grad = 0.008028568559193613
Iteration: 1900, Checking gradient for hidden b[0][48]:	mock grad = 0.046446064721483715, computed grad = 0.046446063114216132
Iteration: 1900, Checking gradient for output layer W[10][0]:	mock grad = 0.010411156255579757, computed grad = 0.010411156252612505
Iteration: 1901, Checking gradient for words E[92][40]:	mock grad = 0.008632214893083967, computed grad = 0.008632214917622876
Iteration: 1901, Checking gradient for hidden W[42][37]:	mock grad = 0.005059732360673808, computed grad = 0.005059732371771105
Iteration: 1901, Checking gradient for hidden b[0][24]:	mock grad = -0.064941904920901017, computed grad = -0.064941906119188569
Iteration: 1901, Checking gradient for output layer W[98][1]:	mock grad = 0.026325169368540680, computed grad = 0.026325169345478263
Iteration: 1902, Checking gradient for words E[114][36]:	mock grad = 0.029943567114554259, computed grad = 0.029943567154868865
Iteration: 1902, Checking gradient for hidden W[217][29]:	mock grad = 0.004823308002874227, computed grad = 0.004823308016238316
Iteration: 1902, Checking gradient for hidden b[0][12]:	mock grad = -0.051059929348312716, computed grad = -0.051059945851029853
Iteration: 1902, Checking gradient for output layer W[109][0]:	mock grad = -0.029076669004618472, computed grad = -0.029076668966605032
Iteration: 1903, Checking gradient for words E[97][26]:	mock grad = 0.014816407748058369, computed grad = 0.014816407830381639
Iteration: 1903, Checking gradient for hidden W[75][31]:	mock grad = -0.006259857024126791, computed grad = -0.006259857067821439
Iteration: 1903, Checking gradient for hidden b[0][22]:	mock grad = -0.034872546474357113, computed grad = -0.034872552451073381
Iteration: 1903, Checking gradient for output layer W[50][1]:	mock grad = -0.156271198851115622, computed grad = -0.156271195648988170
Iteration: 1904, Checking gradient for words E[174][3]:	mock grad = -0.007673626971127501, computed grad = -0.007673626971998181
Iteration: 1904, Checking gradient for hidden W[144][3]:	mock grad = -0.003960589668117187, computed grad = -0.003960589669463336
Iteration: 1904, Checking gradient for hidden b[0][25]:	mock grad = -0.003922641381687297, computed grad = -0.003922638054696503
Iteration: 1904, Checking gradient for output layer W[2][1]:	mock grad = 0.046103602921343523, computed grad = 0.046103602820832104
Iteration: 1905, Checking gradient for words E[185][7]:	mock grad = 0.001044992460008665, computed grad = 0.001044992463279166
Iteration: 1905, Checking gradient for hidden W[114][3]:	mock grad = -0.007700944321259451, computed grad = -0.007700944316519339
Iteration: 1905, Checking gradient for hidden b[0][39]:	mock grad = -0.067834053455922971, computed grad = -0.067834081189606862
Iteration: 1905, Checking gradient for output layer W[12][0]:	mock grad = 0.036466312301447479, computed grad = 0.036466312253161555
Iteration: 1906, Checking gradient for words E[275][47]:	mock grad = 0.011644848172162603, computed grad = 0.011644848164071995
Iteration: 1906, Checking gradient for hidden W[243][21]:	mock grad = 0.001672585771300694, computed grad = 0.001672585774741826
Iteration: 1906, Checking gradient for hidden b[0][33]:	mock grad = 0.060238019824118694, computed grad = 0.060238034240961282
Iteration: 1906, Checking gradient for output layer W[112][0]:	mock grad = -0.004397460719354762, computed grad = -0.004397460719101975
Iteration: 1907, Checking gradient for words E[6][40]:	mock grad = 0.012162188713610300, computed grad = 0.012162188654094888
Iteration: 1907, Checking gradient for hidden W[213][40]:	mock grad = -0.010146445708153484, computed grad = -0.010146445728820392
Iteration: 1907, Checking gradient for hidden b[0][36]:	mock grad = -0.007558497236603845, computed grad = -0.007558491849243659
Iteration: 1907, Checking gradient for output layer W[38][0]:	mock grad = 0.023869750788552135, computed grad = 0.023869750784536851
Iteration: 1908, Checking gradient for words E[225][8]:	mock grad = 0.015764095801479305, computed grad = 0.015764095779193098
Iteration: 1908, Checking gradient for hidden W[161][19]:	mock grad = 0.004841809687489151, computed grad = 0.004841809687879670
Iteration: 1908, Checking gradient for hidden b[0][5]:	mock grad = 0.010876208045407232, computed grad = 0.010876206274415540
Iteration: 1908, Checking gradient for output layer W[101][0]:	mock grad = -0.065176555964607807, computed grad = -0.065176555592120211
current: 10, Cost = 0.295906, Correct(%) = 1, time = 0.768589
Iteration: 1909, Checking gradient for words E[199][8]:	mock grad = -0.007859046579544726, computed grad = -0.007859046600731362
Iteration: 1909, Checking gradient for hidden W[102][0]:	mock grad = -0.014317265016244862, computed grad = -0.014317265181380375
Iteration: 1909, Checking gradient for hidden b[0][43]:	mock grad = -0.009362702108078880, computed grad = -0.009362704803961050
Iteration: 1909, Checking gradient for output layer W[42][1]:	mock grad = -0.061620603333617519, computed grad = -0.061620603118000640
Iteration: 1910, Checking gradient for words E[322][30]:	mock grad = 0.001986494934574923, computed grad = 0.001986494935446963
Iteration: 1910, Checking gradient for hidden W[155][2]:	mock grad = -0.007065615344720166, computed grad = -0.007065615359907036
Iteration: 1910, Checking gradient for hidden b[0][14]:	mock grad = 0.044133129797369053, computed grad = 0.044133144863873097
Iteration: 1910, Checking gradient for output layer W[127][1]:	mock grad = 0.108235524806890582, computed grad = 0.108235522490918251
Iteration: 1911, Checking gradient for words E[335][9]:	mock grad = -0.005183940422093247, computed grad = -0.005183940452350412
Iteration: 1911, Checking gradient for hidden W[38][45]:	mock grad = -0.004058293152864323, computed grad = -0.004058293167255847
Iteration: 1911, Checking gradient for hidden b[0][6]:	mock grad = 0.036477074440682822, computed grad = 0.036477086377877818
Iteration: 1911, Checking gradient for output layer W[72][1]:	mock grad = 0.032916780977526594, computed grad = 0.032916780959335319
Iteration: 1912, Checking gradient for words E[776][9]:	mock grad = -0.000539627206752602, computed grad = -0.000539627206876244
Iteration: 1912, Checking gradient for hidden W[202][15]:	mock grad = -0.010244135534451249, computed grad = -0.010244135569536532
Iteration: 1912, Checking gradient for hidden b[0][47]:	mock grad = 0.048871456559310134, computed grad = 0.048871464090344964
Iteration: 1912, Checking gradient for output layer W[86][1]:	mock grad = -0.051339318409532320, computed grad = -0.051339318196251253
Iteration: 1913, Checking gradient for words E[102][3]:	mock grad = 0.000679239262446618, computed grad = 0.000679239291251355
Iteration: 1913, Checking gradient for hidden W[109][1]:	mock grad = -0.004328209291595830, computed grad = -0.004328209339965474
Iteration: 1913, Checking gradient for hidden b[0][2]:	mock grad = 0.036848793601010343, computed grad = 0.036848813247328194
Iteration: 1913, Checking gradient for output layer W[85][1]:	mock grad = 0.082512173402729538, computed grad = 0.082512172375223389
Iteration: 1914, Checking gradient for words E[34][32]:	mock grad = 0.005827454527679299, computed grad = 0.005827454987916002
Iteration: 1914, Checking gradient for hidden W[104][26]:	mock grad = -0.012565067310094102, computed grad = -0.012565067692684759
Iteration: 1914, Checking gradient for hidden b[0][45]:	mock grad = 0.010834219270616785, computed grad = 0.010834221293382141
Iteration: 1914, Checking gradient for output layer W[70][1]:	mock grad = -0.129378032156346467, computed grad = -0.129378031725576242
Iteration: 1915, Checking gradient for words E[383][41]:	mock grad = -0.004407713265291990, computed grad = -0.004407713256369424
Iteration: 1915, Checking gradient for hidden W[228][33]:	mock grad = 0.003418930847154389, computed grad = 0.003418930856585056
Iteration: 1915, Checking gradient for hidden b[0][15]:	mock grad = -0.072331411851173089, computed grad = -0.072331424560074339
Iteration: 1915, Checking gradient for output layer W[35][1]:	mock grad = 0.056633416057938479, computed grad = 0.056633415869316486
Iteration: 1916, Checking gradient for words E[776][32]:	mock grad = -0.016988722477712104, computed grad = -0.017687932303336377
Iteration: 1916, Checking gradient for hidden W[233][14]:	mock grad = 0.000856637192181431, computed grad = 0.000856637150025406
Iteration: 1916, Checking gradient for hidden b[0][18]:	mock grad = -0.055851430885767162, computed grad = -0.055851423150288571
Iteration: 1916, Checking gradient for output layer W[129][0]:	mock grad = -0.126293556757250958, computed grad = -0.126293555149435965
Iteration: 1917, Checking gradient for words E[119][45]:	mock grad = 0.006318892080178773, computed grad = 0.006318892096563264
Iteration: 1917, Checking gradient for hidden W[155][42]:	mock grad = -0.006445410503333471, computed grad = -0.006445410555754719
Iteration: 1917, Checking gradient for hidden b[0][29]:	mock grad = 0.048738127836608913, computed grad = 0.048738145244104518
Iteration: 1917, Checking gradient for output layer W[29][1]:	mock grad = 0.021369645607921539, computed grad = 0.021369645582938017
Iteration: 1918, Checking gradient for words E[118][16]:	mock grad = -0.017383639244611393, computed grad = -0.017383639292031933
Iteration: 1918, Checking gradient for hidden W[45][9]:	mock grad = -0.002156142720005638, computed grad = -0.008870535968984307
Iteration: 1918, Checking gradient for hidden b[0][41]:	mock grad = 0.023913695393507073, computed grad = 0.023913701113002450
Iteration: 1918, Checking gradient for output layer W[126][1]:	mock grad = 0.125751415045449733, computed grad = 0.125751411553901044
current: 20, Cost = 0.40538, Correct(%) = 1, time = 1.42518
Iteration: 1919, Checking gradient for words E[119][40]:	mock grad = 0.004898694901722678, computed grad = 0.004898694865942256
Iteration: 1919, Checking gradient for hidden W[160][44]:	mock grad = 0.019828074116629013, computed grad = 0.019828074306394688
Iteration: 1919, Checking gradient for hidden b[0][25]:	mock grad = 0.000209092501674002, computed grad = 0.000209084821851155
Iteration: 1919, Checking gradient for output layer W[138][1]:	mock grad = -0.111273041264847716, computed grad = -0.111273040805317566
Iteration: 1920, Checking gradient for words E[315][23]:	mock grad = 0.001678016930323212, computed grad = 0.001678016981358867
Iteration: 1920, Checking gradient for hidden W[212][4]:	mock grad = -0.003855890666398798, computed grad = -0.003855890675744419
Iteration: 1920, Checking gradient for hidden b[0][2]:	mock grad = -0.046702371080120653, computed grad = -0.046702386283258839
Iteration: 1920, Checking gradient for output layer W[100][0]:	mock grad = -0.069448523934034778, computed grad = -0.069448523548418128
Iteration: 1921, Checking gradient for words E[776][9]:	mock grad = 0.009968125526504945, computed grad = 0.009968125571823047
Iteration: 1921, Checking gradient for hidden W[237][37]:	mock grad = -0.000592306198143877, computed grad = -0.000592306202525328
Iteration: 1921, Checking gradient for hidden b[0][27]:	mock grad = -0.041125162817873262, computed grad = -0.041125182223048978
Iteration: 1921, Checking gradient for output layer W[45][1]:	mock grad = 0.016349977708857155, computed grad = 0.016349977707781908
Iteration: 1922, Checking gradient for words E[351][35]:	mock grad = 0.001531852992731064, computed grad = 0.001531853022667892
Iteration: 1922, Checking gradient for hidden W[120][13]:	mock grad = -0.001832595811393500, computed grad = -0.001832595815065966
Iteration: 1922, Checking gradient for hidden b[0][19]:	mock grad = 0.028133460171553271, computed grad = 0.028133458957350206
Iteration: 1922, Checking gradient for output layer W[77][0]:	mock grad = 0.076090330265052297, computed grad = 0.076090329534657150
Iteration: 1923, Checking gradient for words E[459][0]:	mock grad = -0.000676302201130241, computed grad = -0.000676302199858530
Iteration: 1923, Checking gradient for hidden W[245][25]:	mock grad = 0.006939357213714414, computed grad = 0.006939357246387576
Iteration: 1923, Checking gradient for hidden b[0][18]:	mock grad = -0.047644940867563879, computed grad = -0.047644930780921579
Iteration: 1923, Checking gradient for output layer W[123][0]:	mock grad = -0.142908528747598407, computed grad = -0.142908525474390424
Iteration: 1924, Checking gradient for words E[148][30]:	mock grad = -0.019326957198584349, computed grad = -0.019326957215484688
Iteration: 1924, Checking gradient for hidden W[6][34]:	mock grad = -0.006059282968789992, computed grad = -0.006059282973532609
Iteration: 1924, Checking gradient for hidden b[0][20]:	mock grad = 0.084316626591723898, computed grad = 0.084316648853463586
Iteration: 1924, Checking gradient for output layer W[98][0]:	mock grad = -0.022545333826534675, computed grad = -0.022545333816011176
Iteration: 1925, Checking gradient for words E[306][6]:	mock grad = -0.010669433809340489, computed grad = -0.010669433838672420
Iteration: 1925, Checking gradient for hidden W[69][20]:	mock grad = -0.010291310125892794, computed grad = -0.010291310195299538
Iteration: 1925, Checking gradient for hidden b[0][0]:	mock grad = -0.137833152840938222, computed grad = -0.137833182761707418
Iteration: 1925, Checking gradient for output layer W[109][0]:	mock grad = 0.058209959328869854, computed grad = 0.058209959160158364
Iteration: 1926, Checking gradient for words E[152][24]:	mock grad = -0.000312342892022643, computed grad = -0.000312342894405921
Iteration: 1926, Checking gradient for hidden W[62][27]:	mock grad = 0.000399938436770886, computed grad = 0.000399938443092522
Iteration: 1926, Checking gradient for hidden b[0][32]:	mock grad = -0.033664020467255584, computed grad = -0.033664031356707792
Iteration: 1926, Checking gradient for output layer W[45][1]:	mock grad = 0.014221014940599130, computed grad = 0.014221014933664379
Iteration: 1927, Checking gradient for words E[487][19]:	mock grad = 0.017396754708298712, computed grad = 0.017396754650228750
Iteration: 1927, Checking gradient for hidden W[196][29]:	mock grad = -0.003293308680790696, computed grad = -0.003293308681291713
Iteration: 1927, Checking gradient for hidden b[0][6]:	mock grad = 0.039750519350317814, computed grad = 0.039750532446961397
Iteration: 1927, Checking gradient for output layer W[144][0]:	mock grad = 0.024758505123845254, computed grad = 0.024758505118475074
Iteration: 1928, Checking gradient for words E[456][26]:	mock grad = 0.002473635791047646, computed grad = 0.002473635791818734
Iteration: 1928, Checking gradient for hidden W[249][36]:	mock grad = 0.001966883859588764, computed grad = 0.001966883848556119
Iteration: 1928, Checking gradient for hidden b[0][4]:	mock grad = 0.009361340288865927, computed grad = 0.009361341183956895
Iteration: 1928, Checking gradient for output layer W[143][1]:	mock grad = -0.140684174484415059, computed grad = -0.140684168941766108
current: 30, Cost = 0.275583, Correct(%) = 1, time = 2.11657
Iteration: 1929, Checking gradient for words E[289][36]:	mock grad = -0.000387613155072852, computed grad = -0.000387613135130464
Iteration: 1929, Checking gradient for hidden W[228][24]:	mock grad = 0.006090092183497209, computed grad = 0.006090092207321218
Iteration: 1929, Checking gradient for hidden b[0][42]:	mock grad = 0.051876768389108596, computed grad = 0.051876784909830595
Iteration: 1929, Checking gradient for output layer W[62][1]:	mock grad = -0.123112010348591649, computed grad = -0.123112008239737433
Iteration: 1930, Checking gradient for words E[512][38]:	mock grad = -0.011408532638174096, computed grad = -0.010853487140735181
Iteration: 1930, Checking gradient for hidden W[8][36]:	mock grad = 0.007043881536189511, computed grad = 0.007043881628983179
Iteration: 1930, Checking gradient for hidden b[0][7]:	mock grad = 0.058013115517507874, computed grad = 0.058013129793284005
Iteration: 1930, Checking gradient for output layer W[81][0]:	mock grad = -0.029753062619347626, computed grad = -0.029753062582389102
Iteration: 1931, Checking gradient for words E[289][46]:	mock grad = -0.010321501991095161, computed grad = -0.010321502026142649
Iteration: 1931, Checking gradient for hidden W[155][5]:	mock grad = -0.002494529608876306, computed grad = -0.002494529696954703
Iteration: 1931, Checking gradient for hidden b[0][49]:	mock grad = 0.043687251383317527, computed grad = 0.043687265119656406
Iteration: 1931, Checking gradient for output layer W[148][0]:	mock grad = 0.177875542512778706, computed grad = 0.177875539919205211
Iteration: 1932, Checking gradient for words E[517][24]:	mock grad = -0.016090619749498103, computed grad = -0.016090619759019681
Iteration: 1932, Checking gradient for hidden W[193][30]:	mock grad = 0.002936772502543095, computed grad = 0.002936772519922445
Iteration: 1932, Checking gradient for hidden b[0][30]:	mock grad = -0.000712758378151879, computed grad = -0.000712756830204855
Iteration: 1932, Checking gradient for output layer W[48][1]:	mock grad = -0.067182270749560624, computed grad = -0.067182270374645151
Iteration: 1933, Checking gradient for words E[453][16]:	mock grad = 0.005406477678382426, computed grad = 0.005406477633293985
Iteration: 1933, Checking gradient for hidden W[213][19]:	mock grad = -0.008059562672690923, computed grad = -0.008059562651635290
Iteration: 1933, Checking gradient for hidden b[0][11]:	mock grad = 0.053910704833709522, computed grad = 0.053910712411376890
Iteration: 1933, Checking gradient for output layer W[79][1]:	mock grad = -0.017275887213979990, computed grad = -0.017275887205178638
Iteration: 1934, Checking gradient for words E[68][34]:	mock grad = 0.014484539244602468, computed grad = 0.014484539279160111
Iteration: 1934, Checking gradient for hidden W[131][18]:	mock grad = -0.010015025232401609, computed grad = -0.010015025258561643
Iteration: 1934, Checking gradient for hidden b[0][2]:	mock grad = 0.056258720238150373, computed grad = 0.056258750202797025
Iteration: 1934, Checking gradient for output layer W[110][0]:	mock grad = 0.102618477868604563, computed grad = 0.102618477234146951
Iteration: 1935, Checking gradient for words E[201][46]:	mock grad = 0.027185580494248907, computed grad = 0.027185580679876878
Iteration: 1935, Checking gradient for hidden W[166][19]:	mock grad = 0.004225903783239282, computed grad = 0.004225903788999987
Iteration: 1935, Checking gradient for hidden b[0][20]:	mock grad = 0.097871579493064020, computed grad = 0.097871608665860171
Iteration: 1935, Checking gradient for output layer W[11][1]:	mock grad = 0.048038332519928506, computed grad = 0.048038332451432665
Iteration: 1936, Checking gradient for words E[48][36]:	mock grad = -0.001066379525932470, computed grad = -0.001066379618690685
Iteration: 1936, Checking gradient for hidden W[121][47]:	mock grad = 0.009346646268643122, computed grad = 0.009346646294992622
Iteration: 1936, Checking gradient for hidden b[0][17]:	mock grad = 0.019756612592292111, computed grad = 0.019756616687849209
Iteration: 1936, Checking gradient for output layer W[140][0]:	mock grad = 0.113101881384275460, computed grad = 0.113101880885285819
Iteration: 1937, Checking gradient for words E[533][5]:	mock grad = -0.009226366935732866, computed grad = -0.009226366970193148
Iteration: 1937, Checking gradient for hidden W[86][4]:	mock grad = 0.004617076438090395, computed grad = 0.004617076476367567
Iteration: 1937, Checking gradient for hidden b[0][39]:	mock grad = -0.065877802010311237, computed grad = -0.065877828029220389
Iteration: 1937, Checking gradient for output layer W[90][1]:	mock grad = -0.082719671529812855, computed grad = -0.082719670948317864
Iteration: 1938, Checking gradient for words E[10][41]:	mock grad = -0.024551278663659248, computed grad = -0.024551278837961089
Iteration: 1938, Checking gradient for hidden W[55][6]:	mock grad = -0.003865502923916520, computed grad = -0.003865502965193383
Iteration: 1938, Checking gradient for hidden b[0][47]:	mock grad = -0.067265056136051049, computed grad = -0.067265065229743584
Iteration: 1938, Checking gradient for output layer W[67][0]:	mock grad = -0.033681796900525418, computed grad = -0.033681796877450716
current: 40, Cost = 0.21412, Correct(%) = 1, time = 2.55467
Iteration: 1939, Checking gradient for words E[546][1]:	mock grad = -0.027641370973843782, computed grad = -0.028129291464625703
Iteration: 1939, Checking gradient for hidden W[125][18]:	mock grad = 0.004318341670556114, computed grad = 0.004318341784411390
Iteration: 1939, Checking gradient for hidden b[0][1]:	mock grad = 0.065887792087604424, computed grad = 0.065887816998944027
Iteration: 1939, Checking gradient for output layer W[45][1]:	mock grad = 0.002484001636118838, computed grad = 0.002484001636054894
Iteration: 1940, Checking gradient for words E[535][23]:	mock grad = -0.022746572607243443, computed grad = -0.022746572649936792
Iteration: 1940, Checking gradient for hidden W[96][21]:	mock grad = -0.005126995778106935, computed grad = -0.005126995816302173
Iteration: 1940, Checking gradient for hidden b[0][47]:	mock grad = 0.047952286256633392, computed grad = 0.047952296266006146
Iteration: 1940, Checking gradient for output layer W[94][1]:	mock grad = -0.126566280761727934, computed grad = -0.126566276602719713
Iteration: 1941, Checking gradient for words E[24][24]:	mock grad = -0.017217146978915077, computed grad = -0.017217147037088761
Iteration: 1941, Checking gradient for hidden W[234][35]:	mock grad = 0.003955491995139004, computed grad = 0.003955492115306595
Iteration: 1941, Checking gradient for hidden b[0][14]:	mock grad = -0.054758757579986828, computed grad = -0.054758777070857544
Iteration: 1941, Checking gradient for output layer W[3][1]:	mock grad = -0.045820032139737288, computed grad = -0.045820032105621043
Iteration: 1942, Checking gradient for words E[34][21]:	mock grad = -0.023072251705058555, computed grad = -0.023072251799100045
Iteration: 1942, Checking gradient for hidden W[72][7]:	mock grad = -0.002912425054041323, computed grad = -0.002912425059536984
Iteration: 1942, Checking gradient for hidden b[0][7]:	mock grad = 0.087057542397434018, computed grad = 0.087057570687565630
Iteration: 1942, Checking gradient for output layer W[15][0]:	mock grad = -0.143496521162717050, computed grad = -0.143496519645765014
Iteration: 1943, Checking gradient for words E[30][9]:	mock grad = 0.035245687497909017, computed grad = 0.035245687566835965
Iteration: 1943, Checking gradient for hidden W[106][20]:	mock grad = 0.002413754655233857, computed grad = 0.002413754617895857
Iteration: 1943, Checking gradient for hidden b[0][44]:	mock grad = 0.000274969557567672, computed grad = 0.000274962437560926
Iteration: 1943, Checking gradient for output layer W[76][0]:	mock grad = -0.108150760775638233, computed grad = -0.108150760319450034
Iteration: 1944, Checking gradient for words E[574][10]:	mock grad = 0.011620517948104925, computed grad = 0.011620517996519924
Iteration: 1944, Checking gradient for hidden W[215][36]:	mock grad = -0.004710068071719919, computed grad = -0.004710068032442050
Iteration: 1944, Checking gradient for hidden b[0][31]:	mock grad = 0.022158384126502728, computed grad = 0.022158392583387358
Iteration: 1944, Checking gradient for output layer W[103][1]:	mock grad = 0.090019597408688701, computed grad = 0.090019595097299648
Iteration: 1945, Checking gradient for words E[222][17]:	mock grad = 0.014859459944727149, computed grad = 0.014859459992794574
Iteration: 1945, Checking gradient for hidden W[190][21]:	mock grad = -0.001030326686690675, computed grad = -0.001030326689210072
Iteration: 1945, Checking gradient for hidden b[0][1]:	mock grad = -0.097480579756170638, computed grad = -0.097480606753326454
Iteration: 1945, Checking gradient for output layer W[66][1]:	mock grad = 0.050042429201202498, computed grad = 0.050042429111275537
Iteration: 1946, Checking gradient for words E[405][34]:	mock grad = 0.011081899146711538, computed grad = 0.009828423596174585
Iteration: 1946, Checking gradient for hidden W[88][28]:	mock grad = 0.005567048804652752, computed grad = 0.005567048810858394
Iteration: 1946, Checking gradient for hidden b[0][27]:	mock grad = 0.021112414189894402, computed grad = 0.021112425481129023
Iteration: 1946, Checking gradient for output layer W[59][1]:	mock grad = 0.112722657158323414, computed grad = 0.112722654143312701
Iteration: 1947, Checking gradient for words E[596][45]:	mock grad = -0.011826888771479527, computed grad = -0.011826888820178474
Iteration: 1947, Checking gradient for hidden W[76][11]:	mock grad = 0.007970897356240902, computed grad = 0.007970897461348180
Iteration: 1947, Checking gradient for hidden b[0][41]:	mock grad = 0.022155263301723571, computed grad = 0.022155266702957668
Iteration: 1947, Checking gradient for output layer W[24][0]:	mock grad = 0.032844623164163256, computed grad = 0.032844623111388499
Iteration: 1948, Checking gradient for words E[119][32]:	mock grad = -0.004850671229475467, computed grad = -0.004850671244864484
Iteration: 1948, Checking gradient for hidden W[175][35]:	mock grad = -0.000120638645531579, computed grad = -0.000120638641736515
Iteration: 1948, Checking gradient for hidden b[0][37]:	mock grad = 0.037709606611396884, computed grad = 0.037709614460959620
Iteration: 1948, Checking gradient for output layer W[114][1]:	mock grad = -0.056924123465293408, computed grad = -0.056924123147172309
current: 50, Cost = 0.2117, Correct(%) = 1, time = 3.43895
Iteration: 1949, Checking gradient for words E[119][33]:	mock grad = -0.003129657921679962, computed grad = -0.003190203362021401
Iteration: 1949, Checking gradient for hidden W[54][9]:	mock grad = 0.008092042634952334, computed grad = 0.008092042633579892
Iteration: 1949, Checking gradient for hidden b[0][38]:	mock grad = 0.038802268906831805, computed grad = 0.038802276483256321
Iteration: 1949, Checking gradient for output layer W[14][1]:	mock grad = -0.006899113558611614, computed grad = -0.006899113557869412
Iteration: 1950, Checking gradient for words E[360][22]:	mock grad = -0.034320417842792494, computed grad = -0.034320418011117793
Iteration: 1950, Checking gradient for hidden W[45][35]:	mock grad = -0.002716729218449343, computed grad = -0.002716729244092918
Iteration: 1950, Checking gradient for hidden b[0][0]:	mock grad = -0.169673228099159745, computed grad = -0.169673276807872558
Iteration: 1950, Checking gradient for output layer W[70][0]:	mock grad = -0.077764206259267610, computed grad = -0.077764206045556061
Iteration: 1951, Checking gradient for words E[42][4]:	mock grad = -0.027146962190194301, computed grad = -0.027146962142006965
Iteration: 1951, Checking gradient for hidden W[242][13]:	mock grad = -0.020356933994492010, computed grad = -0.020356934572984486
Iteration: 1951, Checking gradient for hidden b[0][41]:	mock grad = 0.028491289353504978, computed grad = 0.028491296042507705
Iteration: 1951, Checking gradient for output layer W[66][0]:	mock grad = 0.031643592524749353, computed grad = 0.031643592493998479
Iteration: 1952, Checking gradient for words E[437][39]:	mock grad = -0.003266087105407989, computed grad = -0.004121008637503778
Iteration: 1952, Checking gradient for hidden W[18][6]:	mock grad = -0.003179649164047893, computed grad = -0.003179649234951361
Iteration: 1952, Checking gradient for hidden b[0][5]:	mock grad = 0.016838042660310970, computed grad = 0.016838045887054003
Iteration: 1952, Checking gradient for output layer W[37][0]:	mock grad = -0.061281590613221359, computed grad = -0.061281590411569269
Iteration: 1953, Checking gradient for words E[300][38]:	mock grad = -0.004227084249075475, computed grad = -0.004227084215186376
Iteration: 1953, Checking gradient for hidden W[115][28]:	mock grad = 0.011264029377316964, computed grad = 0.011264029365929669
Iteration: 1953, Checking gradient for hidden b[0][22]:	mock grad = 0.030812286395009991, computed grad = 0.030812291240132905
Iteration: 1953, Checking gradient for output layer W[53][0]:	mock grad = -0.055243774058938433, computed grad = -0.055243773843205737
Iteration: 1954, Checking gradient for words E[319][47]:	mock grad = 0.005882979759938567, computed grad = 0.006831858815767473
Iteration: 1954, Checking gradient for hidden W[125][19]:	mock grad = -0.000437600806438287, computed grad = -0.000437600810100006
Iteration: 1954, Checking gradient for hidden b[0][23]:	mock grad = -0.081214542890062358, computed grad = -0.081214562443345156
Iteration: 1954, Checking gradient for output layer W[149][0]:	mock grad = -0.046752298035732109, computed grad = -0.046752297784272562
Iteration: 1955, Checking gradient for words E[25][10]:	mock grad = -0.011476450616498823, computed grad = -0.011476450648922805
Iteration: 1955, Checking gradient for hidden W[195][33]:	mock grad = 0.000642642486919920, computed grad = 0.000642642490651261
Iteration: 1955, Checking gradient for hidden b[0][40]:	mock grad = -0.066801551740824916, computed grad = -0.066801565502542917
Iteration: 1955, Checking gradient for output layer W[9][0]:	mock grad = 0.039666327179707639, computed grad = 0.039666327071839792
Iteration: 1956, Checking gradient for words E[112][32]:	mock grad = 0.012436078095701131, computed grad = 0.012436078159101291
Iteration: 1956, Checking gradient for hidden W[18][10]:	mock grad = 0.005266688659522067, computed grad = 0.005266688716326959
Iteration: 1956, Checking gradient for hidden b[0][39]:	mock grad = -0.090523734435909997, computed grad = -0.090523766481397233
Iteration: 1956, Checking gradient for output layer W[5][1]:	mock grad = -0.064213970093845996, computed grad = -0.064213970020480932
Iteration: 1957, Checking gradient for words E[226][13]:	mock grad = 0.001475696924047032, computed grad = 0.001475696915481320
Iteration: 1957, Checking gradient for hidden W[116][32]:	mock grad = 0.009004034541904016, computed grad = 0.009004034643151826
Iteration: 1957, Checking gradient for hidden b[0][21]:	mock grad = -0.038637960315751618, computed grad = -0.038637967806507412
Iteration: 1957, Checking gradient for output layer W[65][1]:	mock grad = 0.148739850539147955, computed grad = 0.148739846787420915
Iteration: 1958, Checking gradient for words E[642][14]:	mock grad = -0.004637636871002160, computed grad = -0.004637636845873129
Iteration: 1958, Checking gradient for hidden W[186][33]:	mock grad = 0.014631177205515211, computed grad = 0.014631177338276543
Iteration: 1958, Checking gradient for hidden b[0][34]:	mock grad = 0.028043700058683063, computed grad = 0.028043710224381114
Iteration: 1958, Checking gradient for output layer W[65][0]:	mock grad = 0.185132438831586699, computed grad = 0.185132433821028258
current: 60, Cost = 0.277556, Correct(%) = 1, time = 4.13054
Iteration: 1959, Checking gradient for words E[68][48]:	mock grad = -0.011990306652248650, computed grad = -0.013590287391959712
Iteration: 1959, Checking gradient for hidden W[58][26]:	mock grad = 0.006132552867077479, computed grad = 0.006132552924569256
Iteration: 1959, Checking gradient for hidden b[0][43]:	mock grad = -0.008701407018807394, computed grad = -0.008701409291027098
Iteration: 1959, Checking gradient for output layer W[115][1]:	mock grad = -0.026833658326896881, computed grad = -0.026833658305497186
Iteration: 1960, Checking gradient for words E[234][18]:	mock grad = 0.002920339951612005, computed grad = 0.002920339934979517
Iteration: 1960, Checking gradient for hidden W[2][45]:	mock grad = 0.002564205439176126, computed grad = 0.002564205455682825
Iteration: 1960, Checking gradient for hidden b[0][3]:	mock grad = -0.037977603186190034, computed grad = -0.037977615064019053
Iteration: 1960, Checking gradient for output layer W[38][1]:	mock grad = 0.001722329952491553, computed grad = 0.001722329952502192
Iteration: 1961, Checking gradient for words E[34][19]:	mock grad = 0.000550177424812892, computed grad = 0.000550177448505016
Iteration: 1961, Checking gradient for hidden W[101][36]:	mock grad = -0.000844659158588668, computed grad = -0.000844659139688949
Iteration: 1961, Checking gradient for hidden b[0][24]:	mock grad = 0.088196563388043270, computed grad = 0.088196591402023919
Iteration: 1961, Checking gradient for output layer W[98][0]:	mock grad = -0.004707176424340043, computed grad = -0.004707176424303412
Iteration: 1962, Checking gradient for words E[68][8]:	mock grad = 0.022494392574345934, computed grad = 0.022494392562822169
Iteration: 1962, Checking gradient for hidden W[27][41]:	mock grad = -0.001907603487866227, computed grad = -0.001907603489728019
Iteration: 1962, Checking gradient for hidden b[0][15]:	mock grad = -0.072553425713967057, computed grad = -0.072553436856904546
Iteration: 1962, Checking gradient for output layer W[141][0]:	mock grad = 0.076301129793510114, computed grad = 0.076301129342822308
Iteration: 1963, Checking gradient for words E[68][43]:	mock grad = 0.012635114310183271, computed grad = 0.011511019468009866
Iteration: 1963, Checking gradient for hidden W[215][48]:	mock grad = 0.008237648325831737, computed grad = 0.008237648416307430
Iteration: 1963, Checking gradient for hidden b[0][18]:	mock grad = -0.041633938842322582, computed grad = -0.041633926498071086
Iteration: 1963, Checking gradient for output layer W[38][0]:	mock grad = -0.003333754908518838, computed grad = -0.003333754908499589
Iteration: 1964, Checking gradient for words E[48][20]:	mock grad = 0.014216959139418117, computed grad = 0.010133963972582398
Iteration: 1964, Checking gradient for hidden W[90][20]:	mock grad = -0.006584492447808676, computed grad = -0.003683674982887675
Iteration: 1964, Checking gradient for hidden b[0][20]:	mock grad = -0.096359255148115164, computed grad = -0.096359284929604258
Iteration: 1964, Checking gradient for output layer W[119][0]:	mock grad = -0.128851777632082998, computed grad = -0.128851776288966269
Iteration: 1965, Checking gradient for words E[666][25]:	mock grad = 0.009343108103535003, computed grad = 0.009343108093084484
Iteration: 1965, Checking gradient for hidden W[181][6]:	mock grad = -0.000077484252569704, computed grad = -0.000077484269657764
Iteration: 1965, Checking gradient for hidden b[0][29]:	mock grad = -0.047990932261238184, computed grad = -0.047990944570859277
Iteration: 1965, Checking gradient for output layer W[2][1]:	mock grad = -0.035175793616629147, computed grad = -0.035175793497487076
Iteration: 1966, Checking gradient for words E[90][12]:	mock grad = 0.000269124148938316, computed grad = 0.000269124166170280
Iteration: 1966, Checking gradient for hidden W[248][34]:	mock grad = 0.004184398448070503, computed grad = 0.004184398452249336
Iteration: 1966, Checking gradient for hidden b[0][7]:	mock grad = 0.053988020064760223, computed grad = 0.053988029187572999
Iteration: 1966, Checking gradient for output layer W[134][0]:	mock grad = 0.115154610542150815, computed grad = 0.115154608483147819
Iteration: 1967, Checking gradient for words E[69][5]:	mock grad = -0.008465977125393653, computed grad = -0.008146163203237029
Iteration: 1967, Checking gradient for hidden W[19][27]:	mock grad = 0.003556976820667801, computed grad = 0.003556976906537911
Iteration: 1967, Checking gradient for hidden b[0][22]:	mock grad = -0.035155122109276071, computed grad = -0.035155129446637984
Iteration: 1967, Checking gradient for output layer W[136][0]:	mock grad = -0.121305150264955364, computed grad = -0.121305148490956791
Iteration: 1968, Checking gradient for words E[673][28]:	mock grad = -0.002018982322701812, computed grad = -0.002018982326144153
Iteration: 1968, Checking gradient for hidden W[73][11]:	mock grad = -0.000005442124154098, computed grad = -0.000005442124303606
Iteration: 1968, Checking gradient for hidden b[0][17]:	mock grad = -0.012297082318354913, computed grad = -0.012297085284723709
Iteration: 1968, Checking gradient for output layer W[19][0]:	mock grad = -0.024650384098864886, computed grad = -0.024650384072256160
current: 70, Cost = 0.220124, Correct(%) = 1, time = 4.74411
Iteration: 1969, Checking gradient for words E[216][17]:	mock grad = 0.001627652547200875, computed grad = 0.001627652542549872
Iteration: 1969, Checking gradient for hidden W[95][13]:	mock grad = 0.002338617769812035, computed grad = 0.002338617771997880
Iteration: 1969, Checking gradient for hidden b[0][32]:	mock grad = -0.035783552210857938, computed grad = -0.035783564202545977
Iteration: 1969, Checking gradient for output layer W[122][0]:	mock grad = -0.100939250354453725, computed grad = -0.100939248223397351
Iteration: 1970, Checking gradient for words E[627][12]:	mock grad = -0.015613923741025770, computed grad = -0.015613923669828299
Iteration: 1970, Checking gradient for hidden W[133][4]:	mock grad = -0.002040881202675093, computed grad = -0.002040881210628670
Iteration: 1970, Checking gradient for hidden b[0][33]:	mock grad = 0.088609974110448464, computed grad = 0.088609993071856893
Iteration: 1970, Checking gradient for output layer W[123][0]:	mock grad = -0.145722986527097031, computed grad = -0.145722984562658808
Iteration: 1971, Checking gradient for words E[444][34]:	mock grad = 0.013425802370542472, computed grad = 0.013425802413509822
Iteration: 1971, Checking gradient for hidden W[124][10]:	mock grad = 0.003313114365077752, computed grad = 0.003313114466429643
Iteration: 1971, Checking gradient for hidden b[0][5]:	mock grad = 0.013934740308807481, computed grad = 0.013934739172042646
Iteration: 1971, Checking gradient for output layer W[14][1]:	mock grad = -0.005183595426977439, computed grad = -0.005183595426929970
Iteration: 1972, Checking gradient for words E[156][32]:	mock grad = 0.034442037169346573, computed grad = 0.034036392312434237
Iteration: 1972, Checking gradient for hidden W[0][36]:	mock grad = -0.001311581035945419, computed grad = -0.001311581024916354
Iteration: 1972, Checking gradient for hidden b[0][24]:	mock grad = 0.083759423839507852, computed grad = 0.083759444869429267
Iteration: 1972, Checking gradient for output layer W[49][0]:	mock grad = 0.032231888155570543, computed grad = 0.032231888134763020
Iteration: 1973, Checking gradient for words E[166][12]:	mock grad = -0.010220493525670005, computed grad = -0.010220493515063198
Iteration: 1973, Checking gradient for hidden W[56][30]:	mock grad = -0.004450799457350607, computed grad = -0.004450799461060655
Iteration: 1973, Checking gradient for hidden b[0][43]:	mock grad = -0.011370926905202028, computed grad = -0.011370929958983998
Iteration: 1973, Checking gradient for output layer W[60][1]:	mock grad = 0.029054143823814549, computed grad = 0.029054143812454847
Iteration: 1974, Checking gradient for words E[73][36]:	mock grad = -0.001098326780718528, computed grad = -0.001098326814332146
Iteration: 1974, Checking gradient for hidden W[41][45]:	mock grad = -0.000374321949048184, computed grad = -0.000374321944988595
Iteration: 1974, Checking gradient for hidden b[0][8]:	mock grad = -0.034517450143656259, computed grad = -0.034517468811089302
Iteration: 1974, Checking gradient for output layer W[38][0]:	mock grad = -0.005462933811983817, computed grad = -0.005462933811920266
Iteration: 1975, Checking gradient for words E[341][41]:	mock grad = -0.022809927972280986, computed grad = -0.022809928021015634
Iteration: 1975, Checking gradient for hidden W[106][18]:	mock grad = -0.004145569152388950, computed grad = -0.004145569032293575
Iteration: 1975, Checking gradient for hidden b[0][22]:	mock grad = -0.027070558642358633, computed grad = -0.027070561248947165
Iteration: 1975, Checking gradient for output layer W[49][1]:	mock grad = -0.034289983840973814, computed grad = -0.034289983779541323
Iteration: 1976, Checking gradient for words E[167][21]:	mock grad = 0.006495518453386584, computed grad = 0.006495518446529734
Iteration: 1976, Checking gradient for hidden W[126][12]:	mock grad = 0.001398333284108322, computed grad = 0.001398333285849487
Iteration: 1976, Checking gradient for hidden b[0][20]:	mock grad = 0.082275782131518982, computed grad = 0.082275805583409745
Iteration: 1976, Checking gradient for output layer W[29][0]:	mock grad = 0.020464470432107751, computed grad = 0.020464470423613931
Iteration: 1977, Checking gradient for words E[698][32]:	mock grad = 0.021048911329646369, computed grad = 0.021048911222137485
Iteration: 1977, Checking gradient for hidden W[113][34]:	mock grad = -0.001782267677219451, computed grad = -0.001782267680284799
Iteration: 1977, Checking gradient for hidden b[0][32]:	mock grad = 0.036584751084756117, computed grad = 0.036584763565062087
Iteration: 1977, Checking gradient for output layer W[141][1]:	mock grad = -0.014455462184273515, computed grad = -0.014455462178173291
Iteration: 1978, Checking gradient for words E[553][23]:	mock grad = 0.023378943732665913, computed grad = 0.023378943754877216
Iteration: 1978, Checking gradient for hidden W[202][48]:	mock grad = -0.002711484950956855, computed grad = -0.002711484964404898
Iteration: 1978, Checking gradient for hidden b[0][45]:	mock grad = 0.007896960811354381, computed grad = 0.007896962860779578
Iteration: 1978, Checking gradient for output layer W[58][0]:	mock grad = 0.048912599242140198, computed grad = 0.048912599131013680
current: 80, Cost = 0.238006, Correct(%) = 1, time = 5.22778
Iteration: 1979, Checking gradient for words E[705][20]:	mock grad = -0.000475608873937161, computed grad = -0.003550921993334035
Iteration: 1979, Checking gradient for hidden W[163][15]:	mock grad = -0.008890974017752851, computed grad = -0.008890974059134358
Iteration: 1979, Checking gradient for hidden b[0][19]:	mock grad = -0.031885648063015304, computed grad = -0.031885654855381328
Iteration: 1979, Checking gradient for output layer W[3][1]:	mock grad = -0.017546244163474389, computed grad = -0.017546244154393202
Iteration: 1980, Checking gradient for words E[403][10]:	mock grad = 0.019637987367926435, computed grad = 0.019637987346647141
Iteration: 1980, Checking gradient for hidden W[177][13]:	mock grad = -0.017004474333059960, computed grad = -0.017004474601882376
Iteration: 1980, Checking gradient for hidden b[0][21]:	mock grad = 0.042125288689048856, computed grad = 0.042125297606359045
Iteration: 1980, Checking gradient for output layer W[52][0]:	mock grad = 0.096302095453582037, computed grad = 0.096302094566169574
Iteration: 1981, Checking gradient for words E[717][39]:	mock grad = 0.003442936571063093, computed grad = 0.003442936585079420
Iteration: 1981, Checking gradient for hidden W[155][29]:	mock grad = 0.000825034879997633, computed grad = 0.000825034884424431
Iteration: 1981, Checking gradient for hidden b[0][27]:	mock grad = 0.025460634141910576, computed grad = 0.025460648674758803
Iteration: 1981, Checking gradient for output layer W[87][1]:	mock grad = 0.088556196210815008, computed grad = 0.088556195208891325
Iteration: 1982, Checking gradient for words E[156][26]:	mock grad = 0.006425701240847514, computed grad = 0.006425701259125000
Iteration: 1982, Checking gradient for hidden W[11][22]:	mock grad = -0.001436752968403399, computed grad = -0.001436752973107334
Iteration: 1982, Checking gradient for hidden b[0][41]:	mock grad = -0.023088209766680690, computed grad = -0.023088211796837822
Iteration: 1982, Checking gradient for output layer W[86][1]:	mock grad = 0.111874084301716303, computed grad = 0.111874082473642972
Iteration: 1983, Checking gradient for words E[156][38]:	mock grad = 0.001374576875518008, computed grad = 0.001374576875041003
Iteration: 1983, Checking gradient for hidden W[136][5]:	mock grad = 0.004171493100696422, computed grad = 0.004171493214038823
Iteration: 1983, Checking gradient for hidden b[0][35]:	mock grad = 0.010493180447573858, computed grad = 0.010493180187700089
Iteration: 1983, Checking gradient for output layer W[33][1]:	mock grad = 0.024924934638848040, computed grad = 0.024924934626428911
Iteration: 1984, Checking gradient for words E[728][38]:	mock grad = 0.001075411535822468, computed grad = 0.001075411522933910
Iteration: 1984, Checking gradient for hidden W[129][31]:	mock grad = 0.002160466433129282, computed grad = 0.002160466445649481
Iteration: 1984, Checking gradient for hidden b[0][31]:	mock grad = -0.017530666451257004, computed grad = -0.017530663165350083
Iteration: 1984, Checking gradient for output layer W[136][1]:	mock grad = -0.090833104961357547, computed grad = -0.090833103660464973
Iteration: 1985, Checking gradient for words E[152][13]:	mock grad = 0.009824870825930754, computed grad = 0.012560203617862399
Iteration: 1985, Checking gradient for hidden W[115][3]:	mock grad = -0.009363522380775868, computed grad = -0.009363522377899374
Iteration: 1985, Checking gradient for hidden b[0][17]:	mock grad = -0.013578803278652130, computed grad = -0.013578807922215922
Iteration: 1985, Checking gradient for output layer W[43][1]:	mock grad = 0.066043408360610956, computed grad = 0.066043407868516521
Iteration: 1986, Checking gradient for words E[537][9]:	mock grad = 0.016930549545768336, computed grad = 0.016946989150011818
Iteration: 1986, Checking gradient for hidden W[187][3]:	mock grad = -0.014333716594705503, computed grad = -0.014333716773656465
Iteration: 1986, Checking gradient for hidden b[0][0]:	mock grad = -0.124879514147885340, computed grad = -0.124879538960889283
Iteration: 1986, Checking gradient for output layer W[144][1]:	mock grad = -0.044957813521429024, computed grad = -0.044957813415749781
Iteration: 1987, Checking gradient for words E[423][2]:	mock grad = -0.006880007376419695, computed grad = -0.006880007379003995
Iteration: 1987, Checking gradient for hidden W[239][4]:	mock grad = 0.002433249998184950, computed grad = 0.002433250051856576
Iteration: 1987, Checking gradient for hidden b[0][38]:	mock grad = 0.049651864189759198, computed grad = 0.049651873802486299
Iteration: 1987, Checking gradient for output layer W[4][1]:	mock grad = 0.013540075674084440, computed grad = 0.013540075671497134
Iteration: 1988, Checking gradient for words E[396][38]:	mock grad = -0.001575127413333544, computed grad = -0.001575127452632434
Iteration: 1988, Checking gradient for hidden W[34][35]:	mock grad = -0.000468243951196046, computed grad = -0.000468243956342292
Iteration: 1988, Checking gradient for hidden b[0][44]:	mock grad = 0.006865941324096791, computed grad = 0.006865948077569482
Iteration: 1988, Checking gradient for output layer W[106][1]:	mock grad = 0.038831650616688584, computed grad = 0.038831650544156000
current: 90, Cost = 0.276544, Correct(%) = 1, time = 6.17325
Iteration: 1989, Checking gradient for words E[79][38]:	mock grad = 0.004418330759980993, computed grad = 0.004418330775128945
Iteration: 1989, Checking gradient for hidden W[154][33]:	mock grad = -0.002867083640722967, computed grad = -0.002867083643938015
Iteration: 1989, Checking gradient for hidden b[0][14]:	mock grad = 0.043669060612566701, computed grad = 0.043669073488545811
Iteration: 1989, Checking gradient for output layer W[8][0]:	mock grad = -0.048604457421635994, computed grad = -0.048604457293150910
Iteration: 1990, Checking gradient for words E[25][19]:	mock grad = -0.004479881997046520, computed grad = -0.004479882010130665
Iteration: 1990, Checking gradient for hidden W[204][20]:	mock grad = 0.002757522210586338, computed grad = 0.002757522214018879
Iteration: 1990, Checking gradient for hidden b[0][8]:	mock grad = 0.022111722830078784, computed grad = 0.022111737617360221
Iteration: 1990, Checking gradient for output layer W[139][0]:	mock grad = 0.105342561867366458, computed grad = 0.105342559979743428
Iteration: 1991, Checking gradient for words E[97][15]:	mock grad = 0.020154345644896043, computed grad = 0.020154345722580090
Iteration: 1991, Checking gradient for hidden W[24][45]:	mock grad = -0.003282532869547383, computed grad = -0.003282532903719849
Iteration: 1991, Checking gradient for hidden b[0][2]:	mock grad = 0.046404958979839872, computed grad = 0.046404982278116340
Iteration: 1991, Checking gradient for output layer W[137][0]:	mock grad = 0.107764662591935956, computed grad = 0.107764661541822238
Iteration: 1992, Checking gradient for words E[48][42]:	mock grad = -0.005063999654067453, computed grad = -0.005063999665898364
Iteration: 1992, Checking gradient for hidden W[188][40]:	mock grad = -0.003460064253932305, computed grad = -0.003460064289721043
Iteration: 1992, Checking gradient for hidden b[0][33]:	mock grad = -0.051804642380734456, computed grad = -0.051804647958757589
Iteration: 1992, Checking gradient for output layer W[115][1]:	mock grad = 0.032407743048926130, computed grad = 0.032407742965628296
Iteration: 1993, Checking gradient for words E[320][41]:	mock grad = -0.016728023095990974, computed grad = -0.016728023137617253
Iteration: 1993, Checking gradient for hidden W[121][31]:	mock grad = -0.016603273217147274, computed grad = -0.016603273654833058
Iteration: 1993, Checking gradient for hidden b[0][26]:	mock grad = 0.023407115285395408, computed grad = 0.023407120341063695
Iteration: 1993, Checking gradient for output layer W[15][1]:	mock grad = -0.063358066565488702, computed grad = -0.063358066292860102
Iteration: 1994, Checking gradient for words E[407][39]:	mock grad = -0.003393972965920478, computed grad = -0.003393972940435403
Iteration: 1994, Checking gradient for hidden W[183][26]:	mock grad = 0.005576769221332656, computed grad = 0.005576769349038549
Iteration: 1994, Checking gradient for hidden b[0][20]:	mock grad = -0.064456902121090565, computed grad = -0.064456921558996044
Iteration: 1994, Checking gradient for output layer W[67][1]:	mock grad = -0.058523628825288276, computed grad = -0.058523628377339487
Iteration: 1995, Checking gradient for words E[49][28]:	mock grad = 0.015653352899130946, computed grad = 0.015653352973411670
Iteration: 1995, Checking gradient for hidden W[81][7]:	mock grad = 0.017052786437904421, computed grad = 0.017052786450241292
Iteration: 1995, Checking gradient for hidden b[0][18]:	mock grad = -0.066676550363020670, computed grad = -0.066676552398210867
Iteration: 1995, Checking gradient for output layer W[6][1]:	mock grad = -0.037751377103989459, computed grad = -0.037751377060859383
Iteration: 1996, Checking gradient for words E[161][35]:	mock grad = -0.003137874099667926, computed grad = -0.003137874096914618
Iteration: 1996, Checking gradient for hidden W[185][48]:	mock grad = 0.006214638589657762, computed grad = 0.006214638612776567
Iteration: 1996, Checking gradient for hidden b[0][35]:	mock grad = 0.008631259868913110, computed grad = 0.008631259310121726
Iteration: 1996, Checking gradient for output layer W[102][1]:	mock grad = -0.088823332703241720, computed grad = -0.088823331676499750
Iteration: 1997, Checking gradient for words E[776][46]:	mock grad = 0.027972774129858546, computed grad = 0.027972774151191253
Iteration: 1997, Checking gradient for hidden W[47][46]:	mock grad = -0.025223317065503625, computed grad = -0.025223317305800842
Iteration: 1997, Checking gradient for hidden b[0][39]:	mock grad = -0.082082113981613025, computed grad = -0.082082144018347153
Iteration: 1997, Checking gradient for output layer W[42][1]:	mock grad = 0.054558627300471674, computed grad = 0.054558627233735349
Iteration: 1998, Checking gradient for words E[156][45]:	mock grad = 0.018195791370564929, computed grad = 0.018195791394876003
Iteration: 1998, Checking gradient for hidden W[118][24]:	mock grad = -0.016876950190425344, computed grad = -0.016876950511779929
Iteration: 1998, Checking gradient for hidden b[0][15]:	mock grad = 0.054569416562233464, computed grad = 0.054569418848617907
Iteration: 1998, Checking gradient for output layer W[8][0]:	mock grad = -0.040116317532812307, computed grad = -0.040116317427771067
current: 100, Cost = 0.279943, Correct(%) = 1, time = 6.95692
Iteration: 1999, Checking gradient for words E[280][1]:	mock grad = -0.009965090838393031, computed grad = -0.009965090863409338
Iteration: 1999, Checking gradient for hidden W[103][41]:	mock grad = 0.005175980684529735, computed grad = 0.005175980705675727
Iteration: 1999, Checking gradient for hidden b[0][7]:	mock grad = 0.060510368026317485, computed grad = 0.060510379277351832
Iteration: 1999, Checking gradient for output layer W[95][0]:	mock grad = -0.068090061930398127, computed grad = -0.068090061589116166
current: 20, Correct(%) = 1, time = 7.00789
Dev start.
Dev finished. Total time taken is: 0.634285
dev:
Accuracy:	P=100/100=1
Test start.
Test finished. Total time taken is: 0.544986
test:
Accuracy:	P=63/100=0.63
##### Iteration 20
random: 0, 99
Iteration: 2000, Checking gradient for words E[51][43]:	mock grad = -0.001862013029360621, computed grad = -0.001862013041559315
Iteration: 2000, Checking gradient for hidden W[124][5]:	mock grad = 0.001101296318556377, computed grad = 0.001101296328546407
Iteration: 2000, Checking gradient for hidden b[0][3]:	mock grad = -0.025010419161294872, computed grad = -0.025010428170306111
Iteration: 2000, Checking gradient for output layer W[87][0]:	mock grad = -0.068455753741702985, computed grad = -0.068455752867525504
Iteration: 2001, Checking gradient for words E[79][41]:	mock grad = -0.004328122811897428, computed grad = -0.004328122820238417
Iteration: 2001, Checking gradient for hidden W[217][35]:	mock grad = 0.000661896339626544, computed grad = 0.000661896341072612
Iteration: 2001, Checking gradient for hidden b[0][5]:	mock grad = -0.000009102333053956, computed grad = -0.000009092779718490
Iteration: 2001, Checking gradient for output layer W[101][1]:	mock grad = -0.118316715389504834, computed grad = -0.118316713175544042
Iteration: 2002, Checking gradient for words E[6][2]:	mock grad = -0.015962894648197112, computed grad = -0.015962894672213603
Iteration: 2002, Checking gradient for hidden W[7][21]:	mock grad = -0.002751911543988350, computed grad = -0.002751911567086338
Iteration: 2002, Checking gradient for hidden b[0][0]:	mock grad = 0.104963118430195790, computed grad = 0.104963135095932344
Iteration: 2002, Checking gradient for output layer W[49][0]:	mock grad = 0.033149937901794102, computed grad = 0.033149937842612109
Iteration: 2003, Checking gradient for words E[138][8]:	mock grad = 0.007530824666701630, computed grad = 0.007530824654853168
Iteration: 2003, Checking gradient for hidden W[94][9]:	mock grad = 0.007830928494406697, computed grad = 0.007830928527476367
Iteration: 2003, Checking gradient for hidden b[0][3]:	mock grad = 0.031826763586401130, computed grad = 0.031826767570351334
Iteration: 2003, Checking gradient for output layer W[19][1]:	mock grad = 0.030147497883048491, computed grad = 0.030147497858334111
Iteration: 2004, Checking gradient for words E[174][23]:	mock grad = -0.005996880917125136, computed grad = -0.005603046736310795
Iteration: 2004, Checking gradient for hidden W[145][2]:	mock grad = -0.002861862327896425, computed grad = -0.002861862321364483
Iteration: 2004, Checking gradient for hidden b[0][19]:	mock grad = 0.025274602381025746, computed grad = 0.025274596215006228
Iteration: 2004, Checking gradient for output layer W[15][0]:	mock grad = 0.064917158766608640, computed grad = 0.064917158470168781
Iteration: 2005, Checking gradient for words E[776][27]:	mock grad = 0.017631023242092869, computed grad = 0.016226997732273405
Iteration: 2005, Checking gradient for hidden W[132][42]:	mock grad = 0.002033679364410457, computed grad = 0.002033679371028280
Iteration: 2005, Checking gradient for hidden b[0][15]:	mock grad = 0.069290458691667611, computed grad = 0.069290470432220946
Iteration: 2005, Checking gradient for output layer W[15][0]:	mock grad = 0.070583087649067622, computed grad = 0.070583087282043849
Iteration: 2006, Checking gradient for words E[255][45]:	mock grad = -0.001457868797322126, computed grad = -0.001457868804580979
Iteration: 2006, Checking gradient for hidden W[21][42]:	mock grad = -0.008025810441583681, computed grad = -0.008025810496354129
Iteration: 2006, Checking gradient for hidden b[0][49]:	mock grad = -0.020452024101835775, computed grad = -0.020452023271800946
Iteration: 2006, Checking gradient for output layer W[37][0]:	mock grad = -0.026526907129856281, computed grad = -0.026526907086442585
Iteration: 2007, Checking gradient for words E[278][28]:	mock grad = -0.014244993679607409, computed grad = -0.014244993658628056
Iteration: 2007, Checking gradient for hidden W[113][44]:	mock grad = 0.013668708710029565, computed grad = 0.013668708784151724
Iteration: 2007, Checking gradient for hidden b[0][33]:	mock grad = -0.102671287501215902, computed grad = -0.102671303789852161
Iteration: 2007, Checking gradient for output layer W[101][0]:	mock grad = 0.150820477606200543, computed grad = 0.150820476556017630
Iteration: 2008, Checking gradient for words E[225][43]:	mock grad = -0.012388726833706398, computed grad = -0.012388726847695011
Iteration: 2008, Checking gradient for hidden W[214][7]:	mock grad = -0.005624034348838958, computed grad = -0.005624034487498120
Iteration: 2008, Checking gradient for hidden b[0][34]:	mock grad = 0.019161654424060259, computed grad = 0.019161658107882958
Iteration: 2008, Checking gradient for output layer W[129][0]:	mock grad = -0.093812982960322921, computed grad = -0.093812981793634925
current: 10, Cost = 0.290463, Correct(%) = 1, time = 0.77136
Iteration: 2009, Checking gradient for words E[74][14]:	mock grad = -0.017111958247328918, computed grad = -0.014606037324817227
Iteration: 2009, Checking gradient for hidden W[155][11]:	mock grad = -0.008549272905750138, computed grad = -0.008549272990949921
Iteration: 2009, Checking gradient for hidden b[0][0]:	mock grad = -0.132353913943450419, computed grad = -0.132353946773768272
Iteration: 2009, Checking gradient for output layer W[48][0]:	mock grad = 0.079170499679398487, computed grad = 0.079170499196768274
Iteration: 2010, Checking gradient for words E[304][16]:	mock grad = 0.000165117372788459, computed grad = 0.000165117400614770
Iteration: 2010, Checking gradient for hidden W[96][35]:	mock grad = 0.003410723929467285, computed grad = 0.003410723956739706
Iteration: 2010, Checking gradient for hidden b[0][10]:	mock grad = -0.058171569512419086, computed grad = -0.058171588986524594
Iteration: 2010, Checking gradient for output layer W[127][0]:	mock grad = -0.106940743190730991, computed grad = -0.106940740855227631
Iteration: 2011, Checking gradient for words E[73][15]:	mock grad = -0.025490155173890638, computed grad = -0.026033580051722946
Iteration: 2011, Checking gradient for hidden W[143][16]:	mock grad = 0.000889755141592063, computed grad = 0.000889755143931132
Iteration: 2011, Checking gradient for hidden b[0][29]:	mock grad = -0.071206145597413339, computed grad = -0.071206156717266614
Iteration: 2011, Checking gradient for output layer W[132][0]:	mock grad = 0.032584376277317029, computed grad = 0.032584376258862999
Iteration: 2012, Checking gradient for words E[356][27]:	mock grad = 0.001832206333207687, computed grad = 0.001832206315277949
Iteration: 2012, Checking gradient for hidden W[122][47]:	mock grad = -0.008208090746794627, computed grad = -0.008208090780377464
Iteration: 2012, Checking gradient for hidden b[0][48]:	mock grad = -0.057815307239458824, computed grad = -0.057815320262351887
Iteration: 2012, Checking gradient for output layer W[106][1]:	mock grad = 0.064452559071850879, computed grad = 0.064452558625027162
Iteration: 2013, Checking gradient for words E[309][29]:	mock grad = -0.013151388660426200, computed grad = -0.013265492179856415
Iteration: 2013, Checking gradient for hidden W[191][33]:	mock grad = 0.003445901194179535, computed grad = 0.003445901195622013
Iteration: 2013, Checking gradient for hidden b[0][6]:	mock grad = 0.024539269010415143, computed grad = 0.024539277148189993
Iteration: 2013, Checking gradient for output layer W[128][0]:	mock grad = 0.034524844210639949, computed grad = 0.034524844131150617
Iteration: 2014, Checking gradient for words E[156][0]:	mock grad = 0.006391286160589704, computed grad = 0.004698479558345502
Iteration: 2014, Checking gradient for hidden W[18][35]:	mock grad = 0.000715845940801874, computed grad = 0.000715845952523116
Iteration: 2014, Checking gradient for hidden b[0][18]:	mock grad = -0.068836739614053233, computed grad = -0.068836728919033197
Iteration: 2014, Checking gradient for output layer W[87][0]:	mock grad = 0.031754346392243171, computed grad = 0.031754346385541116
Iteration: 2015, Checking gradient for words E[383][42]:	mock grad = -0.003571752638265036, computed grad = -0.003571752644841011
Iteration: 2015, Checking gradient for hidden W[15][47]:	mock grad = -0.004035129476304133, computed grad = -0.004035129524059083
Iteration: 2015, Checking gradient for hidden b[0][34]:	mock grad = -0.023034852277309392, computed grad = -0.023034856664719848
Iteration: 2015, Checking gradient for output layer W[27][1]:	mock grad = -0.014161944189933218, computed grad = -0.014161944186810002
Iteration: 2016, Checking gradient for words E[387][47]:	mock grad = -0.000307202881993929, computed grad = -0.000307202879303598
Iteration: 2016, Checking gradient for hidden W[17][9]:	mock grad = -0.000934154440662693, computed grad = -0.000934154439069217
Iteration: 2016, Checking gradient for hidden b[0][37]:	mock grad = -0.038719064968051331, computed grad = -0.038719072490679747
Iteration: 2016, Checking gradient for output layer W[126][1]:	mock grad = 0.126700694733167252, computed grad = 0.126700693034735162
Iteration: 2017, Checking gradient for words E[25][24]:	mock grad = 0.012128130313038277, computed grad = 0.011386994312527703
Iteration: 2017, Checking gradient for hidden W[217][36]:	mock grad = 0.002614697150238321, computed grad = 0.002614697184512140
Iteration: 2017, Checking gradient for hidden b[0][37]:	mock grad = -0.028474199244399312, computed grad = -0.028474207011721712
Iteration: 2017, Checking gradient for output layer W[64][0]:	mock grad = 0.104650046028997856, computed grad = 0.104650042918938549
Iteration: 2018, Checking gradient for words E[178][1]:	mock grad = -0.017792938513505607, computed grad = -0.017792938492980200
Iteration: 2018, Checking gradient for hidden W[32][45]:	mock grad = -0.003491921568968270, computed grad = -0.003491921606157225
Iteration: 2018, Checking gradient for hidden b[0][20]:	mock grad = -0.068867554558524335, computed grad = -0.068867574157382136
Iteration: 2018, Checking gradient for output layer W[85][1]:	mock grad = -0.132292337881492750, computed grad = -0.132292333642191590
current: 20, Cost = 0.400451, Correct(%) = 1, time = 1.42828
Iteration: 2019, Checking gradient for words E[58][14]:	mock grad = -0.028212422511081359, computed grad = -0.030200675721956782
Iteration: 2019, Checking gradient for hidden W[96][48]:	mock grad = -0.010247993108392084, computed grad = -0.010247993129959249
Iteration: 2019, Checking gradient for hidden b[0][48]:	mock grad = 0.094615180269264076, computed grad = 0.094615187374361850
Iteration: 2019, Checking gradient for output layer W[117][0]:	mock grad = 0.112010056815159409, computed grad = 0.112010056325108395
Iteration: 2020, Checking gradient for words E[443][35]:	mock grad = -0.025050232304396625, computed grad = -0.025050232335517460
Iteration: 2020, Checking gradient for hidden W[33][42]:	mock grad = -0.002971088351816364, computed grad = -0.002971088356147006
Iteration: 2020, Checking gradient for hidden b[0][35]:	mock grad = -0.009879559475733934, computed grad = -0.009879559546309220
Iteration: 2020, Checking gradient for output layer W[32][0]:	mock grad = 0.068552462031856720, computed grad = 0.068552461643218418
Iteration: 2021, Checking gradient for words E[776][11]:	mock grad = 0.049879688965831503, computed grad = 0.052829229463438682
Iteration: 2021, Checking gradient for hidden W[207][29]:	mock grad = -0.000185636571309322, computed grad = -0.000185636579728640
Iteration: 2021, Checking gradient for hidden b[0][21]:	mock grad = 0.065191215835286309, computed grad = 0.065191235005922457
Iteration: 2021, Checking gradient for output layer W[113][0]:	mock grad = -0.090339089655167015, computed grad = -0.090339089475418410
Iteration: 2022, Checking gradient for words E[294][4]:	mock grad = 0.005296087685652262, computed grad = 0.005724952378279911
Iteration: 2022, Checking gradient for hidden W[140][41]:	mock grad = -0.003075369127303329, computed grad = -0.003075369180313011
Iteration: 2022, Checking gradient for hidden b[0][31]:	mock grad = 0.028467164817205792, computed grad = 0.028467175961411472
Iteration: 2022, Checking gradient for output layer W[24][1]:	mock grad = -0.038585737561894207, computed grad = -0.038585737461519339
Iteration: 2023, Checking gradient for words E[102][33]:	mock grad = 0.014517868462765593, computed grad = 0.014517868518647972
Iteration: 2023, Checking gradient for hidden W[21][12]:	mock grad = 0.003876359061294465, computed grad = 0.003876359094499420
Iteration: 2023, Checking gradient for hidden b[0][24]:	mock grad = 0.069511240483266512, computed grad = 0.069511259019108415
Iteration: 2023, Checking gradient for output layer W[8][1]:	mock grad = 0.047434208686220058, computed grad = 0.047434208559121421
Iteration: 2024, Checking gradient for words E[58][12]:	mock grad = -0.014552317904426015, computed grad = -0.015476235774867286
Iteration: 2024, Checking gradient for hidden W[27][45]:	mock grad = -0.001881532073921832, computed grad = -0.001881532081504330
Iteration: 2024, Checking gradient for hidden b[0][31]:	mock grad = -0.025479805612432793, computed grad = -0.025479805992494074
Iteration: 2024, Checking gradient for output layer W[122][1]:	mock grad = -0.169499657798388714, computed grad = -0.169499653045907389
Iteration: 2025, Checking gradient for words E[118][37]:	mock grad = -0.010704480117612603, computed grad = -0.010704480013767075
Iteration: 2025, Checking gradient for hidden W[224][17]:	mock grad = -0.001310466490239692, computed grad = -0.001310466491934673
Iteration: 2025, Checking gradient for hidden b[0][36]:	mock grad = -0.011304943084877728, computed grad = -0.011304945095114077
Iteration: 2025, Checking gradient for output layer W[137][0]:	mock grad = 0.101190573551762641, computed grad = 0.101190572629055148
Iteration: 2026, Checking gradient for words E[468][40]:	mock grad = -0.004092870056551789, computed grad = -0.004092870056961549
Iteration: 2026, Checking gradient for hidden W[157][46]:	mock grad = -0.017689481053101419, computed grad = -0.017689481160948709
Iteration: 2026, Checking gradient for hidden b[0][39]:	mock grad = -0.051351240813207744, computed grad = -0.051351262473653736
Iteration: 2026, Checking gradient for output layer W[36][1]:	mock grad = 0.008420765846686717, computed grad = 0.008420765845222677
Iteration: 2027, Checking gradient for words E[68][25]:	mock grad = -0.011537317122167767, computed grad = -0.011537317082180941
Iteration: 2027, Checking gradient for hidden W[57][16]:	mock grad = -0.002098117505705543, computed grad = -0.002098117537703352
Iteration: 2027, Checking gradient for hidden b[0][7]:	mock grad = 0.091920482452129137, computed grad = 0.091920510589541218
Iteration: 2027, Checking gradient for output layer W[101][1]:	mock grad = -0.122264109086916983, computed grad = -0.122264108408338415
Iteration: 2028, Checking gradient for words E[393][41]:	mock grad = -0.008195708641023369, computed grad = -0.008195708657396272
Iteration: 2028, Checking gradient for hidden W[181][12]:	mock grad = 0.000981043171935370, computed grad = 0.000981043173211257
Iteration: 2028, Checking gradient for hidden b[0][11]:	mock grad = -0.056535427581286268, computed grad = -0.056535443423630961
Iteration: 2028, Checking gradient for output layer W[52][0]:	mock grad = -0.056136992988661816, computed grad = -0.056136992615815576
current: 30, Cost = 0.271215, Correct(%) = 1, time = 2.12033
Iteration: 2029, Checking gradient for words E[54][13]:	mock grad = -0.015068988673150319, computed grad = -0.015068988696056902
Iteration: 2029, Checking gradient for hidden W[191][23]:	mock grad = -0.001177588837414367, computed grad = -0.001177588845163124
Iteration: 2029, Checking gradient for hidden b[0][11]:	mock grad = 0.069486686779501872, computed grad = 0.069486704575853514
Iteration: 2029, Checking gradient for output layer W[85][0]:	mock grad = 0.141306532566387588, computed grad = 0.141306529231141870
Iteration: 2030, Checking gradient for words E[102][24]:	mock grad = -0.035662866726182685, computed grad = -0.035662866790868900
Iteration: 2030, Checking gradient for hidden W[175][18]:	mock grad = 0.011453236719111137, computed grad = 0.011453236827638538
Iteration: 2030, Checking gradient for hidden b[0][8]:	mock grad = 0.017757969980686239, computed grad = 0.017757979188122795
Iteration: 2030, Checking gradient for output layer W[107][0]:	mock grad = 0.035917514265293438, computed grad = 0.035917514196996250
Iteration: 2031, Checking gradient for words E[409][21]:	mock grad = 0.005171966791284177, computed grad = 0.005171966824806366
Iteration: 2031, Checking gradient for hidden W[199][0]:	mock grad = 0.020054739014757628, computed grad = 0.020054739199467757
Iteration: 2031, Checking gradient for hidden b[0][0]:	mock grad = -0.165152630053566618, computed grad = -0.165152676862984210
Iteration: 2031, Checking gradient for output layer W[77][1]:	mock grad = 0.081020550436183836, computed grad = 0.081020550176513731
Iteration: 2032, Checking gradient for words E[34][22]:	mock grad = 0.000902891255205596, computed grad = 0.000902891274237118
Iteration: 2032, Checking gradient for hidden W[24][21]:	mock grad = 0.002351099872482942, computed grad = 0.002351099907456780
Iteration: 2032, Checking gradient for hidden b[0][8]:	mock grad = 0.017242197923206248, computed grad = 0.017242205745180733
Iteration: 2032, Checking gradient for output layer W[92][1]:	mock grad = 0.024842737376490343, computed grad = 0.024842737356557287
Iteration: 2033, Checking gradient for words E[167][3]:	mock grad = -0.014494916078092168, computed grad = -0.014494916122310516
Iteration: 2033, Checking gradient for hidden W[142][48]:	mock grad = 0.008964457782381641, computed grad = 0.008964457880955613
Iteration: 2033, Checking gradient for hidden b[0][12]:	mock grad = -0.048664841629936229, computed grad = -0.048664857894919256
Iteration: 2033, Checking gradient for output layer W[47][0]:	mock grad = 0.039554151257212289, computed grad = 0.039554151146800935
Iteration: 2034, Checking gradient for words E[776][47]:	mock grad = -0.000853098121783580, computed grad = -0.000853098160485535
Iteration: 2034, Checking gradient for hidden W[123][13]:	mock grad = 0.015410379500285787, computed grad = 0.015410379707595234
Iteration: 2034, Checking gradient for hidden b[0][4]:	mock grad = 0.014721860010641352, computed grad = 0.014721862104990213
Iteration: 2034, Checking gradient for output layer W[3][1]:	mock grad = 0.014258764058611151, computed grad = 0.014258764056727638
Iteration: 2035, Checking gradient for words E[156][28]:	mock grad = 0.024875542405072837, computed grad = 0.024875542444623967
Iteration: 2035, Checking gradient for hidden W[12][19]:	mock grad = 0.010773452177287313, computed grad = 0.010773452254234222
Iteration: 2035, Checking gradient for hidden b[0][4]:	mock grad = 0.016393381789964190, computed grad = 0.016393385943831169
Iteration: 2035, Checking gradient for output layer W[49][1]:	mock grad = 0.029085679535550968, computed grad = 0.029085679519724475
Iteration: 2036, Checking gradient for words E[531][42]:	mock grad = -0.025518918377820388, computed grad = -0.025518918355583949
Iteration: 2036, Checking gradient for hidden W[198][23]:	mock grad = -0.000545643309340926, computed grad = -0.000545643281739064
Iteration: 2036, Checking gradient for hidden b[0][24]:	mock grad = -0.101219436558080966, computed grad = -0.101219449326757721
Iteration: 2036, Checking gradient for output layer W[116][0]:	mock grad = 0.049629310262172543, computed grad = 0.049629310218108395
Iteration: 2037, Checking gradient for words E[89][27]:	mock grad = 0.003153694273810848, computed grad = 0.004369457422036979
Iteration: 2037, Checking gradient for hidden W[91][41]:	mock grad = 0.004070578991627327, computed grad = 0.004070578998243058
Iteration: 2037, Checking gradient for hidden b[0][9]:	mock grad = 0.045471410412695201, computed grad = 0.045471420856026457
Iteration: 2037, Checking gradient for output layer W[46][1]:	mock grad = -0.071313309955245430, computed grad = -0.071313309562132873
Iteration: 2038, Checking gradient for words E[493][33]:	mock grad = -0.006565269462632806, computed grad = -0.006565269497983660
Iteration: 2038, Checking gradient for hidden W[85][46]:	mock grad = 0.018543887305877416, computed grad = 0.018543887492928606
Iteration: 2038, Checking gradient for hidden b[0][0]:	mock grad = -0.153595170781062329, computed grad = -0.153595213100958361
Iteration: 2038, Checking gradient for output layer W[16][1]:	mock grad = -0.016071780713639017, computed grad = -0.016071780710953235
current: 40, Cost = 0.20973, Correct(%) = 1, time = 2.55953
Iteration: 2039, Checking gradient for words E[160][0]:	mock grad = 0.008690969140814908, computed grad = 0.011514911802269420
Iteration: 2039, Checking gradient for hidden W[171][18]:	mock grad = -0.005100724995377171, computed grad = -0.005100725018118763
Iteration: 2039, Checking gradient for hidden b[0][32]:	mock grad = -0.034230477612723353, computed grad = -0.034230488676132424
Iteration: 2039, Checking gradient for output layer W[34][0]:	mock grad = 0.010585988988107142, computed grad = 0.010585988985288014
Iteration: 2040, Checking gradient for words E[377][39]:	mock grad = 0.003089013254298223, computed grad = 0.003089013245079539
Iteration: 2040, Checking gradient for hidden W[194][2]:	mock grad = -0.009340319034406575, computed grad = -0.009340319002375348
Iteration: 2040, Checking gradient for hidden b[0][16]:	mock grad = -0.011829992046172300, computed grad = -0.011829996522010265
Iteration: 2040, Checking gradient for output layer W[85][0]:	mock grad = 0.123665807449685472, computed grad = 0.123665803337419417
Iteration: 2041, Checking gradient for words E[225][5]:	mock grad = -0.041218546590143790, computed grad = -0.041218546602439746
Iteration: 2041, Checking gradient for hidden W[70][24]:	mock grad = -0.015276621283533132, computed grad = -0.015276621327868457
Iteration: 2041, Checking gradient for hidden b[0][27]:	mock grad = 0.032045336158914584, computed grad = 0.032045350313136080
Iteration: 2041, Checking gradient for output layer W[12][0]:	mock grad = -0.059622513186086135, computed grad = -0.059622513106667906
Iteration: 2042, Checking gradient for words E[6][2]:	mock grad = 0.028858498703737689, computed grad = 0.028858498714177953
Iteration: 2042, Checking gradient for hidden W[48][22]:	mock grad = -0.008563380832798950, computed grad = -0.008563380893220385
Iteration: 2042, Checking gradient for hidden b[0][32]:	mock grad = 0.053181669475405036, computed grad = 0.053181687208891104
Iteration: 2042, Checking gradient for output layer W[69][0]:	mock grad = 0.020918327750890642, computed grad = 0.020918327745876757
Iteration: 2043, Checking gradient for words E[562][14]:	mock grad = 0.001375788652779875, computed grad = 0.001375788642103624
Iteration: 2043, Checking gradient for hidden W[119][36]:	mock grad = -0.003299084159913424, computed grad = -0.003299084136152851
Iteration: 2043, Checking gradient for hidden b[0][24]:	mock grad = -0.097668556612584556, computed grad = -0.097668566976101573
Iteration: 2043, Checking gradient for output layer W[16][1]:	mock grad = -0.034282025640486324, computed grad = -0.034282025624939136
Iteration: 2044, Checking gradient for words E[364][44]:	mock grad = -0.007444993000191302, computed grad = -0.007444993032606233
Iteration: 2044, Checking gradient for hidden W[178][13]:	mock grad = -0.008565288007453264, computed grad = -0.008565288011322645
Iteration: 2044, Checking gradient for hidden b[0][30]:	mock grad = -0.001417362117756005, computed grad = -0.001417364699275800
Iteration: 2044, Checking gradient for output layer W[4][1]:	mock grad = -0.011842470655762383, computed grad = -0.011842470650221825
Iteration: 2045, Checking gradient for words E[194][4]:	mock grad = 0.003477340743435198, computed grad = 0.002426361906419142
Iteration: 2045, Checking gradient for hidden W[208][4]:	mock grad = -0.000299875524578663, computed grad = -0.000299875545225182
Iteration: 2045, Checking gradient for hidden b[0][0]:	mock grad = -0.142385209414036540, computed grad = -0.142385237686244864
Iteration: 2045, Checking gradient for output layer W[53][0]:	mock grad = -0.032342341715341227, computed grad = -0.032342341689763916
Iteration: 2046, Checking gradient for words E[52][27]:	mock grad = 0.006924909247529865, computed grad = 0.006924909255018060
Iteration: 2046, Checking gradient for hidden W[186][12]:	mock grad = 0.004946105010866786, computed grad = 0.004946105078303746
Iteration: 2046, Checking gradient for hidden b[0][47]:	mock grad = -0.042491681971901230, computed grad = -0.042491686031895362
Iteration: 2046, Checking gradient for output layer W[28][1]:	mock grad = 0.024098197446947855, computed grad = 0.024098197416178330
Iteration: 2047, Checking gradient for words E[25][0]:	mock grad = 0.008641754894303522, computed grad = 0.010411941442878343
Iteration: 2047, Checking gradient for hidden W[198][17]:	mock grad = -0.001849501195527736, computed grad = -0.001849501213651148
Iteration: 2047, Checking gradient for hidden b[0][27]:	mock grad = -0.024312301519113788, computed grad = -0.024312308719733545
Iteration: 2047, Checking gradient for output layer W[65][0]:	mock grad = 0.124049022554376109, computed grad = 0.124049019570529204
Iteration: 2048, Checking gradient for words E[120][49]:	mock grad = 0.009274061602490558, computed grad = 0.009274061631702305
Iteration: 2048, Checking gradient for hidden W[209][3]:	mock grad = 0.000416081085308218, computed grad = 0.000416081089210996
Iteration: 2048, Checking gradient for hidden b[0][11]:	mock grad = -0.059505205439763786, computed grad = -0.059505221753678839
Iteration: 2048, Checking gradient for output layer W[49][1]:	mock grad = 0.018481392064964575, computed grad = 0.018481392053494240
current: 50, Cost = 0.20712, Correct(%) = 1, time = 3.44722
Iteration: 2049, Checking gradient for words E[197][18]:	mock grad = -0.002262274028441702, computed grad = -0.002262274050377939
Iteration: 2049, Checking gradient for hidden W[209][25]:	mock grad = -0.002211866945309171, computed grad = -0.002211866965386874
Iteration: 2049, Checking gradient for hidden b[0][28]:	mock grad = 0.015663171118118768, computed grad = 0.015663184399208246
Iteration: 2049, Checking gradient for output layer W[145][1]:	mock grad = -0.070895574115936721, computed grad = -0.070895573252605676
Iteration: 2050, Checking gradient for words E[102][39]:	mock grad = -0.040076874549488384, computed grad = -0.040076874604659703
Iteration: 2050, Checking gradient for hidden W[41][32]:	mock grad = -0.001474755078306744, computed grad = -0.001474755082633703
Iteration: 2050, Checking gradient for hidden b[0][41]:	mock grad = -0.032933270806390080, computed grad = -0.032933275626127904
Iteration: 2050, Checking gradient for output layer W[14][1]:	mock grad = 0.007034087644769427, computed grad = 0.007034087644609264
Iteration: 2051, Checking gradient for words E[69][10]:	mock grad = 0.009055741268915174, computed grad = 0.009055741294794185
Iteration: 2051, Checking gradient for hidden W[189][47]:	mock grad = -0.013573722012549805, computed grad = -0.013573722138346225
Iteration: 2051, Checking gradient for hidden b[0][25]:	mock grad = -0.004409407671157606, computed grad = -0.004409405036707000
Iteration: 2051, Checking gradient for output layer W[120][0]:	mock grad = -0.015169965829342491, computed grad = -0.015169965825691521
Iteration: 2052, Checking gradient for words E[48][11]:	mock grad = -0.008694417378718455, computed grad = -0.008694417469769376
Iteration: 2052, Checking gradient for hidden W[25][22]:	mock grad = 0.004461840866165634, computed grad = 0.004461840878627614
Iteration: 2052, Checking gradient for hidden b[0][26]:	mock grad = 0.028035509735402364, computed grad = 0.028035518144074591
Iteration: 2052, Checking gradient for output layer W[48][1]:	mock grad = 0.062965812620857653, computed grad = 0.062965812387406089
Iteration: 2053, Checking gradient for words E[627][14]:	mock grad = -0.034076796637250162, computed grad = -0.034076796639025776
Iteration: 2053, Checking gradient for hidden W[107][5]:	mock grad = -0.011207940838198205, computed grad = -0.011207940840357830
Iteration: 2053, Checking gradient for hidden b[0][6]:	mock grad = 0.030141549718643024, computed grad = 0.030141560715142644
Iteration: 2053, Checking gradient for output layer W[55][0]:	mock grad = -0.121961282788934922, computed grad = -0.121961280342513287
Iteration: 2054, Checking gradient for words E[48][40]:	mock grad = -0.037757607085922573, computed grad = -0.037757606910123399
Iteration: 2054, Checking gradient for hidden W[94][46]:	mock grad = -0.004782848225720615, computed grad = -0.004782848256923830
Iteration: 2054, Checking gradient for hidden b[0][2]:	mock grad = -0.033963775140852248, computed grad = -0.033963781207090551
Iteration: 2054, Checking gradient for output layer W[22][0]:	mock grad = -0.015194129028853198, computed grad = -0.015194129019672062
Iteration: 2055, Checking gradient for words E[48][0]:	mock grad = 0.000649657003456983, computed grad = 0.000649657087344605
Iteration: 2055, Checking gradient for hidden W[20][46]:	mock grad = 0.002118768146139316, computed grad = 0.002118768149037930
Iteration: 2055, Checking gradient for hidden b[0][0]:	mock grad = 0.109297277038067353, computed grad = 0.109297305520528368
Iteration: 2055, Checking gradient for output layer W[111][1]:	mock grad = 0.085362917489117951, computed grad = 0.085362916357741736
Iteration: 2056, Checking gradient for words E[48][28]:	mock grad = -0.011455972865559438, computed grad = -0.011455972778568902
Iteration: 2056, Checking gradient for hidden W[199][43]:	mock grad = -0.002266439939585529, computed grad = -0.002266439957353331
Iteration: 2056, Checking gradient for hidden b[0][40]:	mock grad = -0.114740024476506219, computed grad = -0.114740056414426139
Iteration: 2056, Checking gradient for output layer W[85][1]:	mock grad = -0.162572008865713435, computed grad = -0.162572007620269243
Iteration: 2057, Checking gradient for words E[104][34]:	mock grad = 0.001662679061542605, computed grad = 0.001662679048290601
Iteration: 2057, Checking gradient for hidden W[21][35]:	mock grad = -0.002196574666019080, computed grad = -0.002196574732883931
Iteration: 2057, Checking gradient for hidden b[0][42]:	mock grad = -0.044132518682710176, computed grad = -0.044132524952571750
Iteration: 2057, Checking gradient for output layer W[83][0]:	mock grad = -0.138225877111430862, computed grad = -0.138225873966717372
Iteration: 2058, Checking gradient for words E[541][47]:	mock grad = 0.014581085277715067, computed grad = 0.014581085302298354
Iteration: 2058, Checking gradient for hidden W[161][17]:	mock grad = -0.005103400580075412, computed grad = -0.005103400631797508
Iteration: 2058, Checking gradient for hidden b[0][49]:	mock grad = -0.027668750814446863, computed grad = -0.027668748202020189
Iteration: 2058, Checking gradient for output layer W[147][1]:	mock grad = 0.029120983646296894, computed grad = 0.029120983625681916
current: 60, Cost = 0.2725, Correct(%) = 1, time = 4.13791
Iteration: 2059, Checking gradient for words E[203][35]:	mock grad = 0.017154245882000652, computed grad = 0.017154245936112231
Iteration: 2059, Checking gradient for hidden W[207][49]:	mock grad = 0.003661790026504574, computed grad = 0.003661790037064066
Iteration: 2059, Checking gradient for hidden b[0][12]:	mock grad = 0.053976132165289847, computed grad = 0.053976147755234349
Iteration: 2059, Checking gradient for output layer W[15][1]:	mock grad = 0.080028707461377024, computed grad = 0.080028706863476962
Iteration: 2060, Checking gradient for words E[654][31]:	mock grad = 0.007883224178634185, computed grad = 0.007883224207169411
Iteration: 2060, Checking gradient for hidden W[57][21]:	mock grad = 0.001784754356859430, computed grad = 0.001784754358485745
Iteration: 2060, Checking gradient for hidden b[0][49]:	mock grad = 0.038844388296460641, computed grad = 0.038844398615030445
Iteration: 2060, Checking gradient for output layer W[61][0]:	mock grad = -0.140750907098807776, computed grad = -0.140750905391123837
Iteration: 2061, Checking gradient for words E[526][31]:	mock grad = -0.012176451849560399, computed grad = -0.012176451889564300
Iteration: 2061, Checking gradient for hidden W[172][12]:	mock grad = 0.003930381183625542, computed grad = 0.003930381209255989
Iteration: 2061, Checking gradient for hidden b[0][41]:	mock grad = 0.027957890362889648, computed grad = 0.027957894807044206
Iteration: 2061, Checking gradient for output layer W[149][1]:	mock grad = 0.100300493440536043, computed grad = 0.100300492646782563
Iteration: 2062, Checking gradient for words E[658][9]:	mock grad = 0.030271497435813144, computed grad = 0.030271497413334007
Iteration: 2062, Checking gradient for hidden W[117][8]:	mock grad = -0.002302883937266298, computed grad = -0.002302883965736549
Iteration: 2062, Checking gradient for hidden b[0][5]:	mock grad = -0.009012583577522326, computed grad = -0.009012579026399610
Iteration: 2062, Checking gradient for output layer W[28][1]:	mock grad = 0.052843941922481319, computed grad = 0.052843941763191939
Iteration: 2063, Checking gradient for words E[661][40]:	mock grad = -0.010399586721138165, computed grad = -0.010399586695117698
Iteration: 2063, Checking gradient for hidden W[7][4]:	mock grad = -0.000706963264512916, computed grad = -0.000706963269765008
Iteration: 2063, Checking gradient for hidden b[0][3]:	mock grad = 0.032051007005318155, computed grad = 0.032051012606134827
Iteration: 2063, Checking gradient for output layer W[118][1]:	mock grad = 0.034043272377048961, computed grad = 0.034043272334052008
Iteration: 2064, Checking gradient for words E[112][1]:	mock grad = 0.022054843098501253, computed grad = 0.022054843081210918
Iteration: 2064, Checking gradient for hidden W[11][42]:	mock grad = 0.007297335907929803, computed grad = 0.007297335949935267
Iteration: 2064, Checking gradient for hidden b[0][39]:	mock grad = -0.073048297140360097, computed grad = -0.073048324793211369
Iteration: 2064, Checking gradient for output layer W[36][1]:	mock grad = 0.030611255962598172, computed grad = 0.030611255943774379
Iteration: 2065, Checking gradient for words E[19][43]:	mock grad = -0.008362597649719250, computed grad = -0.008362597613258062
Iteration: 2065, Checking gradient for hidden W[27][16]:	mock grad = -0.003294282333365928, computed grad = -0.003294282397812300
Iteration: 2065, Checking gradient for hidden b[0][44]:	mock grad = 0.012819992635054733, computed grad = 0.012820003648653568
Iteration: 2065, Checking gradient for output layer W[33][0]:	mock grad = -0.011993688176670236, computed grad = -0.011993688171692406
Iteration: 2066, Checking gradient for words E[145][28]:	mock grad = 0.000801556515189761, computed grad = 0.000801556519504504
Iteration: 2066, Checking gradient for hidden W[48][46]:	mock grad = -0.006980144112100906, computed grad = -0.006980144149757118
Iteration: 2066, Checking gradient for hidden b[0][16]:	mock grad = 0.017452573451148812, computed grad = 0.017452581549209301
Iteration: 2066, Checking gradient for output layer W[12][0]:	mock grad = -0.041806614699713140, computed grad = -0.041806614595393678
Iteration: 2067, Checking gradient for words E[487][25]:	mock grad = 0.008593175763849104, computed grad = 0.008593175760308979
Iteration: 2067, Checking gradient for hidden W[26][23]:	mock grad = -0.005039559709874020, computed grad = -0.005039559782544819
Iteration: 2067, Checking gradient for hidden b[0][42]:	mock grad = 0.052022071061752495, computed grad = 0.052022087163572324
Iteration: 2067, Checking gradient for output layer W[103][1]:	mock grad = 0.097436705567555126, computed grad = 0.097436704589415307
Iteration: 2068, Checking gradient for words E[480][21]:	mock grad = 0.022658538713013487, computed grad = 0.022658538758325363
Iteration: 2068, Checking gradient for hidden W[212][17]:	mock grad = -0.000688832081000013, computed grad = -0.000688832085937446
Iteration: 2068, Checking gradient for hidden b[0][48]:	mock grad = -0.061619954453154269, computed grad = -0.061619973037754773
Iteration: 2068, Checking gradient for output layer W[101][1]:	mock grad = 0.049353093214202226, computed grad = 0.049353092986815048
current: 70, Cost = 0.216693, Correct(%) = 1, time = 4.74942
Iteration: 2069, Checking gradient for words E[677][42]:	mock grad = -0.005762654065072370, computed grad = -0.005762654066963920
Iteration: 2069, Checking gradient for hidden W[209][1]:	mock grad = 0.011942804481301916, computed grad = 0.011942804600262497
Iteration: 2069, Checking gradient for hidden b[0][22]:	mock grad = -0.024446965037533408, computed grad = -0.024446968045033102
Iteration: 2069, Checking gradient for output layer W[28][0]:	mock grad = 0.018572847560890948, computed grad = 0.018572847547072318
Iteration: 2070, Checking gradient for words E[684][38]:	mock grad = -0.022090916399930549, computed grad = -0.022090916372740493
Iteration: 2070, Checking gradient for hidden W[64][39]:	mock grad = -0.007094605478541460, computed grad = -0.007094605545278512
Iteration: 2070, Checking gradient for hidden b[0][10]:	mock grad = -0.073976163805222761, computed grad = -0.073976184354012292
Iteration: 2070, Checking gradient for output layer W[75][0]:	mock grad = 0.125629649511904695, computed grad = 0.125629648185400816
Iteration: 2071, Checking gradient for words E[410][21]:	mock grad = -0.004569783357083468, computed grad = -0.006543818062518382
Iteration: 2071, Checking gradient for hidden W[9][0]:	mock grad = 0.006886329248861900, computed grad = 0.006886329243451503
Iteration: 2071, Checking gradient for hidden b[0][34]:	mock grad = 0.023996442242324445, computed grad = 0.023996447805429911
Iteration: 2071, Checking gradient for output layer W[42][1]:	mock grad = 0.030909175202736838, computed grad = 0.030909175179835807
Iteration: 2072, Checking gradient for words E[48][0]:	mock grad = 0.001582240205161289, computed grad = 0.001582240256674744
Iteration: 2072, Checking gradient for hidden W[30][23]:	mock grad = 0.001393074147898465, computed grad = 0.001393074153806553
Iteration: 2072, Checking gradient for hidden b[0][13]:	mock grad = 0.106714112960148988, computed grad = 0.106714135063005658
Iteration: 2072, Checking gradient for output layer W[136][0]:	mock grad = -0.163321830332130435, computed grad = -0.163321827500542638
Iteration: 2073, Checking gradient for words E[166][35]:	mock grad = 0.009551700613397163, computed grad = 0.009551700598400280
Iteration: 2073, Checking gradient for hidden W[40][0]:	mock grad = -0.014449767331892183, computed grad = -0.014449767356747764
Iteration: 2073, Checking gradient for hidden b[0][31]:	mock grad = -0.040436657109288454, computed grad = -0.040436670385110694
Iteration: 2073, Checking gradient for output layer W[42][1]:	mock grad = -0.032075047078167218, computed grad = -0.032075047061961938
Iteration: 2074, Checking gradient for words E[609][19]:	mock grad = 0.004335518325376819, computed grad = 0.004335518367886489
Iteration: 2074, Checking gradient for hidden W[212][20]:	mock grad = 0.004066549285308696, computed grad = 0.004066549335036729
Iteration: 2074, Checking gradient for hidden b[0][28]:	mock grad = -0.005482954139157670, computed grad = -0.005482963023666403
Iteration: 2074, Checking gradient for output layer W[57][1]:	mock grad = -0.114842615704935369, computed grad = -0.114842614887796249
Iteration: 2075, Checking gradient for words E[510][18]:	mock grad = 0.011662462372025173, computed grad = 0.011662462388475760
Iteration: 2075, Checking gradient for hidden W[126][1]:	mock grad = 0.016306853951861267, computed grad = 0.016306854138863409
Iteration: 2075, Checking gradient for hidden b[0][40]:	mock grad = -0.068093568510049773, computed grad = -0.068093582651914619
Iteration: 2075, Checking gradient for output layer W[94][0]:	mock grad = 0.142366412695188460, computed grad = 0.142366408021294966
Iteration: 2076, Checking gradient for words E[76][44]:	mock grad = -0.002582851874793901, computed grad = -0.002582851849551961
Iteration: 2076, Checking gradient for hidden W[98][9]:	mock grad = 0.005952893692207084, computed grad = 0.005952893675142213
Iteration: 2076, Checking gradient for hidden b[0][0]:	mock grad = -0.133779641888126521, computed grad = -0.133779678745515218
Iteration: 2076, Checking gradient for output layer W[21][1]:	mock grad = -0.022235521286551041, computed grad = -0.022235521274976057
Iteration: 2077, Checking gradient for words E[397][29]:	mock grad = -0.001867637110047893, computed grad = -0.001867637076772479
Iteration: 2077, Checking gradient for hidden W[55][41]:	mock grad = -0.001275234293038330, computed grad = -0.001275234297956080
Iteration: 2077, Checking gradient for hidden b[0][12]:	mock grad = 0.045514023437012718, computed grad = 0.045514038746984391
Iteration: 2077, Checking gradient for output layer W[23][1]:	mock grad = -0.074613395545680694, computed grad = -0.074613394673555877
Iteration: 2078, Checking gradient for words E[25][5]:	mock grad = -0.017525359444492405, computed grad = -0.017525359388106402
Iteration: 2078, Checking gradient for hidden W[67][40]:	mock grad = 0.000442742457862355, computed grad = 0.000442742437240775
Iteration: 2078, Checking gradient for hidden b[0][22]:	mock grad = -0.032523497367598075, computed grad = -0.032523501840923431
Iteration: 2078, Checking gradient for output layer W[140][0]:	mock grad = -0.108967000636311928, computed grad = -0.108966999345547128
current: 80, Cost = 0.233356, Correct(%) = 1, time = 5.23244
Iteration: 2079, Checking gradient for words E[156][32]:	mock grad = -0.002178363181587484, computed grad = -0.002178363188347299
Iteration: 2079, Checking gradient for hidden W[55][41]:	mock grad = 0.005921891379379041, computed grad = 0.005921891390390972
Iteration: 2079, Checking gradient for hidden b[0][19]:	mock grad = -0.031515239784965510, computed grad = -0.031515246655247943
Iteration: 2079, Checking gradient for output layer W[18][1]:	mock grad = 0.043292167228647926, computed grad = 0.043292167084372946
Iteration: 2080, Checking gradient for words E[306][16]:	mock grad = -0.011550157007128581, computed grad = -0.011550157040159706
Iteration: 2080, Checking gradient for hidden W[204][15]:	mock grad = 0.006908229626456652, computed grad = 0.006908229660881383
Iteration: 2080, Checking gradient for hidden b[0][32]:	mock grad = -0.044492268636453502, computed grad = -0.044492283339357439
Iteration: 2080, Checking gradient for output layer W[99][0]:	mock grad = 0.147341552818008870, computed grad = 0.147341549498468866
Iteration: 2081, Checking gradient for words E[33][27]:	mock grad = -0.004618068713310719, computed grad = -0.004618068701966160
Iteration: 2081, Checking gradient for hidden W[28][35]:	mock grad = 0.000683622897179603, computed grad = 0.000683622888691620
Iteration: 2081, Checking gradient for hidden b[0][45]:	mock grad = -0.005324365268080666, computed grad = -0.005324365068058872
Iteration: 2081, Checking gradient for output layer W[42][1]:	mock grad = -0.021585724995124012, computed grad = -0.021585724979652793
Iteration: 2082, Checking gradient for words E[438][41]:	mock grad = 0.001439309050360604, computed grad = 0.001439309059784784
Iteration: 2082, Checking gradient for hidden W[145][40]:	mock grad = 0.002710037726894043, computed grad = 0.004617339879983546
Iteration: 2082, Checking gradient for hidden b[0][30]:	mock grad = -0.002771403472351386, computed grad = -0.002771404471107157
Iteration: 2082, Checking gradient for output layer W[31][1]:	mock grad = -0.050858601670311643, computed grad = -0.050858601487214876
Iteration: 2083, Checking gradient for words E[174][3]:	mock grad = -0.009645922720347144, computed grad = -0.009645922698810080
Iteration: 2083, Checking gradient for hidden W[38][28]:	mock grad = -0.019874289874821649, computed grad = -0.019874289912735009
Iteration: 2083, Checking gradient for hidden b[0][37]:	mock grad = 0.042168820625437808, computed grad = 0.042168826945673483
Iteration: 2083, Checking gradient for output layer W[23][1]:	mock grad = -0.070539049654733121, computed grad = -0.070539049353606817
Iteration: 2084, Checking gradient for words E[82][13]:	mock grad = -0.005840722709343771, computed grad = -0.005840722685284105
Iteration: 2084, Checking gradient for hidden W[164][3]:	mock grad = 0.000528293417803583, computed grad = 0.000528293413184565
Iteration: 2084, Checking gradient for hidden b[0][21]:	mock grad = -0.032941622700538575, computed grad = -0.032941629136322544
Iteration: 2084, Checking gradient for output layer W[17][0]:	mock grad = 0.025016349124926118, computed grad = 0.025016349095883038
Iteration: 2085, Checking gradient for words E[525][12]:	mock grad = -0.001127059051239665, computed grad = -0.001127059033591076
Iteration: 2085, Checking gradient for hidden W[67][0]:	mock grad = 0.019096511628069468, computed grad = 0.019096511915627421
Iteration: 2085, Checking gradient for hidden b[0][0]:	mock grad = 0.107607672090864748, computed grad = 0.107607695842233383
Iteration: 2085, Checking gradient for output layer W[11][0]:	mock grad = 0.020428577405612325, computed grad = 0.020428577390110964
Iteration: 2086, Checking gradient for words E[10][42]:	mock grad = 0.007990642218291821, computed grad = 0.007990642246909863
Iteration: 2086, Checking gradient for hidden W[94][37]:	mock grad = 0.010304068300909242, computed grad = 0.010304068352260403
Iteration: 2086, Checking gradient for hidden b[0][17]:	mock grad = 0.013378711202632898, computed grad = 0.013378713557745692
Iteration: 2086, Checking gradient for output layer W[108][0]:	mock grad = 0.085768736474756135, computed grad = 0.085768735695384041
Iteration: 2087, Checking gradient for words E[201][45]:	mock grad = -0.001775195859476941, computed grad = -0.001775195865306713
Iteration: 2087, Checking gradient for hidden W[180][46]:	mock grad = -0.016885972426244900, computed grad = -0.016885972405308904
Iteration: 2087, Checking gradient for hidden b[0][31]:	mock grad = -0.029339596211819252, computed grad = -0.029339602265831648
Iteration: 2087, Checking gradient for output layer W[98][1]:	mock grad = -0.000098335880244216, computed grad = -0.000098335880232069
Iteration: 2088, Checking gradient for words E[124][15]:	mock grad = -0.002994242329051877, computed grad = -0.002994242343851463
Iteration: 2088, Checking gradient for hidden W[139][0]:	mock grad = -0.016555197371426189, computed grad = -0.016555197520703176
Iteration: 2088, Checking gradient for hidden b[0][19]:	mock grad = 0.021545626910979276, computed grad = 0.021545619650887551
Iteration: 2088, Checking gradient for output layer W[140][1]:	mock grad = 0.112458457424940983, computed grad = 0.112458455554438425
current: 90, Cost = 0.271436, Correct(%) = 1, time = 6.18062
Iteration: 2089, Checking gradient for words E[476][12]:	mock grad = -0.005176008341767346, computed grad = -0.005176008356836692
Iteration: 2089, Checking gradient for hidden W[175][1]:	mock grad = 0.017368603728112886, computed grad = 0.017368604042368636
Iteration: 2089, Checking gradient for hidden b[0][7]:	mock grad = -0.051578028635618223, computed grad = -0.051578030901863949
Iteration: 2089, Checking gradient for output layer W[139][1]:	mock grad = 0.002274129360269050, computed grad = 0.002274129360152000
Iteration: 2090, Checking gradient for words E[48][32]:	mock grad = -0.011587740291266191, computed grad = -0.011587740316789764
Iteration: 2090, Checking gradient for hidden W[18][28]:	mock grad = 0.011534776696034421, computed grad = 0.011534776827406439
Iteration: 2090, Checking gradient for hidden b[0][12]:	mock grad = 0.049321353550235703, computed grad = 0.049321369486331416
Iteration: 2090, Checking gradient for output layer W[124][1]:	mock grad = -0.062959180253494007, computed grad = -0.062959179825271716
Iteration: 2091, Checking gradient for words E[751][28]:	mock grad = 0.023130650336489200, computed grad = 0.023130650427524421
Iteration: 2091, Checking gradient for hidden W[171][10]:	mock grad = 0.003898510814226430, computed grad = 0.003898510833055039
Iteration: 2091, Checking gradient for hidden b[0][16]:	mock grad = 0.014666474775021365, computed grad = 0.014666478730332163
Iteration: 2091, Checking gradient for output layer W[61][0]:	mock grad = -0.152103563958699839, computed grad = -0.152103560863750764
Iteration: 2092, Checking gradient for words E[752][16]:	mock grad = 0.004596261441697225, computed grad = 0.004596261483849535
Iteration: 2092, Checking gradient for hidden W[153][21]:	mock grad = -0.000783462429501669, computed grad = -0.000783462498191446
Iteration: 2092, Checking gradient for hidden b[0][23]:	mock grad = 0.081668881630456158, computed grad = 0.081668895260974927
Iteration: 2092, Checking gradient for output layer W[130][1]:	mock grad = -0.069477957225408393, computed grad = -0.069477956351659459
Iteration: 2093, Checking gradient for words E[760][25]:	mock grad = -0.010310399585128360, computed grad = -0.010310399599566651
Iteration: 2093, Checking gradient for hidden W[85][5]:	mock grad = -0.001659366501066684, computed grad = -0.001659366492934095
Iteration: 2093, Checking gradient for hidden b[0][18]:	mock grad = -0.049367620466589379, computed grad = -0.049367615184621474
Iteration: 2093, Checking gradient for output layer W[40][0]:	mock grad = -0.023408011951941976, computed grad = -0.023408011937439872
Iteration: 2094, Checking gradient for words E[611][46]:	mock grad = 0.002357411541337173, computed grad = 0.002357411550600693
Iteration: 2094, Checking gradient for hidden W[73][31]:	mock grad = 0.010726196079363648, computed grad = 0.010726196291635744
Iteration: 2094, Checking gradient for hidden b[0][42]:	mock grad = 0.040125507827296070, computed grad = 0.040125520759257932
Iteration: 2094, Checking gradient for output layer W[40][1]:	mock grad = 0.026923192888828051, computed grad = 0.026923192842433677
Iteration: 2095, Checking gradient for words E[148][32]:	mock grad = -0.008375570619778561, computed grad = -0.008375570622878289
Iteration: 2095, Checking gradient for hidden W[17][20]:	mock grad = -0.006884133374546186, computed grad = -0.006884133378686094
Iteration: 2095, Checking gradient for hidden b[0][27]:	mock grad = -0.030233768839788011, computed grad = -0.030233779384773250
Iteration: 2095, Checking gradient for output layer W[107][0]:	mock grad = -0.027018035234332993, computed grad = -0.027018035217625601
Iteration: 2096, Checking gradient for words E[603][20]:	mock grad = 0.012575927488184258, computed grad = 0.012575927448158742
Iteration: 2096, Checking gradient for hidden W[183][29]:	mock grad = -0.005590535187255385, computed grad = -0.005590535176125741
Iteration: 2096, Checking gradient for hidden b[0][7]:	mock grad = 0.050978419017369547, computed grad = 0.050978428035983819
Iteration: 2096, Checking gradient for output layer W[33][0]:	mock grad = -0.024415039110742276, computed grad = -0.024415039087949866
Iteration: 2097, Checking gradient for words E[411][4]:	mock grad = -0.005129516063628881, computed grad = -0.005079219639270141
Iteration: 2097, Checking gradient for hidden W[212][45]:	mock grad = -0.003257413292268030, computed grad = -0.003257413304124152
Iteration: 2097, Checking gradient for hidden b[0][47]:	mock grad = 0.064643003385489450, computed grad = 0.064643011133289865
Iteration: 2097, Checking gradient for output layer W[141][1]:	mock grad = 0.122873014197028008, computed grad = 0.122873013377347615
Iteration: 2098, Checking gradient for words E[102][22]:	mock grad = 0.002692198521572764, computed grad = 0.002692198523614264
Iteration: 2098, Checking gradient for hidden W[156][10]:	mock grad = 0.002743849115971475, computed grad = 0.002743849182683160
Iteration: 2098, Checking gradient for hidden b[0][18]:	mock grad = -0.037029295984578048, computed grad = -0.037029286096912951
Iteration: 2098, Checking gradient for output layer W[105][0]:	mock grad = -0.101291367255951226, computed grad = -0.101291365456815566
current: 100, Cost = 0.274221, Correct(%) = 1, time = 6.96305
Iteration: 2099, Checking gradient for words E[102][33]:	mock grad = -0.004327979692686501, computed grad = -0.004327979692742762
Iteration: 2099, Checking gradient for hidden W[169][1]:	mock grad = 0.001611547341323627, computed grad = 0.001611547367515735
Iteration: 2099, Checking gradient for hidden b[0][39]:	mock grad = 0.063553378128494709, computed grad = 0.063553396955400493
Iteration: 2099, Checking gradient for output layer W[34][0]:	mock grad = 0.013698027666320112, computed grad = 0.013698027663350825
current: 21, Correct(%) = 1, time = 7.01397
Dev start.
Dev finished. Total time taken is: 0.634482
dev:
Accuracy:	P=100/100=1
Test start.
Test finished. Total time taken is: 0.545402
test:
Accuracy:	P=63/100=0.63
##### Iteration 21
random: 0, 99
Iteration: 2100, Checking gradient for words E[21][23]:	mock grad = 0.012297902722141485, computed grad = 0.012297902781013930
Iteration: 2100, Checking gradient for hidden W[38][45]:	mock grad = -0.001300590133271329, computed grad = -0.001300590143525839
Iteration: 2100, Checking gradient for hidden b[0][5]:	mock grad = -0.005278302941641444, computed grad = -0.005278298521909626
Iteration: 2100, Checking gradient for output layer W[11][0]:	mock grad = -0.029370835047448640, computed grad = -0.029370834974685858
Iteration: 2101, Checking gradient for words E[68][0]:	mock grad = -0.024378700077182236, computed grad = -0.024378700047684002
Iteration: 2101, Checking gradient for hidden W[139][37]:	mock grad = 0.008487602116247217, computed grad = 0.008487602158171157
Iteration: 2101, Checking gradient for hidden b[0][43]:	mock grad = -0.008471432693613057, computed grad = -0.008471435145877509
Iteration: 2101, Checking gradient for output layer W[15][1]:	mock grad = 0.055129023895567064, computed grad = 0.055129023661501957
Iteration: 2102, Checking gradient for words E[102][17]:	mock grad = -0.004613491851712670, computed grad = -0.003582210776233846
Iteration: 2102, Checking gradient for hidden W[41][43]:	mock grad = 0.001125128100812334, computed grad = 0.001125128106104318
Iteration: 2102, Checking gradient for hidden b[0][6]:	mock grad = -0.025141746803652998, computed grad = -0.025141754489961036
Iteration: 2102, Checking gradient for output layer W[129][1]:	mock grad = 0.043303696654090063, computed grad = 0.043303696515350543
Iteration: 2103, Checking gradient for words E[139][47]:	mock grad = 0.004624162873667625, computed grad = 0.004624162878548164
Iteration: 2103, Checking gradient for hidden W[180][3]:	mock grad = -0.001308284435896923, computed grad = -0.001308284446568216
Iteration: 2103, Checking gradient for hidden b[0][17]:	mock grad = -0.014535260663944838, computed grad = -0.014535263936737118
Iteration: 2103, Checking gradient for output layer W[103][0]:	mock grad = -0.097372613905716277, computed grad = -0.097372613016625323
Iteration: 2104, Checking gradient for words E[102][20]:	mock grad = 0.002826971911340515, computed grad = 0.002826971934097737
Iteration: 2104, Checking gradient for hidden W[85][35]:	mock grad = -0.001832743516727575, computed grad = -0.001832743545675660
Iteration: 2104, Checking gradient for hidden b[0][15]:	mock grad = 0.073237258985864306, computed grad = 0.073237277580253210
Iteration: 2104, Checking gradient for output layer W[61][0]:	mock grad = 0.107726462477863993, computed grad = 0.107726461031218426
Iteration: 2105, Checking gradient for words E[180][25]:	mock grad = 0.000938751796214499, computed grad = 0.000938751799460997
Iteration: 2105, Checking gradient for hidden W[199][33]:	mock grad = -0.003744872175803771, computed grad = -0.006110953364201930
Iteration: 2105, Checking gradient for hidden b[0][13]:	mock grad = 0.085366060188468706, computed grad = 0.085366075924840357
Iteration: 2105, Checking gradient for output layer W[26][0]:	mock grad = -0.036639934516663475, computed grad = -0.036639934463193032
Iteration: 2106, Checking gradient for words E[267][19]:	mock grad = 0.004243734233272400, computed grad = 0.004243734246123643
Iteration: 2106, Checking gradient for hidden W[38][18]:	mock grad = 0.003727793143920577, computed grad = 0.003727793106717009
Iteration: 2106, Checking gradient for hidden b[0][31]:	mock grad = 0.024254636116161077, computed grad = 0.024254645306551293
Iteration: 2106, Checking gradient for output layer W[68][0]:	mock grad = 0.143786651671015675, computed grad = 0.143786644437844474
Iteration: 2107, Checking gradient for words E[25][31]:	mock grad = 0.025791487744120101, computed grad = 0.025791487721028496
Iteration: 2107, Checking gradient for hidden W[197][3]:	mock grad = 0.006601062044919725, computed grad = 0.006601062121802678
Iteration: 2107, Checking gradient for hidden b[0][19]:	mock grad = -0.058802130597418456, computed grad = -0.058802149450959400
Iteration: 2107, Checking gradient for output layer W[102][1]:	mock grad = -0.095406079687609946, computed grad = -0.095406079405200822
Iteration: 2108, Checking gradient for words E[99][34]:	mock grad = -0.021551943584802169, computed grad = -0.021388120011439360
Iteration: 2108, Checking gradient for hidden W[128][45]:	mock grad = 0.000135530654962746, computed grad = 0.000135530652788445
Iteration: 2108, Checking gradient for hidden b[0][42]:	mock grad = 0.046865255723033128, computed grad = 0.046865270050596097
Iteration: 2108, Checking gradient for output layer W[10][0]:	mock grad = -0.019285805658048050, computed grad = -0.019285805647366816
current: 10, Cost = 0.285511, Correct(%) = 1, time = 0.76984
Iteration: 2109, Checking gradient for words E[74][1]:	mock grad = -0.017454330904526083, computed grad = -0.018001955600134213
Iteration: 2109, Checking gradient for hidden W[50][38]:	mock grad = -0.001185834414163534, computed grad = -0.001185834416704368
Iteration: 2109, Checking gradient for hidden b[0][1]:	mock grad = -0.079991576500987316, computed grad = -0.079991588270477851
Iteration: 2109, Checking gradient for output layer W[66][1]:	mock grad = 0.049965076417912702, computed grad = 0.049965076290526572
Iteration: 2110, Checking gradient for words E[309][34]:	mock grad = 0.001438865453604055, computed grad = 0.001438865414190889
Iteration: 2110, Checking gradient for hidden W[24][27]:	mock grad = 0.001744964625877143, computed grad = 0.001744964635573044
Iteration: 2110, Checking gradient for hidden b[0][33]:	mock grad = 0.060848212843689353, computed grad = 0.060848224272539671
Iteration: 2110, Checking gradient for output layer W[106][0]:	mock grad = -0.065995289478079644, computed grad = -0.065995288900557891
Iteration: 2111, Checking gradient for words E[329][3]:	mock grad = -0.038287458908853322, computed grad = -0.038287458962283152
Iteration: 2111, Checking gradient for hidden W[19][25]:	mock grad = 0.000625967211592693, computed grad = 0.000625967221042597
Iteration: 2111, Checking gradient for hidden b[0][14]:	mock grad = -0.048875515049506824, computed grad = -0.048875532778136374
Iteration: 2111, Checking gradient for output layer W[105][1]:	mock grad = -0.058462762934807788, computed grad = -0.058462762822377043
Iteration: 2112, Checking gradient for words E[188][15]:	mock grad = -0.019439656071262856, computed grad = -0.019439656127554800
Iteration: 2112, Checking gradient for hidden W[170][36]:	mock grad = -0.004665777012996530, computed grad = -0.004665777013979701
Iteration: 2112, Checking gradient for hidden b[0][14]:	mock grad = 0.039441043901591866, computed grad = 0.039441055975056573
Iteration: 2112, Checking gradient for output layer W[66][0]:	mock grad = 0.023597422983204219, computed grad = 0.023597422960263133
Iteration: 2113, Checking gradient for words E[371][18]:	mock grad = 0.000801641445766221, computed grad = 0.000801641415445717
Iteration: 2113, Checking gradient for hidden W[146][25]:	mock grad = -0.003168783720711166, computed grad = -0.003168783880300280
Iteration: 2113, Checking gradient for hidden b[0][6]:	mock grad = 0.024040120541732324, computed grad = 0.024040128524874586
Iteration: 2113, Checking gradient for output layer W[124][0]:	mock grad = 0.054632015134670198, computed grad = 0.054632014799196611
Iteration: 2114, Checking gradient for words E[156][47]:	mock grad = 0.034293829796305797, computed grad = 0.034293829941205028
Iteration: 2114, Checking gradient for hidden W[134][15]:	mock grad = 0.012573878222899015, computed grad = 0.012573878347418061
Iteration: 2114, Checking gradient for hidden b[0][11]:	mock grad = 0.108539231855303564, computed grad = 0.108539264351524861
Iteration: 2114, Checking gradient for output layer W[73][0]:	mock grad = 0.013254361621806154, computed grad = 0.013254361621238366
Iteration: 2115, Checking gradient for words E[383][40]:	mock grad = -0.007247496946749532, computed grad = -0.007627569129459388
Iteration: 2115, Checking gradient for hidden W[206][44]:	mock grad = -0.002347035833405453, computed grad = -0.002347035834777492
Iteration: 2115, Checking gradient for hidden b[0][37]:	mock grad = 0.037357576060131903, computed grad = 0.037357580607629975
Iteration: 2115, Checking gradient for output layer W[124][1]:	mock grad = 0.049136542404931705, computed grad = 0.049136542268330453
Iteration: 2116, Checking gradient for words E[197][23]:	mock grad = -0.019887338644275498, computed grad = -0.019887338655165988
Iteration: 2116, Checking gradient for hidden W[239][4]:	mock grad = 0.009071442895414172, computed grad = 0.009071443004009434
Iteration: 2116, Checking gradient for hidden b[0][12]:	mock grad = -0.060581085955607739, computed grad = -0.060581105100639104
Iteration: 2116, Checking gradient for output layer W[63][1]:	mock grad = -0.015325933696463956, computed grad = -0.015325933693325910
Iteration: 2117, Checking gradient for words E[416][17]:	mock grad = -0.001803590455276560, computed grad = -0.001803590453034143
Iteration: 2117, Checking gradient for hidden W[106][27]:	mock grad = 0.007886738708592911, computed grad = 0.007886738689308151
Iteration: 2117, Checking gradient for hidden b[0][27]:	mock grad = -0.017370245961073616, computed grad = -0.017370249613875538
Iteration: 2117, Checking gradient for output layer W[123][0]:	mock grad = -0.111591069088368133, computed grad = -0.111591065096154227
Iteration: 2118, Checking gradient for words E[34][46]:	mock grad = 0.034841940921875869, computed grad = 0.034841940935642766
Iteration: 2118, Checking gradient for hidden W[179][18]:	mock grad = -0.008609182662849824, computed grad = -0.008609182778056225
Iteration: 2118, Checking gradient for hidden b[0][4]:	mock grad = -0.014362712784071752, computed grad = -0.014362719669308723
Iteration: 2118, Checking gradient for output layer W[21][0]:	mock grad = -0.024006612776711345, computed grad = -0.024006612750093335
current: 20, Cost = 0.39659, Correct(%) = 1, time = 1.4309
Iteration: 2119, Checking gradient for words E[49][34]:	mock grad = -0.038995217097326229, computed grad = -0.038995217208905524
Iteration: 2119, Checking gradient for hidden W[242][10]:	mock grad = 0.000315219036012282, computed grad = 0.000315219057218410
Iteration: 2119, Checking gradient for hidden b[0][29]:	mock grad = -0.083763570938949483, computed grad = -0.083763591713442984
Iteration: 2119, Checking gradient for output layer W[39][1]:	mock grad = 0.049951700211298133, computed grad = 0.049951700166237623
Iteration: 2120, Checking gradient for words E[185][22]:	mock grad = 0.022989358218872269, computed grad = 0.022989358250113251
Iteration: 2120, Checking gradient for hidden W[224][24]:	mock grad = 0.006469129047947941, computed grad = 0.006469129066142177
Iteration: 2120, Checking gradient for hidden b[0][2]:	mock grad = -0.045135613295693711, computed grad = -0.045135627656596525
Iteration: 2120, Checking gradient for output layer W[147][0]:	mock grad = -0.117452036674764448, computed grad = -0.117452034627040555
Iteration: 2121, Checking gradient for words E[6][44]:	mock grad = -0.000042064849825607, computed grad = -0.000042064892003268
Iteration: 2121, Checking gradient for hidden W[203][36]:	mock grad = -0.000879744320836373, computed grad = -0.000879744325182121
Iteration: 2121, Checking gradient for hidden b[0][15]:	mock grad = 0.109952007065428070, computed grad = 0.109952040034986764
Iteration: 2121, Checking gradient for output layer W[3][1]:	mock grad = 0.001595988624203404, computed grad = 0.001595988624284813
Iteration: 2122, Checking gradient for words E[377][48]:	mock grad = -0.003397722036641015, computed grad = -0.003551390236962455
Iteration: 2122, Checking gradient for hidden W[72][36]:	mock grad = -0.008497807047855299, computed grad = -0.008497807087305605
Iteration: 2122, Checking gradient for hidden b[0][34]:	mock grad = 0.016053916184213812, computed grad = 0.016053918164721719
Iteration: 2122, Checking gradient for output layer W[20][0]:	mock grad = 0.006268574913273484, computed grad = 0.006268574912846895
Iteration: 2123, Checking gradient for words E[446][30]:	mock grad = 0.010922627459275525, computed grad = 0.010922627435820832
Iteration: 2123, Checking gradient for hidden W[131][25]:	mock grad = -0.002945450707919628, computed grad = -0.002945450718294120
Iteration: 2123, Checking gradient for hidden b[0][2]:	mock grad = -0.038016620319769601, computed grad = -0.038016627455471783
Iteration: 2123, Checking gradient for output layer W[14][1]:	mock grad = 0.009011083827309418, computed grad = 0.009011083826401162
Iteration: 2124, Checking gradient for words E[54][23]:	mock grad = 0.002541483013124290, computed grad = 0.002541482991058183
Iteration: 2124, Checking gradient for hidden W[219][34]:	mock grad = 0.001775832178474435, computed grad = 0.001775832179129159
Iteration: 2124, Checking gradient for hidden b[0][45]:	mock grad = -0.006924857034934506, computed grad = -0.006924857809651734
Iteration: 2124, Checking gradient for output layer W[138][1]:	mock grad = -0.079655750804652481, computed grad = -0.079655750287939225
Iteration: 2125, Checking gradient for words E[316][23]:	mock grad = 0.054613016697957484, computed grad = 0.054613016553686111
Iteration: 2125, Checking gradient for hidden W[38][31]:	mock grad = -0.003873333166121151, computed grad = -0.003873333191008689
Iteration: 2125, Checking gradient for hidden b[0][47]:	mock grad = -0.064844881159764611, computed grad = -0.064844894019880209
Iteration: 2125, Checking gradient for output layer W[101][1]:	mock grad = -0.124428290812245157, computed grad = -0.124428289014565527
Iteration: 2126, Checking gradient for words E[476][41]:	mock grad = -0.001959383490124367, computed grad = -0.001959383498234304
Iteration: 2126, Checking gradient for hidden W[22][39]:	mock grad = 0.001666725488777554, computed grad = 0.001666725487658102
Iteration: 2126, Checking gradient for hidden b[0][21]:	mock grad = 0.026829279298798414, computed grad = 0.026829282752894985
Iteration: 2126, Checking gradient for output layer W[32][1]:	mock grad = -0.056274392725527700, computed grad = -0.056274392243947834
Iteration: 2127, Checking gradient for words E[485][27]:	mock grad = 0.050501349136727880, computed grad = 0.048360697538399132
Iteration: 2127, Checking gradient for hidden W[153][0]:	mock grad = 0.002597209993931981, computed grad = 0.002597210007661170
Iteration: 2127, Checking gradient for hidden b[0][45]:	mock grad = -0.009350776971117636, computed grad = -0.009350778555314555
Iteration: 2127, Checking gradient for output layer W[142][1]:	mock grad = -0.136046905963277398, computed grad = -0.136046904974212179
Iteration: 2128, Checking gradient for words E[488][39]:	mock grad = -0.002576353465136294, computed grad = -0.002576353457786381
Iteration: 2128, Checking gradient for hidden W[36][39]:	mock grad = -0.002307658792999323, computed grad = -0.002307658827485349
Iteration: 2128, Checking gradient for hidden b[0][1]:	mock grad = -0.054687862448027369, computed grad = -0.054687867010444785
Iteration: 2128, Checking gradient for output layer W[122][1]:	mock grad = -0.114493057776263174, computed grad = -0.114493054445968176
current: 30, Cost = 0.266716, Correct(%) = 1, time = 2.12712
Iteration: 2129, Checking gradient for words E[509][32]:	mock grad = 0.006455092590784695, computed grad = 0.006455092555523059
Iteration: 2129, Checking gradient for hidden W[247][27]:	mock grad = -0.000053729833388028, computed grad = -0.000053729814959698
Iteration: 2129, Checking gradient for hidden b[0][48]:	mock grad = -0.067038253422746497, computed grad = -0.067038270057791297
Iteration: 2129, Checking gradient for output layer W[104][0]:	mock grad = -0.027976333549506460, computed grad = -0.027976333522413421
Iteration: 2130, Checking gradient for words E[102][41]:	mock grad = 0.035695661767712328, computed grad = 0.035695661784470076
Iteration: 2130, Checking gradient for hidden W[61][24]:	mock grad = -0.003007673866975424, computed grad = -0.003007673893310680
Iteration: 2130, Checking gradient for hidden b[0][3]:	mock grad = -0.030560947148886020, computed grad = -0.030560958739621620
Iteration: 2130, Checking gradient for output layer W[33][1]:	mock grad = 0.012867669480151611, computed grad = 0.012867669476894845
Iteration: 2131, Checking gradient for words E[102][34]:	mock grad = -0.036103632057138402, computed grad = -0.036103632168850874
Iteration: 2131, Checking gradient for hidden W[33][10]:	mock grad = 0.013003466511229966, computed grad = 0.013003466783080260
Iteration: 2131, Checking gradient for hidden b[0][11]:	mock grad = -0.088367881859674480, computed grad = -0.088367907517535496
Iteration: 2131, Checking gradient for output layer W[77][0]:	mock grad = -0.079994470802091655, computed grad = -0.079994470534396081
Iteration: 2132, Checking gradient for words E[143][23]:	mock grad = 0.043430599306809592, computed grad = 0.040574486389760779
Iteration: 2132, Checking gradient for hidden W[147][22]:	mock grad = 0.006153263661629049, computed grad = 0.006153263850207987
Iteration: 2132, Checking gradient for hidden b[0][3]:	mock grad = -0.029558841515259227, computed grad = -0.029558850099088606
Iteration: 2132, Checking gradient for output layer W[82][1]:	mock grad = 0.142044293835158486, computed grad = 0.142044289928773437
Iteration: 2133, Checking gradient for words E[90][17]:	mock grad = -0.007471447345946958, computed grad = -0.007732427268077617
Iteration: 2133, Checking gradient for hidden W[248][36]:	mock grad = -0.004071491886512013, computed grad = -0.004071491887392229
Iteration: 2133, Checking gradient for hidden b[0][8]:	mock grad = -0.029343539894061133, computed grad = -0.029343555778127379
Iteration: 2133, Checking gradient for output layer W[55][0]:	mock grad = 0.088374117254222395, computed grad = 0.088374115959764507
Iteration: 2134, Checking gradient for words E[529][7]:	mock grad = 0.001428127100550070, computed grad = 0.001428127038182419
Iteration: 2134, Checking gradient for hidden W[78][42]:	mock grad = -0.017889627851597201, computed grad = -0.017889628129141751
Iteration: 2134, Checking gradient for hidden b[0][14]:	mock grad = -0.047749508111566419, computed grad = -0.047749527030549632
Iteration: 2134, Checking gradient for output layer W[82][0]:	mock grad = -0.170376303908481574, computed grad = -0.170376300645563722
Iteration: 2135, Checking gradient for words E[201][24]:	mock grad = -0.005753122439833280, computed grad = -0.004195920104171760
Iteration: 2135, Checking gradient for hidden W[226][26]:	mock grad = -0.004621444506719419, computed grad = -0.004621444539081023
Iteration: 2135, Checking gradient for hidden b[0][43]:	mock grad = -0.010099405920477311, computed grad = -0.010099408459791499
Iteration: 2135, Checking gradient for output layer W[83][1]:	mock grad = 0.155588047453719680, computed grad = 0.155588044886499355
Iteration: 2136, Checking gradient for words E[102][42]:	mock grad = -0.011217940631774725, computed grad = -0.011217940610988341
Iteration: 2136, Checking gradient for hidden W[127][30]:	mock grad = 0.007870420648675713, computed grad = 0.007870420761269582
Iteration: 2136, Checking gradient for hidden b[0][2]:	mock grad = 0.056063150412760665, computed grad = 0.056063175838817356
Iteration: 2136, Checking gradient for output layer W[127][0]:	mock grad = 0.089446950589572305, computed grad = 0.089446950319228502
Iteration: 2137, Checking gradient for words E[451][18]:	mock grad = 0.001062017319058972, computed grad = 0.001062017328095844
Iteration: 2137, Checking gradient for hidden W[55][30]:	mock grad = 0.006823920219140023, computed grad = 0.006823920236030305
Iteration: 2137, Checking gradient for hidden b[0][36]:	mock grad = 0.003376679965833018, computed grad = 0.003376673788689844
Iteration: 2137, Checking gradient for output layer W[4][1]:	mock grad = -0.014301754218876139, computed grad = -0.014301754215670960
Iteration: 2138, Checking gradient for words E[25][11]:	mock grad = -0.010626411863112750, computed grad = -0.009139899267427384
Iteration: 2138, Checking gradient for hidden W[167][49]:	mock grad = 0.001570903850223182, computed grad = 0.001570903847946802
Iteration: 2138, Checking gradient for hidden b[0][42]:	mock grad = -0.055533207642538551, computed grad = -0.055533218024341253
Iteration: 2138, Checking gradient for output layer W[48][0]:	mock grad = 0.084484053091721245, computed grad = 0.084484052687169811
current: 40, Cost = 0.205284, Correct(%) = 1, time = 2.56207
Iteration: 2139, Checking gradient for words E[549][35]:	mock grad = -0.007381344647866550, computed grad = -0.007381344639354958
Iteration: 2139, Checking gradient for hidden W[204][17]:	mock grad = 0.002295840934252169, computed grad = 0.002295840941623360
Iteration: 2139, Checking gradient for hidden b[0][10]:	mock grad = -0.053058419354379271, computed grad = -0.053058437122796898
Iteration: 2139, Checking gradient for output layer W[39][0]:	mock grad = 0.032401386738600824, computed grad = 0.032401386654348663
Iteration: 2140, Checking gradient for words E[332][0]:	mock grad = -0.001753630826439867, computed grad = -0.001753630857160318
Iteration: 2140, Checking gradient for hidden W[179][42]:	mock grad = 0.000410878824699812, computed grad = 0.000410878803129656
Iteration: 2140, Checking gradient for hidden b[0][37]:	mock grad = -0.032369098634493776, computed grad = -0.032369110184151478
Iteration: 2140, Checking gradient for output layer W[136][1]:	mock grad = 0.083186424282080673, computed grad = 0.083186422961464029
Iteration: 2141, Checking gradient for words E[558][35]:	mock grad = 0.019547854755136429, computed grad = 0.021227663823194041
Iteration: 2141, Checking gradient for hidden W[0][32]:	mock grad = 0.008259261805865226, computed grad = 0.008259261862515138
Iteration: 2141, Checking gradient for hidden b[0][28]:	mock grad = 0.005039195600048396, computed grad = 0.005039203644554354
Iteration: 2141, Checking gradient for output layer W[28][0]:	mock grad = -0.046506701132154049, computed grad = -0.046506701092045834
Iteration: 2142, Checking gradient for words E[6][31]:	mock grad = 0.014186987522757377, computed grad = 0.014186987509567537
Iteration: 2142, Checking gradient for hidden W[213][21]:	mock grad = 0.002411809824964806, computed grad = 0.002411809833224093
Iteration: 2142, Checking gradient for hidden b[0][21]:	mock grad = -0.053913000656269361, computed grad = -0.053913017340878139
Iteration: 2142, Checking gradient for output layer W[106][0]:	mock grad = 0.104494245097452865, computed grad = 0.104494244439758355
Iteration: 2143, Checking gradient for words E[269][47]:	mock grad = 0.008008462400899141, computed grad = 0.008008462392897467
Iteration: 2143, Checking gradient for hidden W[34][16]:	mock grad = 0.004333073434947821, computed grad = 0.004333073530147606
Iteration: 2143, Checking gradient for hidden b[0][10]:	mock grad = 0.085496632267478834, computed grad = 0.085496646968983750
Iteration: 2143, Checking gradient for output layer W[60][0]:	mock grad = -0.044318248267810612, computed grad = -0.044318248232331103
Iteration: 2144, Checking gradient for words E[85][48]:	mock grad = 0.005383465374916852, computed grad = 0.005383465370855513
Iteration: 2144, Checking gradient for hidden W[105][10]:	mock grad = 0.006166267119911417, computed grad = 0.006166267197670407
Iteration: 2144, Checking gradient for hidden b[0][39]:	mock grad = -0.043666679801282471, computed grad = -0.043666697350219261
Iteration: 2144, Checking gradient for output layer W[28][1]:	mock grad = -0.023517612723164549, computed grad = -0.023517612677473591
Iteration: 2145, Checking gradient for words E[194][1]:	mock grad = 0.006104422873798310, computed grad = 0.006104422901369031
Iteration: 2145, Checking gradient for hidden W[234][46]:	mock grad = -0.008162708453218759, computed grad = -0.008162708529081030
Iteration: 2145, Checking gradient for hidden b[0][1]:	mock grad = -0.094722259052770763, computed grad = -0.094722285266157460
Iteration: 2145, Checking gradient for output layer W[82][1]:	mock grad = 0.139668189296698886, computed grad = 0.139668187111546327
Iteration: 2146, Checking gradient for words E[160][23]:	mock grad = 0.010759494146389548, computed grad = 0.010759494136929802
Iteration: 2146, Checking gradient for hidden W[229][32]:	mock grad = -0.003724916621941632, computed grad = -0.003724916638418914
Iteration: 2146, Checking gradient for hidden b[0][34]:	mock grad = -0.013117216771441065, computed grad = -0.013117214921464060
Iteration: 2146, Checking gradient for output layer W[84][0]:	mock grad = -0.090931370966010339, computed grad = -0.090931369244099725
Iteration: 2147, Checking gradient for words E[600][5]:	mock grad = -0.001149916001405571, computed grad = -0.001149915981822202
Iteration: 2147, Checking gradient for hidden W[109][47]:	mock grad = -0.012056577495256349, computed grad = -0.012056577604895454
Iteration: 2147, Checking gradient for hidden b[0][21]:	mock grad = 0.035220891889078954, computed grad = 0.035220898756097220
Iteration: 2147, Checking gradient for output layer W[74][0]:	mock grad = 0.092609267122914507, computed grad = 0.092609265816652686
Iteration: 2148, Checking gradient for words E[588][18]:	mock grad = -0.001850266094782782, computed grad = -0.001412722163522526
Iteration: 2148, Checking gradient for hidden W[174][18]:	mock grad = -0.001795986918490255, computed grad = -0.001795986886386002
Iteration: 2148, Checking gradient for hidden b[0][22]:	mock grad = 0.021099333102903017, computed grad = 0.021099333043981518
Iteration: 2148, Checking gradient for output layer W[143][0]:	mock grad = 0.118972108508488827, computed grad = 0.118972105308367174
current: 50, Cost = 0.20232, Correct(%) = 1, time = 3.45048
Iteration: 2149, Checking gradient for words E[74][0]:	mock grad = 0.006844014371357177, computed grad = 0.006844014375529433
Iteration: 2149, Checking gradient for hidden W[156][43]:	mock grad = -0.000646262866957348, computed grad = -0.000646262868471541
Iteration: 2149, Checking gradient for hidden b[0][44]:	mock grad = 0.002188092532953712, computed grad = 0.002188090818871359
Iteration: 2149, Checking gradient for output layer W[39][0]:	mock grad = -0.020308499307092331, computed grad = -0.020308499285571262
Iteration: 2150, Checking gradient for words E[48][46]:	mock grad = -0.056154605119512757, computed grad = -0.056154605069725111
Iteration: 2150, Checking gradient for hidden W[44][25]:	mock grad = -0.000179420653834139, computed grad = -0.000179420663800223
Iteration: 2150, Checking gradient for hidden b[0][16]:	mock grad = 0.017618105247363403, computed grad = 0.017618110730018673
Iteration: 2150, Checking gradient for output layer W[149][1]:	mock grad = -0.010356539822148347, computed grad = -0.010356539821632019
Iteration: 2151, Checking gradient for words E[619][3]:	mock grad = 0.002682892615873955, computed grad = 0.002682892617427299
Iteration: 2151, Checking gradient for hidden W[22][18]:	mock grad = -0.001562741682420432, computed grad = -0.001562741675276991
Iteration: 2151, Checking gradient for hidden b[0][33]:	mock grad = 0.072387795254758025, computed grad = 0.072387809328807270
Iteration: 2151, Checking gradient for output layer W[32][1]:	mock grad = -0.065931102669974928, computed grad = -0.065931102357272425
Iteration: 2152, Checking gradient for words E[623][4]:	mock grad = 0.001305539443080539, computed grad = 0.004651976998041155
Iteration: 2152, Checking gradient for hidden W[105][35]:	mock grad = 0.001907843745874160, computed grad = 0.001907843739178354
Iteration: 2152, Checking gradient for hidden b[0][23]:	mock grad = -0.116255329232844984, computed grad = -0.116255366135471161
Iteration: 2152, Checking gradient for output layer W[22][1]:	mock grad = -0.000321331730002150, computed grad = -0.000321331729962939
Iteration: 2153, Checking gradient for words E[776][4]:	mock grad = -0.005252108409598000, computed grad = -0.005252108451376697
Iteration: 2153, Checking gradient for hidden W[177][15]:	mock grad = 0.003700727423783245, computed grad = 0.003700727421640211
Iteration: 2153, Checking gradient for hidden b[0][14]:	mock grad = -0.038790633162255617, computed grad = -0.038790648790176713
Iteration: 2153, Checking gradient for output layer W[20][1]:	mock grad = 0.007223159012553770, computed grad = 0.007223159012028093
Iteration: 2154, Checking gradient for words E[481][16]:	mock grad = 0.006964629127567679, computed grad = 0.006964629128238486
Iteration: 2154, Checking gradient for hidden W[131][14]:	mock grad = 0.001799582943176614, computed grad = 0.001799582948592747
Iteration: 2154, Checking gradient for hidden b[0][46]:	mock grad = -0.078999859140282958, computed grad = -0.078999872707801372
Iteration: 2154, Checking gradient for output layer W[77][1]:	mock grad = -0.060167798185670196, computed grad = -0.060167797580941332
Iteration: 2155, Checking gradient for words E[776][47]:	mock grad = -0.015287986154396971, computed grad = -0.013538297979073593
Iteration: 2155, Checking gradient for hidden W[116][26]:	mock grad = 0.002022250721725460, computed grad = 0.002022250746833107
Iteration: 2155, Checking gradient for hidden b[0][42]:	mock grad = 0.043178503672400281, computed grad = 0.043178516678135263
Iteration: 2155, Checking gradient for output layer W[45][0]:	mock grad = -0.006268554219340805, computed grad = -0.006268554218810872
Iteration: 2156, Checking gradient for words E[102][10]:	mock grad = -0.006091017580978075, computed grad = -0.006091017634002137
Iteration: 2156, Checking gradient for hidden W[60][42]:	mock grad = 0.005391561264417311, computed grad = 0.005391561288527458
Iteration: 2156, Checking gradient for hidden b[0][10]:	mock grad = -0.093353501101983705, computed grad = -0.093353530981990465
Iteration: 2156, Checking gradient for output layer W[134][1]:	mock grad = 0.145340796604220834, computed grad = 0.145340795678863183
Iteration: 2157, Checking gradient for words E[215][30]:	mock grad = -0.007224116875814701, computed grad = -0.007224116891351056
Iteration: 2157, Checking gradient for hidden W[244][26]:	mock grad = 0.000728379166314852, computed grad = 0.000728379178975037
Iteration: 2157, Checking gradient for hidden b[0][42]:	mock grad = -0.043484279162925299, computed grad = -0.043484285243654164
Iteration: 2157, Checking gradient for output layer W[46][1]:	mock grad = 0.055635746138371633, computed grad = 0.055635745924633780
Iteration: 2158, Checking gradient for words E[644][1]:	mock grad = -0.003758099879347387, computed grad = -0.003758099890502298
Iteration: 2158, Checking gradient for hidden W[139][44]:	mock grad = 0.005870911404776136, computed grad = 0.005870911500293083
Iteration: 2158, Checking gradient for hidden b[0][44]:	mock grad = -0.004151251361328434, computed grad = -0.004151249492442177
Iteration: 2158, Checking gradient for output layer W[40][0]:	mock grad = -0.016090343440411381, computed grad = -0.016090343436782766
current: 60, Cost = 0.266315, Correct(%) = 1, time = 4.14625
Iteration: 2159, Checking gradient for words E[647][2]:	mock grad = 0.015978052596038461, computed grad = 0.016411147670450987
Iteration: 2159, Checking gradient for hidden W[192][21]:	mock grad = 0.001375154381616506, computed grad = 0.001375154379501476
Iteration: 2159, Checking gradient for hidden b[0][17]:	mock grad = 0.014649984075992828, computed grad = 0.014649987581851516
Iteration: 2159, Checking gradient for output layer W[94][0]:	mock grad = -0.057416045619179679, computed grad = -0.057416045383701965
Iteration: 2160, Checking gradient for words E[6][47]:	mock grad = -0.008951982906957312, computed grad = -0.008951982951824018
Iteration: 2160, Checking gradient for hidden W[173][24]:	mock grad = -0.019208792499225691, computed grad = -0.019208792493079774
Iteration: 2160, Checking gradient for hidden b[0][14]:	mock grad = -0.047235494462949390, computed grad = -0.047235512801251961
Iteration: 2160, Checking gradient for output layer W[111][0]:	mock grad = 0.063526990812023953, computed grad = 0.063526990648660436
Iteration: 2161, Checking gradient for words E[405][5]:	mock grad = 0.000501505384331580, computed grad = 0.000701724294700260
Iteration: 2161, Checking gradient for hidden W[204][48]:	mock grad = 0.011283590005461974, computed grad = 0.011283590074022229
Iteration: 2161, Checking gradient for hidden b[0][2]:	mock grad = -0.046662085535442710, computed grad = -0.046662096744000660
Iteration: 2161, Checking gradient for output layer W[139][1]:	mock grad = 0.032529991644936196, computed grad = 0.032529991616219639
Iteration: 2162, Checking gradient for words E[16][7]:	mock grad = -0.017994530177672940, computed grad = -0.020369801890429560
Iteration: 2162, Checking gradient for hidden W[200][35]:	mock grad = -0.001936946036285025, computed grad = -0.001936946092455592
Iteration: 2162, Checking gradient for hidden b[0][6]:	mock grad = 0.030004100816299895, computed grad = 0.030004110894891415
Iteration: 2162, Checking gradient for output layer W[92][0]:	mock grad = -0.059297494898968850, computed grad = -0.059297494661290279
Iteration: 2163, Checking gradient for words E[662][40]:	mock grad = -0.025207628657486492, computed grad = -0.025207628607811297
Iteration: 2163, Checking gradient for hidden W[82][6]:	mock grad = -0.000740085360023679, computed grad = -0.000740085383261155
Iteration: 2163, Checking gradient for hidden b[0][5]:	mock grad = 0.014104126922015459, computed grad = 0.014104128248750274
Iteration: 2163, Checking gradient for output layer W[12][1]:	mock grad = -0.052793630196196206, computed grad = -0.052793630025282513
Iteration: 2164, Checking gradient for words E[194][22]:	mock grad = -0.018330340042754800, computed grad = -0.018330340054361474
Iteration: 2164, Checking gradient for hidden W[130][29]:	mock grad = 0.014709918801980582, computed grad = 0.017573117798734501
Iteration: 2164, Checking gradient for hidden b[0][36]:	mock grad = 0.003094675166515071, computed grad = 0.003094667765770573
Iteration: 2164, Checking gradient for output layer W[3][1]:	mock grad = -0.005922990728018940, computed grad = -0.005922990727903863
Iteration: 2165, Checking gradient for words E[650][16]:	mock grad = -0.006483045662045162, computed grad = -0.006483045670164842
Iteration: 2165, Checking gradient for hidden W[104][28]:	mock grad = -0.000200360909738406, computed grad = -0.000200360919602478
Iteration: 2165, Checking gradient for hidden b[0][15]:	mock grad = -0.052413202203727138, computed grad = -0.052413212375429376
Iteration: 2165, Checking gradient for output layer W[100][1]:	mock grad = 0.008395797377727665, computed grad = 0.008395797376002998
Iteration: 2166, Checking gradient for words E[215][9]:	mock grad = -0.000428523953183158, computed grad = -0.000428523926955210
Iteration: 2166, Checking gradient for hidden W[166][23]:	mock grad = 0.003607059015378611, computed grad = 0.003607059026816303
Iteration: 2166, Checking gradient for hidden b[0][25]:	mock grad = 0.004104791359205162, computed grad = 0.004104789446801112
Iteration: 2166, Checking gradient for output layer W[103][0]:	mock grad = 0.041117019609279715, computed grad = 0.041117019505238314
Iteration: 2167, Checking gradient for words E[295][15]:	mock grad = 0.007014029460489235, computed grad = 0.006019313504833056
Iteration: 2167, Checking gradient for hidden W[61][17]:	mock grad = 0.002030363779936817, computed grad = 0.002030363778484892
Iteration: 2167, Checking gradient for hidden b[0][25]:	mock grad = -0.006618337005298569, computed grad = -0.006618337021824658
Iteration: 2167, Checking gradient for output layer W[0][0]:	mock grad = 0.058222743876318317, computed grad = 0.058222743657862987
Iteration: 2168, Checking gradient for words E[119][31]:	mock grad = 0.007828628884806732, computed grad = 0.007509083859710667
Iteration: 2168, Checking gradient for hidden W[207][44]:	mock grad = -0.003795406213633057, computed grad = -0.003795406145495960
Iteration: 2168, Checking gradient for hidden b[0][31]:	mock grad = 0.027358066074262277, computed grad = 0.027358076855591360
Iteration: 2168, Checking gradient for output layer W[57][1]:	mock grad = -0.107161271414787929, computed grad = -0.107161268946349031
current: 70, Cost = 0.212083, Correct(%) = 1, time = 4.75764
Iteration: 2169, Checking gradient for words E[147][23]:	mock grad = -0.007721940841656783, computed grad = -0.007721940864043162
Iteration: 2169, Checking gradient for hidden W[208][47]:	mock grad = -0.006186428360172025, computed grad = -0.006186428385839628
Iteration: 2169, Checking gradient for hidden b[0][4]:	mock grad = -0.013000499551482636, computed grad = -0.013000505438535558
Iteration: 2169, Checking gradient for output layer W[96][0]:	mock grad = 0.135845886706828511, computed grad = 0.135845880989406143
Iteration: 2170, Checking gradient for words E[184][13]:	mock grad = 0.013627812257360761, computed grad = 0.013627812276233484
Iteration: 2170, Checking gradient for hidden W[219][24]:	mock grad = -0.004952487201753675, computed grad = -0.004952487198421270
Iteration: 2170, Checking gradient for hidden b[0][45]:	mock grad = 0.008414873772649711, computed grad = 0.008414875716295732
Iteration: 2170, Checking gradient for output layer W[112][1]:	mock grad = -0.025578182175245434, computed grad = -0.025578182163477930
Iteration: 2171, Checking gradient for words E[555][27]:	mock grad = -0.020885145701782237, computed grad = -0.020885145770342679
Iteration: 2171, Checking gradient for hidden W[91][11]:	mock grad = -0.002503145395960082, computed grad = -0.002503145407182710
Iteration: 2171, Checking gradient for hidden b[0][10]:	mock grad = -0.074804138625705896, computed grad = -0.074804162869346449
Iteration: 2171, Checking gradient for output layer W[149][0]:	mock grad = -0.077268688939230445, computed grad = -0.077268688562928678
Iteration: 2172, Checking gradient for words E[60][1]:	mock grad = -0.019053998299267683, computed grad = -0.019053998330852650
Iteration: 2172, Checking gradient for hidden W[50][25]:	mock grad = 0.003761849537070594, computed grad = 0.003761849560340317
Iteration: 2172, Checking gradient for hidden b[0][28]:	mock grad = -0.013945425913181042, computed grad = -0.013945443982393535
Iteration: 2172, Checking gradient for output layer W[124][1]:	mock grad = -0.006930158396201147, computed grad = -0.006930158395948352
Iteration: 2173, Checking gradient for words E[102][40]:	mock grad = -0.014989590534891795, computed grad = -0.014989590627178110
Iteration: 2173, Checking gradient for hidden W[179][40]:	mock grad = -0.000388360916314667, computed grad = -0.000388360914225020
Iteration: 2173, Checking gradient for hidden b[0][48]:	mock grad = 0.088041802063371088, computed grad = 0.088041809812127128
Iteration: 2173, Checking gradient for output layer W[17][1]:	mock grad = -0.040892196026098837, computed grad = -0.040892195990550669
Iteration: 2174, Checking gradient for words E[304][1]:	mock grad = -0.016752234929584553, computed grad = -0.016591461054888464
Iteration: 2174, Checking gradient for hidden W[228][7]:	mock grad = 0.007122093933886742, computed grad = 0.007122093956888148
Iteration: 2174, Checking gradient for hidden b[0][39]:	mock grad = -0.079363094461160077, computed grad = -0.079363123082433512
Iteration: 2174, Checking gradient for output layer W[53][0]:	mock grad = 0.099001964860234715, computed grad = 0.099001964306999854
Iteration: 2175, Checking gradient for words E[25][31]:	mock grad = 0.008952721423927112, computed grad = 0.006730293586468982
Iteration: 2175, Checking gradient for hidden W[128][19]:	mock grad = 0.005390157436266363, computed grad = 0.005390157403041303
Iteration: 2175, Checking gradient for hidden b[0][49]:	mock grad = -0.023612212057017024, computed grad = -0.023612211552152238
Iteration: 2175, Checking gradient for output layer W[21][0]:	mock grad = -0.014409820298752485, computed grad = -0.014409820293706103
Iteration: 2176, Checking gradient for words E[48][16]:	mock grad = 0.037688834984311859, computed grad = 0.037688834960419880
Iteration: 2176, Checking gradient for hidden W[38][31]:	mock grad = 0.005882804489154569, computed grad = 0.005882804504806766
Iteration: 2176, Checking gradient for hidden b[0][22]:	mock grad = 0.032267053653223243, computed grad = 0.032267058827142245
Iteration: 2176, Checking gradient for output layer W[17][0]:	mock grad = 0.031000844456646748, computed grad = 0.031000844423701299
Iteration: 2177, Checking gradient for words E[397][8]:	mock grad = 0.002255252148211651, computed grad = 0.003217137277384205
Iteration: 2177, Checking gradient for hidden W[233][6]:	mock grad = -0.000170822578846042, computed grad = -0.000170822579372335
Iteration: 2177, Checking gradient for hidden b[0][26]:	mock grad = -0.021871557189787061, computed grad = -0.021871564240154701
Iteration: 2177, Checking gradient for output layer W[149][1]:	mock grad = 0.015415406998398051, computed grad = 0.015415406990302000
Iteration: 2178, Checking gradient for words E[776][43]:	mock grad = 0.023043839162739665, computed grad = 0.023043839119539746
Iteration: 2178, Checking gradient for hidden W[19][14]:	mock grad = -0.001765141008081006, computed grad = -0.001765140898881934
Iteration: 2178, Checking gradient for hidden b[0][7]:	mock grad = -0.055063722733089682, computed grad = -0.055063726400152876
Iteration: 2178, Checking gradient for output layer W[129][0]:	mock grad = -0.121903607209011833, computed grad = -0.121903605291519404
current: 80, Cost = 0.228388, Correct(%) = 1, time = 5.24357
Iteration: 2179, Checking gradient for words E[156][13]:	mock grad = 0.030999507769524648, computed grad = 0.030047102804141440
Iteration: 2179, Checking gradient for hidden W[16][24]:	mock grad = -0.003770562041760517, computed grad = -0.003770562048878703
Iteration: 2179, Checking gradient for hidden b[0][21]:	mock grad = -0.035740973393358355, computed grad = -0.035740982966385011
Iteration: 2179, Checking gradient for output layer W[135][0]:	mock grad = -0.014193692083011578, computed grad = -0.014193692077594884
Iteration: 2180, Checking gradient for words E[354][0]:	mock grad = 0.030919876311291672, computed grad = 0.032853294943030145
Iteration: 2180, Checking gradient for hidden W[246][2]:	mock grad = -0.008009200875458422, computed grad = -0.008009200847507982
Iteration: 2180, Checking gradient for hidden b[0][46]:	mock grad = -0.106443262235272895, computed grad = -0.106443282840391062
Iteration: 2180, Checking gradient for output layer W[132][0]:	mock grad = 0.002062845922023993, computed grad = 0.002062845922071414
Iteration: 2181, Checking gradient for words E[466][5]:	mock grad = 0.008781539265950711, computed grad = 0.008781539279874608
Iteration: 2181, Checking gradient for hidden W[59][19]:	mock grad = 0.009098203128846127, computed grad = 0.009098203202299543
Iteration: 2181, Checking gradient for hidden b[0][7]:	mock grad = 0.051876166186140549, computed grad = 0.051876175607026134
Iteration: 2181, Checking gradient for output layer W[89][0]:	mock grad = -0.128909706403237956, computed grad = -0.128909702921581493
Iteration: 2182, Checking gradient for words E[718][46]:	mock grad = 0.007106897793235367, computed grad = 0.007106897775651115
Iteration: 2182, Checking gradient for hidden W[188][9]:	mock grad = 0.000982633392820897, computed grad = 0.000982633391904963
Iteration: 2182, Checking gradient for hidden b[0][22]:	mock grad = 0.023853118343547530, computed grad = 0.023853117618198263
Iteration: 2182, Checking gradient for output layer W[80][1]:	mock grad = 0.099849944189323026, computed grad = 0.099849942744387132
Iteration: 2183, Checking gradient for words E[274][31]:	mock grad = -0.012709273581545943, computed grad = -0.012709273578197519
Iteration: 2183, Checking gradient for hidden W[18][40]:	mock grad = -0.013171762821978517, computed grad = -0.013171763135305933
Iteration: 2183, Checking gradient for hidden b[0][43]:	mock grad = -0.009143536178340961, computed grad = -0.009143538621089220
