wordCutOff = 0
featCutOff = 0
initRange = 0.01
maxIter = 100
batchSize = 1
adaEps = 1e-06
adaAlpha = 0.01
regParameter = 1e-08
dropProb = -1
hiddenSize = 50
wordEmbSize = 50
wordcontext = 2
wordEmbFineTune = 1
cnnLayerSize = 2
verboseIter = 1
saveItermediate = 1
train = 0
maxInstance = 100
outBest = .debug
seg = 0
wordFile = 

instance num: 100

instance num: 100

instance num: 100
Creating Alphabet...
1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 
41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 
81 82 83 84 85 86 87 88 89 90 91 92 93 94 95 96 97 98 99 100 100 
Label num: 2
Sparse Feature num: 0
Word num: 776
Adding word Alphabet...
1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 
41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 
81 82 83 84 85 86 87 88 89 90 91 92 93 94 95 96 97 98 99 100 100 
Adding word Alphabet...
1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 
41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 
81 82 83 84 85 86 87 88 89 90 91 92 93 94 95 96 97 98 99 100 100 
1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 
41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 
81 82 83 84 85 86 87 88 89 90 91 92 93 94 95 96 97 98 99 100 100 
1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 
41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 
81 82 83 84 85 86 87 88 89 90 91 92 93 94 95 96 97 98 99 100 100 
1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 
41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 
81 82 83 84 85 86 87 88 89 90 91 92 93 94 95 96 97 98 99 100 100 
##### Iteration 0
random: 0, 99
current: 1, Cost = 0.565845, Correct(%) = 1, time = 0.0183078
Iteration: 0, Checking gradient for words E[19][26]:	mock grad = -0.076062783106956022, computed grad = -0.076062783047958341
Iteration: 0, Checking gradient for hidden W[81][9]:	mock grad = 0.019677631588110867, computed grad = 0.019677631644363657
Iteration: 0, Checking gradient for hidden b[0][9]:	mock grad = -0.071410134140692527, computed grad = -0.071410135406560854
Iteration: 0, Checking gradient for output layer W[15][0]:	mock grad = -0.093505919386449587, computed grad = -0.093505919330228199
current: 2, Cost = 0.2464, Correct(%) = 1, time = 0.0911531
Iteration: 1, Checking gradient for words E[78][35]:	mock grad = -0.002724749661342507, computed grad = -0.002724749650616743
Iteration: 1, Checking gradient for hidden W[89][10]:	mock grad = 0.009085702129749129, computed grad = 0.009085702143885563
Iteration: 1, Checking gradient for hidden b[0][16]:	mock grad = 0.017371641517341385, computed grad = 0.017371646826069318
Iteration: 1, Checking gradient for output layer W[82][0]:	mock grad = -0.139161517318062145, computed grad = -0.139161513172354617
current: 3, Cost = 1.86662, Correct(%) = 0.666667, time = 0.173119
Iteration: 2, Checking gradient for words E[32][34]:	mock grad = -0.028983091629353197, computed grad = -0.028983091665769473
Iteration: 2, Checking gradient for hidden W[152][42]:	mock grad = 0.037253097756373421, computed grad = 0.037253098001641484
Iteration: 2, Checking gradient for hidden b[0][12]:	mock grad = -0.179485656319422482, computed grad = -0.179485717218962754
Iteration: 2, Checking gradient for output layer W[96][0]:	mock grad = 0.527702910131488956, computed grad = 0.527702913792205019
current: 4, Cost = 1.47195, Correct(%) = 0.5, time = 0.236588
Iteration: 3, Checking gradient for words E[102][41]:	mock grad = -0.006671265621371880, computed grad = -0.006671265550567671
Iteration: 3, Checking gradient for hidden W[183][48]:	mock grad = 0.006201765880509136, computed grad = 0.006201765857978101
Iteration: 3, Checking gradient for hidden b[0][5]:	mock grad = 0.032582170395034638, computed grad = 0.032582158216592431
Iteration: 3, Checking gradient for output layer W[51][0]:	mock grad = 0.416919657905356900, computed grad = 0.416919660431206263
current: 5, Cost = 0.941124, Correct(%) = 0.4, time = 0.306918
Iteration: 4, Checking gradient for words E[64][27]:	mock grad = -0.038290956992703684, computed grad = -0.038290957143493391
Iteration: 4, Checking gradient for hidden W[90][14]:	mock grad = 0.018365563829447762, computed grad = 0.018365563971196375
Iteration: 4, Checking gradient for hidden b[0][46]:	mock grad = -0.255510391935331871, computed grad = -0.255510456330951929
Iteration: 4, Checking gradient for output layer W[97][0]:	mock grad = 0.317074086124857768, computed grad = 0.317074087349122347
current: 6, Cost = 0.565145, Correct(%) = 0.5, time = 0.421398
Iteration: 5, Checking gradient for words E[240][45]:	mock grad = 0.004248817660479354, computed grad = 0.004248817621573668
Iteration: 5, Checking gradient for hidden W[231][46]:	mock grad = 0.022063313072195889, computed grad = 0.022063313153350559
Iteration: 5, Checking gradient for hidden b[0][37]:	mock grad = -0.070436559046427316, computed grad = -0.070436584830665241
Iteration: 5, Checking gradient for output layer W[63][0]:	mock grad = 0.117744746006398504, computed grad = 0.117744745893134439
current: 7, Cost = 0.399012, Correct(%) = 0.571429, time = 0.616665
Iteration: 6, Checking gradient for words E[277][7]:	mock grad = -0.010019128881544326, computed grad = -0.010019128798197056
Iteration: 6, Checking gradient for hidden W[172][25]:	mock grad = 0.005592998490622225, computed grad = 0.005592998519473096
Iteration: 6, Checking gradient for hidden b[0][30]:	mock grad = 0.000763432354444760, computed grad = 0.000763429453907605
Iteration: 6, Checking gradient for output layer W[118][1]:	mock grad = -0.007370765028297255, computed grad = -0.007370765028193112
current: 8, Cost = 1.11669, Correct(%) = 0.5, time = 0.69791
Iteration: 7, Checking gradient for words E[25][15]:	mock grad = -0.031510329976502938, computed grad = -0.031510329932569303
Iteration: 7, Checking gradient for hidden W[235][41]:	mock grad = -0.014733642905717659, computed grad = -0.014733643092894044
Iteration: 7, Checking gradient for hidden b[0][44]:	mock grad = -0.004500438874788237, computed grad = -0.004500469048609623
Iteration: 7, Checking gradient for output layer W[22][0]:	mock grad = 0.023412459255633955, computed grad = 0.023412459256241601
current: 9, Cost = 0.511401, Correct(%) = 0.555556, time = 0.733154
Iteration: 8, Checking gradient for words E[48][14]:	mock grad = -0.003311481003798189, computed grad = -0.003311481038476918
Iteration: 8, Checking gradient for hidden W[51][15]:	mock grad = -0.009430659337839042, computed grad = -0.009430659396413285
Iteration: 8, Checking gradient for hidden b[0][24]:	mock grad = 0.113223034423570734, computed grad = 0.113223057723844389
Iteration: 8, Checking gradient for output layer W[69][1]:	mock grad = -0.028260768238197453, computed grad = -0.028260768235411966
current: 10, Cost = 1.00859, Correct(%) = 0.5, time = 0.793292
Iteration: 9, Checking gradient for words E[25][25]:	mock grad = -0.003067221023500544, computed grad = -0.003067220863155757
Iteration: 9, Checking gradient for hidden W[177][17]:	mock grad = 0.008192357486058732, computed grad = 0.008192357520635163
Iteration: 9, Checking gradient for hidden b[0][48]:	mock grad = 0.132178224206525030, computed grad = 0.132178217808487208
Iteration: 9, Checking gradient for output layer W[135][0]:	mock grad = 0.089333162124005661, computed grad = 0.089333162152928733
current: 11, Cost = 0.457824, Correct(%) = 0.545455, time = 0.847738
Iteration: 10, Checking gradient for words E[124][24]:	mock grad = 0.005001816118205271, computed grad = 0.005001816134595735
Iteration: 10, Checking gradient for hidden W[112][29]:	mock grad = 0.016116008354105649, computed grad = 0.016116008503234379
Iteration: 10, Checking gradient for hidden b[0][22]:	mock grad = -0.049261209179418675, computed grad = -0.049261222085249620
Iteration: 10, Checking gradient for output layer W[65][1]:	mock grad = -0.211913621639830962, computed grad = -0.211913619666894232
current: 12, Cost = 0.981739, Correct(%) = 0.5, time = 0.930264
Iteration: 11, Checking gradient for words E[102][0]:	mock grad = 0.027022823171907184, computed grad = 0.027022823107023298
Iteration: 11, Checking gradient for hidden W[210][43]:	mock grad = -0.000278786531193731, computed grad = -0.000278786543464189
Iteration: 11, Checking gradient for hidden b[0][26]:	mock grad = -0.070825087489956307, computed grad = -0.070825101083326736
Iteration: 11, Checking gradient for output layer W[33][0]:	mock grad = -0.041178958125831500, computed grad = -0.041178958128665170
current: 13, Cost = 0.581154, Correct(%) = 0.538462, time = 0.992722
Iteration: 12, Checking gradient for words E[776][14]:	mock grad = -0.004406804972600931, computed grad = -0.004406804962580946
Iteration: 12, Checking gradient for hidden W[7][12]:	mock grad = 0.000874708294573701, computed grad = 0.000874708314858020
Iteration: 12, Checking gradient for hidden b[0][12]:	mock grad = -0.105718450755409865, computed grad = -0.105718484541225774
